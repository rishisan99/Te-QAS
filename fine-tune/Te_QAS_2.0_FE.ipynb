{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9FKlKwNhL0B"
      },
      "source": [
        "# TeQAS 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC86j6Ju1daR"
      },
      "source": [
        "\n",
        "\n",
        "> English (1 epoch) + Telugu (3 epochs) Fine-Tuning | complete dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDL22VuVkpeY"
      },
      "source": [
        "##  XLM-R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBYcqpQqjii3"
      },
      "source": [
        "#### 锔 FIne-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aom9_ZaFi42n",
        "outputId": "12d9d1b6-1b51-42b0-8b6a-dac4c0775621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6CSy4aGhK4T",
        "outputId": "e9343cfa-42d0-4433-c86b-67928a465499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets==2.12.0\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (17.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.12.0)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (4.67.1)\n",
            "Collecting xxhash (from datasets==2.12.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.12.0)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.12.0) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (24.2)\n",
            "Collecting responses<0.19 (from datasets==2.12.0)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.12.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.12.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.12.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.12.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.12.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.12.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.12.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.12.0) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.12.0) (2024.12.14)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.12.0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.12.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.12.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.12.0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.12.0) (1.17.0)\n",
            "Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.12.0 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.5.0\n",
            "Collecting evaluate==0.4.0\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (24.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.0) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.0) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate==0.4.0) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate==0.4.0) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate==0.4.0) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.0) (1.17.0)\n",
            "Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=77cec2e9871d04d531a05f59945ce92e4215db5babc2ba7c3a85735ee69af6a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "# Install specific compatible versions\n",
        "!pip install datasets==2.12.0\n",
        "!pip install evaluate==0.4.0\n",
        "!pip install nltk\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsIvzQe22gi3",
        "outputId": "24f90300-c29c-4caa-d001-f8d894655cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Library Versions:\n",
            "torch: 2.5.1+cu121\n",
            "numpy: 1.26.4\n",
            "wandb: 0.19.1\n",
            "evaluate: 0.4.0\n",
            "datasets: 2.12.0\n",
            "transformers: 4.47.1\n",
            "nltk: 3.9.1\n",
            "rouge_score: rouge_score (no __version__ attribute)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import wandb\n",
        "import evaluate\n",
        "import datasets\n",
        "import transformers\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Print library versions\n",
        "print(\"Library Versions:\")\n",
        "print(f\"torch: {torch.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"wandb: {wandb.__version__}\")\n",
        "print(f\"evaluate: {evaluate.__version__}\")\n",
        "print(f\"datasets: {datasets.__version__}\")\n",
        "print(f\"transformers: {transformers.__version__}\")\n",
        "print(f\"nltk: {nltk.__version__}\")\n",
        "print(f\"rouge_score: rouge_score (no __version__ attribute)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFsVxkIVhJ43"
      },
      "outputs": [],
      "source": [
        "  #!/usr/bin/env python\n",
        "# fine_tune_qa_trainer.py\n",
        "\n",
        "### Import statements\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import wandb\n",
        "import evaluate\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    XLMRobertaForQuestionAnswering,\n",
        "    XLMRobertaTokenizerFast,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "from typing import List\n",
        "\n",
        "### QATrainer With Custom Loss\n",
        "\n",
        "class QATrainerWithCustomLoss(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Custom loss computation with boundary token weighting.\n",
        "        \"\"\"\n",
        "        # Model outputs\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Get the original loss components\n",
        "        start_logits = outputs.start_logits  # Shape: [batch_size, seq_len]\n",
        "        end_logits = outputs.end_logits      # Shape: [batch_size, seq_len]\n",
        "        start_positions = inputs[\"start_positions\"]  # Shape: [batch_size]\n",
        "        end_positions = inputs[\"end_positions\"]      # Shape: [batch_size]\n",
        "\n",
        "        # Original loss using CrossEntropyLoss\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')  # Token-wise loss\n",
        "        start_loss = loss_fct(start_logits, start_positions)  # Shape: [batch_size, seq_len]\n",
        "        end_loss = loss_fct(end_logits, end_positions)        # Shape: [batch_size, seq_len]\n",
        "\n",
        "        # Add boundary weighting\n",
        "        # Higher weights for tokens near the true boundaries\n",
        "        batch_size, seq_len = start_logits.shape\n",
        "        weighted_start_loss = []\n",
        "        weighted_end_loss = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Get true start and end positions for this example\n",
        "            true_start = start_positions[i]\n",
        "            true_end = end_positions[i]\n",
        "\n",
        "            # Create weight distribution centered around true positions\n",
        "            start_weights = torch.tensor(\n",
        "                create_boundary_weights(true_start.item(), seq_len),\n",
        "                device=start_logits.device,\n",
        "                dtype=start_logits.dtype\n",
        "            )\n",
        "            end_weights = torch.tensor(\n",
        "                create_boundary_weights(true_end.item(), seq_len),\n",
        "                device=end_logits.device,\n",
        "                dtype=end_logits.dtype\n",
        "            )\n",
        "\n",
        "            # Apply weights to the loss\n",
        "            weighted_start_loss.append(start_loss[i] * start_weights)\n",
        "            weighted_end_loss.append(end_loss[i] * end_weights)\n",
        "\n",
        "        # Stack and compute mean loss across the batch\n",
        "        weighted_start_loss = torch.stack(weighted_start_loss)  # Shape: [batch_size, seq_len]\n",
        "        weighted_end_loss = torch.stack(weighted_end_loss)      # Shape: [batch_size, seq_len]\n",
        "\n",
        "        total_loss = (weighted_start_loss.mean() + weighted_end_loss.mean()) / 2.0\n",
        "\n",
        "        return (total_loss, outputs) if return_outputs else total_loss\n",
        "\n",
        "def create_boundary_weights(position, seq_len, window_size=3, peak_weight=2.0):\n",
        "    \"\"\"Create weight distribution around boundary position.\"\"\"\n",
        "    weights = np.ones(seq_len)\n",
        "\n",
        "    # Create window around the boundary position\n",
        "    start_idx = max(0, position - window_size)\n",
        "    end_idx = min(seq_len, position + window_size + 1)\n",
        "\n",
        "    # Linear decay of weights from center\n",
        "    for idx in range(start_idx, end_idx):\n",
        "        distance = abs(idx - position)\n",
        "        weight = peak_weight * (1 - distance/window_size)\n",
        "        if weight > 1:\n",
        "            weights[idx] = weight\n",
        "\n",
        "    return weights\n",
        "\n",
        "### Metrics and Processing\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Text normalization.\"\"\"\n",
        "    def remove_articles(txt):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", txt)\n",
        "    def remove_punc(txt):\n",
        "        return re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
        "    def white_space_fix(txt):\n",
        "        return \" \".join(txt.split())\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def compute_bleu(pred, gold):\n",
        "    \"\"\"Compute BLEU scores for different n-grams.\"\"\"\n",
        "    if not pred or not gold:\n",
        "        return {\"unigram\": 0.0, \"bigram\": 0.0, \"trigram\": 0.0, \"quadgram\": 0.0}\n",
        "\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    gold_tokens = normalize_text(gold).split()\n",
        "\n",
        "    weights_dict = {\n",
        "        \"unigram\": (1.0, 0, 0, 0),\n",
        "        \"bigram\": (0.5, 0.5, 0, 0),\n",
        "        \"trigram\": (0.33, 0.33, 0.34, 0),\n",
        "        \"quadgram\": (0.25, 0.25, 0.25, 0.25),\n",
        "    }\n",
        "\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    scores = {}\n",
        "\n",
        "    for name, weights in weights_dict.items():\n",
        "        try:\n",
        "            score = sentence_bleu([gold_tokens], pred_tokens, weights=weights, smoothing_function=smoothing)\n",
        "        except Exception:\n",
        "            score = 0.0\n",
        "        scores[name] = score\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def compute_rouge(pred, gold):\n",
        "    \"\"\"Compute ROUGE scores.\"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    if not pred or not gold:\n",
        "        return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
        "    try:\n",
        "        scores = scorer.score(pred, gold)\n",
        "        return {k: v.fmeasure for k, v in scores.items()}\n",
        "    except Exception:\n",
        "        return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
        "\n",
        "def exact_match(pred, gold):\n",
        "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
        "\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    gold_tokens = normalize_text(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same = len(common)\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall    = num_same / len(gold_tokens)\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "### Answer Post-processing and Metrics Computation:\n",
        "\n",
        "def clean_prediction(text):\n",
        "    \"\"\"Clean predicted text by removing extra spaces and normalizing.\"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    # Example cleaning: strip whitespace and remove extra spaces\n",
        "    text = text.strip()  # Remove leading/trailing whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
        "    return text\n",
        "\n",
        "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
        "    \"\"\"Post-process model predictions with improved span extraction.\"\"\"\n",
        "    preds = {}\n",
        "    limit = min(len(examples), len(start_logits))\n",
        "    max_answer_length = 100\n",
        "    n_best_size = 20\n",
        "\n",
        "    for i in range(limit):\n",
        "        ex = examples[i]\n",
        "        offsets = ex[\"offset_mapping\"]\n",
        "        context = ex[\"context\"]\n",
        "        ex_id = ex[\"id\"]\n",
        "\n",
        "        start_indices = np.argsort(start_logits[i])[-n_best_size:].tolist()\n",
        "        end_indices = np.argsort(end_logits[i])[-n_best_size:].tolist()\n",
        "\n",
        "        best_score = float('-inf')\n",
        "        best_start = 0\n",
        "        best_end = 0\n",
        "\n",
        "        for start_idx in start_indices:\n",
        "            for end_idx in end_indices:\n",
        "                if (end_idx < start_idx or\n",
        "                    end_idx - start_idx + 1 > max_answer_length or\n",
        "                    start_idx >= len(offsets) or\n",
        "                    end_idx >= len(offsets) or\n",
        "                    offsets[start_idx] is None or\n",
        "                    offsets[end_idx] is None):\n",
        "                    continue\n",
        "\n",
        "                score = start_logits[i][start_idx] + end_logits[i][end_idx]\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_start = start_idx\n",
        "                    best_end = end_idx\n",
        "\n",
        "        if best_score != float('-inf'):\n",
        "            start_char = offsets[best_start][0]\n",
        "            end_char = offsets[best_end][1]\n",
        "            pred_text = context[start_char:end_char]\n",
        "            pred_text = clean_prediction(pred_text)\n",
        "        else:\n",
        "            pred_text = \"\"\n",
        "\n",
        "        preds[ex_id] = pred_text\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds: EvalPrediction, raw_dataset):\n",
        "    \"\"\"Compute EM, F1, BLEU, and ROUGE metrics.\"\"\"\n",
        "    start_logits, end_logits = eval_preds.predictions\n",
        "\n",
        "    if isinstance(start_logits, torch.Tensor):\n",
        "        start_logits = start_logits.cpu().numpy()\n",
        "        end_logits = end_logits.cpu().numpy()\n",
        "\n",
        "    preds_dict = postprocess_qa_predictions(raw_dataset, start_logits, end_logits)\n",
        "\n",
        "    metrics = {\n",
        "        \"em\": 0.0,\n",
        "        \"f1\": 0.0,\n",
        "        \"bleu_unigram\": 0.0,\n",
        "        \"bleu_bigram\": 0.0,\n",
        "        \"rouge1\": 0.0,\n",
        "        \"rouge2\": 0.0,\n",
        "        \"rougeL\": 0.0\n",
        "    }\n",
        "\n",
        "    total = 0\n",
        "\n",
        "    for ex in raw_dataset:\n",
        "        ex_id = ex[\"id\"]\n",
        "        pred = preds_dict.get(ex_id, \"\")\n",
        "        gold = ex[\"gold_text\"]\n",
        "\n",
        "        # Basic metrics\n",
        "        metrics[\"em\"] += exact_match(pred, gold)\n",
        "        metrics[\"f1\"] += f1_score(pred, gold)\n",
        "\n",
        "        # BLEU scores\n",
        "        bleu_scores = compute_bleu(pred, gold)\n",
        "        metrics[\"bleu_unigram\"] += bleu_scores[\"unigram\"]\n",
        "        metrics[\"bleu_bigram\"] += bleu_scores[\"bigram\"]\n",
        "\n",
        "        # ROUGE scores\n",
        "        rouge_scores = compute_rouge(pred, gold)\n",
        "        metrics[\"rouge1\"] += rouge_scores[\"rouge1\"]\n",
        "        metrics[\"rouge2\"] += rouge_scores[\"rouge2\"]\n",
        "        metrics[\"rougeL\"] += rouge_scores[\"rougeL\"]\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    # Average all metrics\n",
        "    for key in metrics:\n",
        "        metrics[key] = (metrics[key] / total) * 100.0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "### Custom Data Collator\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def squad_collate(features):\n",
        "    \"\"\"Collate features into batches.\"\"\"\n",
        "    for f in features:\n",
        "        for key in [\"input_ids\", \"attention_mask\", \"start_positions\", \"end_positions\"]:\n",
        "            if not isinstance(f[key], torch.Tensor):\n",
        "                f[key] = torch.tensor(f[key], dtype=torch.long)\n",
        "\n",
        "    batch = {\n",
        "        \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
        "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
        "        \"start_positions\": torch.stack([f[\"start_positions\"] for f in features]),\n",
        "        \"end_positions\": torch.stack([f[\"end_positions\"] for f in features]),\n",
        "    }\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDYkpsNAz4rI"
      },
      "source": [
        "##### English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5a7f7d2def8345779f12f255da7f1c2b",
            "f73583944fae4a838195cd80a06da378",
            "bd7a90472d7a4c3498ac56ca12f9effa",
            "77496d9f91234ebd94eebf9fdbac4332",
            "e2955f1e4c3441cfb471990418cf92e0",
            "39efa6ac1f114b3c97411dbccdbe3c33",
            "11b37d6a94884d80a984fac969a3e52f",
            "c61a668671b643e1b435a23176b3d4a2",
            "815682acf19d4ed3b53679d406ea6277",
            "e460989269c147c09bf610ff9911ce92",
            "569e68b7b6fa47d9bb823113069f5716",
            "dec2be8902974511ac11b0b7ab90ec4a",
            "e9fe0d2582964c9cbea0c5edd615b3d4",
            "3ac5e8ee3a8b483c87f80952c55d0d91",
            "6b7b8986320b4b46ab0a458bcf4b4dff",
            "cfe3ec6b3f7047e68c0fa033639f52c6",
            "943bad166f7648fc80ccc7f23af03607",
            "a55b4458661c49f982097b7df8271128",
            "e65307eaf4664f9e8bd5e4d8e5211f38",
            "6215fdc504c8428ca9c2db1d2d8f89c0",
            "ba19c4f8e5e44d3d886b3316221b8054",
            "abb33d6f7fc742fa817a4cab34bb42c9",
            "0e6b951fae174bad9ae021b609f09114",
            "b83d052b5f464946b957c4f181f9f0de",
            "a2be2d231aec4c408154cc454427e62f",
            "3fa49b1c5fc04e0fae24e47c6634456f",
            "74744e7a22a84c6ba33999bd06b81ee4",
            "8b6f69fee4b24eaf9ddc9beef6f850b3",
            "33a808c2d11f42838e88792a9003df37",
            "0f1aac8fef19443c8e0e80d838bb6ed2",
            "c2ea45e3e0954a4ab0e215556074ee5d",
            "afe74905c7d94e16b027d30027e82d16",
            "ebc6fc53fc9542bd9a700ac5bc377d95",
            "536b194539c74d06910ca559b1bd2cac",
            "85ae0934d4654c08835847f020f336de",
            "2b19d0ddd89d444bb0f15bd46bb3b8f5",
            "81ca5ee199b94e71a1e7bece8d96361a",
            "ecc6a0fe42374103a2ae306ff4eaf94c",
            "cb4547940d6b4b0fbf75ca259a87d32e",
            "a3987479d4d14059ab116e2d330d5b85",
            "3f5e018ea8fb4bac9d1633cdb10c89a5",
            "a995fbc1785b41b9a0554d168cde0827",
            "8dceb930687849069a5131ffaed0c750",
            "d4dde1d1dc104ba6aabfc57fb681a269",
            "7408181f4451494f8eb7d010f06be3f2",
            "3000a3d20e624d7691f5a80a5eeca39d",
            "9571edcc41c44e52a73334c11c2fd68e",
            "c945197bc778482e821193f5afa9e846",
            "533ffb379dd641528dc0254adabc57a9",
            "12eb6c71d29a44ed95d4fc9d22241137",
            "8856807d28bb4676837cc8420aebff59",
            "2f3d6e7c586b4c2bb8267c2f3101d427",
            "f0f1400cf08146f1bfc181326e65edd5",
            "27077a216e5b4d71b8f0b84c47b05cc1",
            "808bcadb03cb4312b4af06ff9663456d"
          ]
        },
        "id": "FQQm4aXthfYd",
        "outputId": "a0e914f0-a959-40b1-a0cc-e50ed0d3a4b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250108_115015-oytuwvj6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/oytuwvj6' target=\"_blank\">xlmr_eng_run1</a></strong> to <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%202.0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/oytuwvj6' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/oytuwvj6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a7f7d2def8345779f12f255da7f1c2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dec2be8902974511ac11b0b7ab90ec4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e6b951fae174bad9ae021b609f09114",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "536b194539c74d06910ca559b1bd2cac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-9716f86f4893>:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_list = torch.load(os.path.join(DATA_DIR, \"train.pt\"))\n",
            "<ipython-input-7-9716f86f4893>:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  val_list = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n",
            "<ipython-input-7-9716f86f4893>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_list = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7408181f4451494f8eb7d010f06be3f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3804' max='3804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3804/3804 2:05:35, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Em</th>\n",
              "      <th>F1</th>\n",
              "      <th>Bleu Unigram</th>\n",
              "      <th>Bleu Bigram</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.910300</td>\n",
              "      <td>0.958987</td>\n",
              "      <td>68.656195</td>\n",
              "      <td>79.672090</td>\n",
              "      <td>56.360128</td>\n",
              "      <td>40.980833</td>\n",
              "      <td>59.525473</td>\n",
              "      <td>37.845258</td>\n",
              "      <td>59.485369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving final model and tokenizer...\n",
            "\n",
            "Reloading final model for evaluation...\n",
            "\n",
            "Evaluating on test set with the final model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='371' max='371' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [371/371 03:49]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Metrics: {'eval_loss': 0.8145819306373596, 'eval_model_preparation_time': 0.0069, 'eval_em': 73.18266149434982, 'eval_f1': 80.50265218050467, 'eval_bleu_unigram': 35.56329938915903, 'eval_bleu_bigram': 25.910992238915902, 'eval_rouge1': 37.778763421064305, 'eval_rouge2': 23.783572813894303, 'eval_rougeL': 37.763108128402266, 'eval_runtime': 235.8855, 'eval_samples_per_second': 50.27, 'eval_steps_per_second': 1.573}\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu_bigram</td><td></td></tr><tr><td>eval/bleu_unigram</td><td></td></tr><tr><td>eval/em</td><td></td></tr><tr><td>eval/f1</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/model_preparation_time</td><td></td></tr><tr><td>eval/rouge1</td><td></td></tr><tr><td>eval/rouge2</td><td></td></tr><tr><td>eval/rougeL</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu_bigram</td><td>25.91099</td></tr><tr><td>eval/bleu_unigram</td><td>35.5633</td></tr><tr><td>eval/em</td><td>73.18266</td></tr><tr><td>eval/f1</td><td>80.50265</td></tr><tr><td>eval/loss</td><td>0.81458</td></tr><tr><td>eval/model_preparation_time</td><td>0.0069</td></tr><tr><td>eval/rouge1</td><td>37.77876</td></tr><tr><td>eval/rouge2</td><td>23.78357</td></tr><tr><td>eval/rougeL</td><td>37.76311</td></tr><tr><td>eval/runtime</td><td>235.8855</td></tr><tr><td>eval/samples_per_second</td><td>50.27</td></tr><tr><td>eval/steps_per_second</td><td>1.573</td></tr><tr><td>total_flos</td><td>1.1304592990978867e+17</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>0</td></tr><tr><td>train/grad_norm</td><td>27.91961</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9103</td></tr><tr><td>train_loss</td><td>1.2466</td></tr><tr><td>train_runtime</td><td>7538.6511</td></tr><tr><td>train_samples_per_second</td><td>16.147</td></tr><tr><td>train_steps_per_second</td><td>0.505</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">xlmr_eng_run1</strong> at: <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/oytuwvj6' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/oytuwvj6</a><br> View project at: <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%202.0</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250108_115015-oytuwvj6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training completed successfully!\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Missing files in /content/drive/MyDrive/TeQAS V5/XLM_R/final_xlmr_2.0_eng_1: vocab.json, merges.txt, added_tokens.json, pytorch_model.bin",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9716f86f4893>\u001b[0m in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-9716f86f4893>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Verify saved files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mverify_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFINAL_MODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOKENIZER_FILES\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_FILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9716f86f4893>\u001b[0m in \u001b[0;36mverify_directory\u001b[0;34m(path, required_files)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmissing_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequired_files\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmissing_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing files in {path}: {', '.join(missing_files)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Verified directory: {path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Missing files in /content/drive/MyDrive/TeQAS V5/XLM_R/final_xlmr_2.0_eng_1: vocab.json, merges.txt, added_tokens.json, pytorch_model.bin"
          ]
        }
      ],
      "source": [
        "### Main Training Code\n",
        "\n",
        "# Base directory setup\n",
        "BASE_DIR = \"/content/drive/MyDrive/TeQAS V5/XLM_R\"\n",
        "MODEL_NAME = \"xlm-roberta-large\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"xlm_r_processed_english_squad_v2\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"checkpoints_xlmr_answerable_1\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_DIR, \"final_xlmr_2.0_eng_1\")\n",
        "\n",
        "# Wandb configuration\n",
        "WANDB_PROJECT = \"TeQAS 2.0\"\n",
        "RUN_NAME = \"xlmr_eng_run1\"\n",
        "\n",
        "def verify_directory(path, required_files):\n",
        "    \"\"\"Check if a directory contains all required files.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Directory not found: {path}\")\n",
        "    missing_files = [f for f in required_files if not os.path.exists(os.path.join(path, f))]\n",
        "    if missing_files:\n",
        "        raise FileNotFoundError(f\"Missing files in {path}: {', '.join(missing_files)}\")\n",
        "    print(f\"Verified directory: {path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=WANDB_PROJECT, name=RUN_NAME)\n",
        "\n",
        "    TOKENIZER_FILES = [\n",
        "        \"tokenizer_config.json\",\n",
        "        \"special_tokens_map.json\",\n",
        "        \"sentencepiece.bpe.model\",\n",
        "        \"tokenizer.json\"\n",
        "    ]\n",
        "\n",
        "    MODEL_FILES = [\n",
        "        \"config.json\",\n",
        "        \"model.safetensors\"  # Replace with \"pytorch_model.bin\" if not using safetensors\n",
        "    ]\n",
        "\n",
        "\n",
        "    # 1) Load tokenizer and save\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "    # 2) Load data\n",
        "    print(\"Loading data...\")\n",
        "    train_list = torch.load(os.path.join(DATA_DIR, \"train.pt\"))\n",
        "    val_list = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n",
        "    test_list = []\n",
        "    if os.path.exists(os.path.join(DATA_DIR, \"test.pt\")):\n",
        "        test_list = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n",
        "\n",
        "    # 3) Convert to HF Datasets\n",
        "    train_dataset = Dataset.from_list(train_list)\n",
        "    val_dataset = Dataset.from_list(val_list)\n",
        "    test_dataset = Dataset.from_list(test_list) if test_list else None\n",
        "\n",
        "    # 4) Load model\n",
        "    print(\"Loading model...\")\n",
        "    model = XLMRobertaForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    model.to(device)  # Ensure the model is on the correct device\n",
        "\n",
        "    # Set dropout for regularization\n",
        "    model.config.hidden_dropout_prob = 0.2\n",
        "    model.config.attention_probs_dropout_prob = 0.2\n",
        "\n",
        "    # 5) Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        evaluation_strategy=\"epoch\",      # Evaluate once per epoch\n",
        "        save_strategy=\"epoch\",            # Save once per epoch\n",
        "        num_train_epochs=1,               # Increase to 2 for better convergence\n",
        "        learning_rate=2e-5,               # Slightly higher LR for faster training\n",
        "        per_device_train_batch_size=32,   # Reduce batch size for better gradient diversity\n",
        "        per_device_eval_batch_size=32,\n",
        "        warmup_ratio=0.1,                 # Retain warmup for smoother start\n",
        "        weight_decay=0.01,                # Regularization for stability\n",
        "        max_grad_norm=1.0,                # Gradient clipping\n",
        "        gradient_accumulation_steps=1,    # Disable accumulation to update weights more frequently\n",
        "        label_smoothing_factor=0.1,       # Keep smoothing for better generalization\n",
        "        logging_dir=\"logs_answerable\",\n",
        "        logging_steps=100,\n",
        "        report_to=\"wandb\"\n",
        "    )\n",
        "\n",
        "    # 6) Initialize Trainer\n",
        "    def compute_metrics_wrapper(eval_pred):\n",
        "        return compute_metrics(eval_pred, val_list)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=squad_collate,\n",
        "        compute_metrics=compute_metrics_wrapper,\n",
        "    )\n",
        "\n",
        "    # 7) Train\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # 8) Save final model and tokenizer\n",
        "    print(\"Saving final model and tokenizer...\")\n",
        "    trainer.save_model(FINAL_MODEL_DIR)\n",
        "    tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "    # Reload final model for evaluation\n",
        "    print(\"\\nReloading final model for evaluation...\")\n",
        "    final_model = XLMRobertaForQuestionAnswering.from_pretrained(FINAL_MODEL_DIR)\n",
        "    final_model.to(device)\n",
        "\n",
        "    # # Evaluate on validation set using the final model\n",
        "    # print(\"\\nEvaluating on validation set with the final model...\")\n",
        "    # val_trainer = Trainer(\n",
        "    #     model=final_model,\n",
        "    #     args=training_args,\n",
        "    #     eval_dataset=val_dataset,\n",
        "    #     data_collator=squad_collate,\n",
        "    #     compute_metrics=lambda eval_pred: compute_metrics(eval_pred, val_list),\n",
        "    # )\n",
        "    # val_metrics = val_trainer.evaluate()\n",
        "    # print(\"Validation Metrics:\", val_metrics)\n",
        "\n",
        "    # Evaluate on test set using the final model\n",
        "    if test_dataset and len(test_list) > 0:\n",
        "        print(\"\\nEvaluating on test set with the final model...\")\n",
        "        test_trainer = Trainer(\n",
        "            model=final_model,\n",
        "            args=training_args,\n",
        "            eval_dataset=test_dataset,\n",
        "            data_collator=squad_collate,\n",
        "            compute_metrics=lambda eval_pred: compute_metrics(eval_pred, test_list),\n",
        "        )\n",
        "        test_metrics = test_trainer.evaluate()\n",
        "        print(\"Test Metrics:\", test_metrics)\n",
        "\n",
        "        # Log metrics to WandB\n",
        "        wandb.log({\n",
        "           # \"final_val_metrics\": val_metrics,\n",
        "            \"final_test_metrics\": test_metrics\n",
        "        })\n",
        "\n",
        "    # Close WandB\n",
        "    wandb.finish()\n",
        "    print(\"\\nTraining completed successfully!\")\n",
        "\n",
        "    # Verify saved files\n",
        "    verify_directory(FINAL_MODEL_DIR, TOKENIZER_FILES + MODEL_FILES)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBxGVuWtz-LE"
      },
      "source": [
        "##### 喟む喟侧喟啾"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7bc1d7d639e74f449f278074ddd56c8c",
            "799c7f1222b942b2b851262d01e95fd8",
            "f12fd7a71aa8412e99bc512b55c52000",
            "e58fbc5277ff46b6a1ea40aa02cf3eac",
            "834af19ccf6e4393b626e7be99b63118",
            "d26e41305f714d379cbab0aab1bf1c80",
            "f1953d03dbc94c7e91e7181ac0d9404c",
            "1f494496a6e44a709c8a3dc233df086d",
            "01d94cb1501947ec8a5945e443720079",
            "f779e76d858d4b719f2b11d002719730",
            "2b088d8fc12744ee97ca90bd32ad8431",
            "954cdbabcadb4b7092fc49d4bc6081d9",
            "a49d59c5eb1f4114b226af3964eb185c",
            "08571731367f46cdb4474f9530caf6e7",
            "755abd7860114e578e2c65d8e8c03824",
            "373f0de7d6a34b10b69eb3b6081e3ede",
            "431e41f128a7495cbf4cbd28a03bc39b",
            "a3707170219d4392868438141478dd01",
            "07426cd8d8304accaf01e091dd87d3ec",
            "e8b47630e40e4492a819a5be2a7d9a34",
            "c904e2eb36db4335ae82afc9dd67a0d7",
            "6eed9f2ba14b4c0f8141580d5cc1d43b",
            "deead121a9064812a984d9b1c3d93481",
            "14e5e48a14194aae9246ed69064119c2",
            "9ad8741b2eb84ebda65d3537bfef5805",
            "6d7fa2e353e1496780863d98dfd954d9",
            "e372ecd13e6d489cadefb674bd1b9e37",
            "0afa423a9fea4e3685145fbe6d56f94a",
            "c292852b6ded4efebad0f8d5f9ad3526",
            "212851dfa28e44cda35b876c0a2f38cd",
            "3dedbe4783cb4bab94b4647ce54cd6d0",
            "ce5855731d5f4a6eb6c404a196c975d6",
            "16d5dc07992343df9602c43dd8ca9b91",
            "b07c6af29f18423cb7dad02cd0d7cd2e",
            "6c7e3e8939104582b42e46f8cf778ec6",
            "90f6ec8e4f37421abad1cb8e8cc2f75d",
            "23809099844740e1839daa6e528f21a6",
            "9eaa133e6dcb45cfa9d92fef51932870",
            "25a0afae73db472a8b085133d613eda1",
            "0dd811f7be6346d3ad88ab6c3948891e",
            "92f374cd10ed413082d7e5a80435aca9",
            "206aadc7eaa447b985537b2ada2365b2",
            "dcc058e5a36845d5a27fb06e3b27dfab",
            "246a20b2c929407ba5472824aa9065bd"
          ]
        },
        "id": "8FsB8o8Luz6v",
        "outputId": "619433b8-8df2-4d5e-fa43-94d5e09e3739"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250108_142703-jawkqxy2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/jawkqxy2' target=\"_blank\">xlmr_tel_run3</a></strong> to <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%202.0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/jawkqxy2' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/jawkqxy2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bc1d7d639e74f449f278074ddd56c8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "954cdbabcadb4b7092fc49d4bc6081d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deead121a9064812a984d9b1c3d93481",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b07c6af29f18423cb7dad02cd0d7cd2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-02c9ac6e1f52>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_list = torch.load(os.path.join(DATA_DIR, \"train.pt\"))\n",
            "<ipython-input-7-02c9ac6e1f52>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  val_list = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n",
            "<ipython-input-7-02c9ac6e1f52>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_list = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9477' max='9477' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9477/9477 5:13:58, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Em</th>\n",
              "      <th>F1</th>\n",
              "      <th>Bleu Unigram</th>\n",
              "      <th>Bleu Bigram</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.579300</td>\n",
              "      <td>1.596374</td>\n",
              "      <td>54.072609</td>\n",
              "      <td>66.749584</td>\n",
              "      <td>43.490724</td>\n",
              "      <td>32.752506</td>\n",
              "      <td>13.016367</td>\n",
              "      <td>3.340144</td>\n",
              "      <td>13.016367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.228600</td>\n",
              "      <td>1.485224</td>\n",
              "      <td>55.690016</td>\n",
              "      <td>69.682899</td>\n",
              "      <td>46.322471</td>\n",
              "      <td>35.094840</td>\n",
              "      <td>13.497645</td>\n",
              "      <td>3.435246</td>\n",
              "      <td>13.497645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.092300</td>\n",
              "      <td>1.511566</td>\n",
              "      <td>55.899465</td>\n",
              "      <td>69.965079</td>\n",
              "      <td>46.249508</td>\n",
              "      <td>35.040951</td>\n",
              "      <td>13.530699</td>\n",
              "      <td>3.457289</td>\n",
              "      <td>13.530699</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving final model...\n",
            "\n",
            "Reloading final model for evaluation...\n",
            "\n",
            "Evaluating on validation set with the final model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='269' max='269' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [269/269 02:45]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Metrics: {'eval_loss': 1.5115656852722168, 'eval_model_preparation_time': 0.0078, 'eval_em': 55.89946474284384, 'eval_f1': 69.96507908106994, 'eval_bleu_unigram': 46.24950823241404, 'eval_bleu_bigram': 35.04095097089434, 'eval_rouge1': 13.530699125300002, 'eval_rouge2': 3.457289323300494, 'eval_rougeL': 13.530699125300002, 'eval_runtime': 170.853, 'eval_samples_per_second': 50.301, 'eval_steps_per_second': 1.574}\n",
            "\n",
            "Evaluating on test set with the final model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [339/339 03:28]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Metrics: {'eval_loss': 1.4080579280853271, 'eval_model_preparation_time': 0.0068, 'eval_em': 61.40512631384843, 'eval_f1': 70.91967629635985, 'eval_bleu_unigram': 27.295939117647627, 'eval_bleu_bigram': 21.57858508907183, 'eval_rouge1': 7.426291310672646, 'eval_rouge2': 2.0978767682621635, 'eval_rougeL': 7.426291310672646, 'eval_runtime': 214.1874, 'eval_samples_per_second': 50.638, 'eval_steps_per_second': 1.583}\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu_bigram</td><td></td></tr><tr><td>eval/bleu_unigram</td><td></td></tr><tr><td>eval/em</td><td></td></tr><tr><td>eval/f1</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/model_preparation_time</td><td></td></tr><tr><td>eval/rouge1</td><td></td></tr><tr><td>eval/rouge2</td><td></td></tr><tr><td>eval/rougeL</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu_bigram</td><td>21.57859</td></tr><tr><td>eval/bleu_unigram</td><td>27.29594</td></tr><tr><td>eval/em</td><td>61.40513</td></tr><tr><td>eval/f1</td><td>70.91968</td></tr><tr><td>eval/loss</td><td>1.40806</td></tr><tr><td>eval/model_preparation_time</td><td>0.0068</td></tr><tr><td>eval/rouge1</td><td>7.42629</td></tr><tr><td>eval/rouge2</td><td>2.09788</td></tr><tr><td>eval/rougeL</td><td>7.42629</td></tr><tr><td>eval/runtime</td><td>214.1874</td></tr><tr><td>eval/samples_per_second</td><td>50.638</td></tr><tr><td>eval/steps_per_second</td><td>1.583</td></tr><tr><td>total_flos</td><td>2.8163782501875302e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>0</td></tr><tr><td>train/grad_norm</td><td>24.28714</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.0923</td></tr><tr><td>train_loss</td><td>1.36785</td></tr><tr><td>train_runtime</td><td>18842.0858</td></tr><tr><td>train_samples_per_second</td><td>16.095</td></tr><tr><td>train_steps_per_second</td><td>0.503</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">xlmr_tel_run3</strong> at: <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/jawkqxy2' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%202.0/runs/jawkqxy2</a><br> View project at: <a href='https://wandb.ai/santhosh-rishi/TeQAS%202.0' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%202.0</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250108_142703-jawkqxy2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Base directory setup\n",
        "BASE_DIR = \"/content/drive/MyDrive/TeQAS V5/XLM_R\"\n",
        "MODEL_NAME = os.path.join(BASE_DIR, \"final_xlmr_2.0_eng_1\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"xlm_r_processed_telugu_squad_v2\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"checkpoints_xlmr_answerable_tel_3\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_DIR, \"final_xlmr_2.0_tel_3\")\n",
        "\n",
        "# Wandb configuration\n",
        "WANDB_PROJECT = \"TeQAS 2.0\"\n",
        "RUN_NAME = \"xlmr_tel_run3\"\n",
        "\n",
        "def main():\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=WANDB_PROJECT, name=RUN_NAME)\n",
        "\n",
        "    # 1) Load tokenizer and save\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")\n",
        "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "    # 2) Load data\n",
        "    print(\"Loading data...\")\n",
        "    train_list = torch.load(os.path.join(DATA_DIR, \"train.pt\"))\n",
        "    val_list = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n",
        "    test_list = []\n",
        "    if os.path.exists(os.path.join(DATA_DIR, \"test.pt\")):\n",
        "        test_list = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n",
        "\n",
        "    # 3) Convert to HF Datasets\n",
        "    train_dataset = Dataset.from_list(train_list)\n",
        "    val_dataset = Dataset.from_list(val_list)\n",
        "    test_dataset = Dataset.from_list(test_list) if test_list else None\n",
        "\n",
        "    # 4) Load model\n",
        "    print(\"Loading model...\")\n",
        "    model = XLMRobertaForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    model.to(device)  # Ensure the model is on the correct device\n",
        "\n",
        "    # Set dropout for regularization\n",
        "    model.config.hidden_dropout_prob = 0.2\n",
        "    model.config.attention_probs_dropout_prob = 0.2\n",
        "\n",
        "    # 5) Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        evaluation_strategy=\"epoch\",      # Evaluate once per epoch\n",
        "        save_strategy=\"epoch\",            # Save once per epoch\n",
        "        num_train_epochs=3,               # Increase to 2 for better convergence\n",
        "        learning_rate=2e-5,               # Slightly higher LR for faster training\n",
        "        per_device_train_batch_size=32,   # Reduce batch size for better gradient diversity\n",
        "        per_device_eval_batch_size=32,\n",
        "        warmup_ratio=0.1,                 # Retain warmup for smoother start\n",
        "        weight_decay=0.01,                # Regularization for stability\n",
        "        max_grad_norm=1.0,                # Gradient clipping\n",
        "        gradient_accumulation_steps=1,    # Disable accumulation to update weights more frequently\n",
        "        label_smoothing_factor=0.1,       # Keep smoothing for better generalization\n",
        "        logging_dir=\"logs_answerable\",\n",
        "        logging_steps=100,\n",
        "        report_to=\"wandb\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # 6) Initialize Trainer\n",
        "    def compute_metrics_wrapper(eval_pred):\n",
        "        return compute_metrics(eval_pred, val_list)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=squad_collate,\n",
        "        compute_metrics=compute_metrics_wrapper,\n",
        "    )\n",
        "\n",
        "    # 7) Train\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # 8) Save final model\n",
        "    print(\"Saving final model...\")\n",
        "    trainer.save_model(FINAL_MODEL_DIR)\n",
        "\n",
        "    # Reload final model for evaluation\n",
        "    print(\"\\nReloading final model for evaluation...\")\n",
        "    final_model = XLMRobertaForQuestionAnswering.from_pretrained(FINAL_MODEL_DIR)\n",
        "    final_model.to(device)\n",
        "\n",
        "    # Evaluate on validation set using the final model\n",
        "    print(\"\\nEvaluating on validation set with the final model...\")\n",
        "    val_trainer = Trainer(\n",
        "        model=final_model,\n",
        "        args=training_args,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=squad_collate,\n",
        "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, val_list),\n",
        "    )\n",
        "    val_metrics = val_trainer.evaluate()\n",
        "    print(\"Validation Metrics:\", val_metrics)\n",
        "\n",
        "    # Evaluate on test set using the final model\n",
        "    if test_dataset and len(test_list) > 0:\n",
        "        print(\"\\nEvaluating on test set with the final model...\")\n",
        "        test_trainer = Trainer(\n",
        "            model=final_model,\n",
        "            args=training_args,\n",
        "            eval_dataset=test_dataset,\n",
        "            data_collator=squad_collate,\n",
        "            compute_metrics=lambda eval_pred: compute_metrics(eval_pred, test_list),\n",
        "        )\n",
        "        test_metrics = test_trainer.evaluate()\n",
        "        print(\"Test Metrics:\", test_metrics)\n",
        "\n",
        "        # Log metrics to WandB\n",
        "        wandb.log({\n",
        "            \"final_val_metrics\": val_metrics,\n",
        "            \"final_test_metrics\": test_metrics\n",
        "        })\n",
        "    else:\n",
        "        wandb.log({\"final_val_metrics\": val_metrics})\n",
        "\n",
        "    # Close WandB\n",
        "    wandb.finish()\n",
        "    print(\"\\nTraining completed successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD_jThfm0JIH"
      },
      "source": [
        "#### 锔 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAeFOoUGetpR",
        "outputId": "3aa94aeb-0147-4b87-9fe5-f2be7c40a2fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "\n",
            "Loading final model from: /content/drive/MyDrive/TeQAS V5/XLM_R/final_xlmr_2.0_tel_3\n",
            "Loading [val] data from: /content/drive/MyDrive/TeQAS V5/XLM_R/xlm_r_processed_telugu_squad_v2/val.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-24835cca1373>:284: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_list = torch.load(data_file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running inference on val set... (num_examples=8594)\n",
            "\n",
            "[val Metrics]:\n",
            "  em: 55.52\n",
            "  f1: 69.59\n",
            "  bleu_unigram: 45.88\n",
            "  bleu_bigram: 34.80\n",
            "  rouge1: 13.45\n",
            "  rouge2: 1.35\n",
            "  rougeL: 13.45\n",
            "  is_impossible_acc: 87.42\n",
            "----------------------------------------\n",
            "Loading [test] data from: /content/drive/MyDrive/TeQAS V5/XLM_R/xlm_r_processed_telugu_squad_v2/test.pt\n",
            "Running inference on test set... (num_examples=10846)\n",
            "\n",
            "[test Metrics]:\n",
            "  em: 61.14\n",
            "  f1: 70.65\n",
            "  bleu_unigram: 27.03\n",
            "  bleu_bigram: 21.40\n",
            "  rouge1: 7.38\n",
            "  rouge2: 0.84\n",
            "  rougeL: 7.38\n",
            "  is_impossible_acc: 82.20\n",
            "----------------------------------------\n",
            "\n",
            "Evaluation script completed.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# eval_squad_v2_telugu.py\n",
        "#\n",
        "# A separate evaluation script that:\n",
        "#  1) Loads a fine-tuned XLM-R model (SQuAD v2.0 style).\n",
        "#  2) Runs inference on val/test datasets.\n",
        "#  3) Computes EM, F1, BLEU, ROUGE, plus is_impossible accuracy.\n",
        "#\n",
        "# NOTE: Adapt paths (MODEL_DIR, DATA_DIR, etc.) as needed.\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    XLMRobertaForQuestionAnswering,\n",
        "    XLMRobertaTokenizerFast\n",
        ")\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import re\n",
        "\n",
        "##############################################################################\n",
        "# 1) Telugu-Specific Normalization\n",
        "##############################################################################\n",
        "def normalize_text_telugu(s):\n",
        "    \"\"\"\n",
        "    Minimal Telugu-oriented normalization:\n",
        "      - Removes extra punctuation\n",
        "      - Lowercases any English letters\n",
        "      - Strips extra whitespace\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    # Remove everything not Telugu or alphanumeric. Adjust to your corpus if needed.\n",
        "    s = re.sub(r\"[^\\u0C00-\\u0C7Fa-zA-Z0-9\\s]\", \"\", s)\n",
        "    # Lowercase (affects only English letters)\n",
        "    s = s.lower()\n",
        "    # Remove multiple spaces\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "##############################################################################\n",
        "# 2) Basic Cleaning for Predicted Spans\n",
        "##############################################################################\n",
        "def clean_prediction(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "##############################################################################\n",
        "# 3) No-Answer Post-processing for SQuAD v2.0\n",
        "##############################################################################\n",
        "def postprocess_qa_predictions_squad_v2(\n",
        "    examples,\n",
        "    start_logits,\n",
        "    end_logits,\n",
        "    cls_index=0,\n",
        "    max_answer_length=100,\n",
        "    n_best_size=20,\n",
        "    null_score_diff_threshold=0.0\n",
        "):\n",
        "    \"\"\"\n",
        "    - examples: list of dicts with \"id\", \"context\", \"offset_mapping\", etc.\n",
        "    - start_logits[i], end_logits[i]: arrays of length [sequence_length].\n",
        "    - cls_index: index for [CLS] token logit (XLM-R often has CLS at position 0).\n",
        "    - null_score_diff_threshold: threshold for \"no answer\" decision.\n",
        "    \"\"\"\n",
        "    preds = {}\n",
        "\n",
        "    for i, ex in enumerate(examples):\n",
        "        context = ex[\"context\"]\n",
        "        offsets = ex[\"offset_mapping\"]\n",
        "        ex_id   = ex[\"id\"]\n",
        "\n",
        "        # 1) Find best span\n",
        "        best_score = float('-inf')\n",
        "        best_start, best_end = 0, 0\n",
        "\n",
        "        start_idxs = np.argsort(start_logits[i])[-n_best_size:].tolist()\n",
        "        end_idxs   = np.argsort(end_logits[i])[-n_best_size:].tolist()\n",
        "\n",
        "        for st in start_idxs:\n",
        "            for en in end_idxs:\n",
        "                if en < st:\n",
        "                    continue\n",
        "                if (en - st + 1) > max_answer_length:\n",
        "                    continue\n",
        "                if st >= len(offsets) or en >= len(offsets):\n",
        "                    continue\n",
        "\n",
        "                span_score = start_logits[i][st] + end_logits[i][en]\n",
        "                if span_score > best_score:\n",
        "                    best_score = span_score\n",
        "                    best_start = st\n",
        "                    best_end   = en\n",
        "\n",
        "        # 2) Calculate no-answer score from [CLS] token\n",
        "        cls_score = start_logits[i][cls_index] + end_logits[i][cls_index]\n",
        "\n",
        "        # 3) Decide if no-answer\n",
        "        score_diff = best_score - cls_score\n",
        "        if score_diff < null_score_diff_threshold:\n",
        "            preds[ex_id] = \"\"\n",
        "        else:\n",
        "            start_char = offsets[best_start][0]\n",
        "            end_char   = offsets[best_end][1]\n",
        "            text_span  = context[start_char:end_char]\n",
        "            preds[ex_id] = clean_prediction(text_span)\n",
        "\n",
        "    return preds\n",
        "\n",
        "##############################################################################\n",
        "# 4) Metrics: EM, F1, BLEU, ROUGE, plus is_impossible accuracy\n",
        "##############################################################################\n",
        "def exact_match(pred, gold):\n",
        "    return 1.0 if normalize_text_telugu(pred) == normalize_text_telugu(gold) else 0.0\n",
        "\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_text_telugu(pred).split()\n",
        "    gold_tokens = normalize_text_telugu(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same = len(common)\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return float(pred_tokens == gold_tokens)\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall    = num_same / len(gold_tokens)\n",
        "    if (precision + recall) == 0:\n",
        "        return 0.0\n",
        "    return (2.0 * precision * recall) / (precision + recall)\n",
        "\n",
        "def compute_bleu(pred, gold):\n",
        "    if not pred or not gold:\n",
        "        return {\"unigram\": 0.0, \"bigram\": 0.0}\n",
        "    pred_tokens = normalize_text_telugu(pred).split()\n",
        "    gold_tokens = normalize_text_telugu(gold).split()\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    unigram = sentence_bleu([gold_tokens], pred_tokens,\n",
        "                            weights=(1, 0, 0, 0),\n",
        "                            smoothing_function=smoothing)\n",
        "    bigram  = sentence_bleu([gold_tokens], pred_tokens,\n",
        "                            weights=(0.5, 0.5, 0, 0),\n",
        "                            smoothing_function=smoothing)\n",
        "    return {\"unigram\": unigram, \"bigram\": bigram}\n",
        "\n",
        "def compute_rouge(pred, gold):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=False)\n",
        "    if not pred or not gold:\n",
        "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
        "    pred_clean = normalize_text_telugu(pred)\n",
        "    gold_clean = normalize_text_telugu(gold)\n",
        "    scores = scorer.score(pred_clean, gold_clean)\n",
        "    return {\n",
        "        \"rouge1\": scores[\"rouge1\"].fmeasure,\n",
        "        \"rouge2\": scores[\"rouge2\"].fmeasure,\n",
        "        \"rougeL\": scores[\"rougeL\"].fmeasure\n",
        "    }\n",
        "\n",
        "def compute_is_impossible_accuracy(examples, predictions):\n",
        "    \"\"\"\n",
        "    For each example, if gold_text is empty => gold is_impossible.\n",
        "    We check if predicted text is also empty => predicted is_impossible.\n",
        "    Then compute accuracy over *only* the is_impossible subset.\n",
        "    \"\"\"\n",
        "    total_impossible = 0\n",
        "    correct_impossible = 0\n",
        "    for ex in examples:\n",
        "        gold = ex.get(\"gold_text\", \"\")\n",
        "        gold_impossible = (gold.strip() == \"\")\n",
        "        if gold_impossible:\n",
        "            total_impossible += 1\n",
        "            pred_text = predictions.get(ex[\"id\"], \"\")\n",
        "            if pred_text.strip() == \"\":\n",
        "                correct_impossible += 1\n",
        "\n",
        "    if total_impossible == 0:\n",
        "        return 100.0  # If no impossible examples at all, define it as 100% or 0%\n",
        "    return (correct_impossible / total_impossible) * 100.0\n",
        "\n",
        "##############################################################################\n",
        "# 5) Master Evaluation Function\n",
        "##############################################################################\n",
        "def evaluate_squad_v2(examples, start_logits, end_logits, null_score_diff_threshold=0.0):\n",
        "    \"\"\"\n",
        "    Returns a dict with EM, F1, BLEU, ROUGE, plus 'is_impossible_acc'.\n",
        "    \"\"\"\n",
        "    predictions = postprocess_qa_predictions_squad_v2(\n",
        "        examples,\n",
        "        start_logits,\n",
        "        end_logits,\n",
        "        cls_index=0,  # For xlm-roberta, typically 0 is [CLS]\n",
        "        null_score_diff_threshold=null_score_diff_threshold\n",
        "    )\n",
        "\n",
        "    total = len(examples)\n",
        "    metrics = {\n",
        "        \"em\": 0.0,\n",
        "        \"f1\": 0.0,\n",
        "        \"bleu_unigram\": 0.0,\n",
        "        \"bleu_bigram\": 0.0,\n",
        "        \"rouge1\": 0.0,\n",
        "        \"rouge2\": 0.0,\n",
        "        \"rougeL\": 0.0\n",
        "    }\n",
        "\n",
        "    for ex in examples:\n",
        "        gold = ex.get(\"gold_text\", \"\")\n",
        "        pred = predictions.get(ex[\"id\"], \"\")\n",
        "\n",
        "        metrics[\"em\"] += exact_match(pred, gold)\n",
        "        metrics[\"f1\"] += f1_score(pred, gold)\n",
        "\n",
        "        bleu_scores = compute_bleu(pred, gold)\n",
        "        metrics[\"bleu_unigram\"] += bleu_scores[\"unigram\"]\n",
        "        metrics[\"bleu_bigram\"]  += bleu_scores[\"bigram\"]\n",
        "\n",
        "        rouge_scores = compute_rouge(pred, gold)\n",
        "        metrics[\"rouge1\"] += rouge_scores[\"rouge1\"]\n",
        "        metrics[\"rouge2\"] += rouge_scores[\"rouge2\"]\n",
        "        metrics[\"rougeL\"] += rouge_scores[\"rougeL\"]\n",
        "\n",
        "    # Average the main metrics\n",
        "    for key in metrics:\n",
        "        metrics[key] = (metrics[key] / total) * 100.0\n",
        "\n",
        "    # Calculate is_impossible accuracy\n",
        "    is_imp_acc = compute_is_impossible_accuracy(examples, predictions)\n",
        "    metrics[\"is_impossible_acc\"] = is_imp_acc\n",
        "\n",
        "    return metrics, predictions\n",
        "\n",
        "##############################################################################\n",
        "# 6) Minimal Collate for Eval\n",
        "##############################################################################\n",
        "def squad_collate_eval(features):\n",
        "    \"\"\"\n",
        "    For evaluation, we only need input_ids & attention_mask for the model.\n",
        "    Convert lists to Tensors.\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for f in features:\n",
        "        input_ids.append(torch.tensor(f[\"input_ids\"], dtype=torch.long))\n",
        "        attention_masks.append(torch.tensor(f[\"attention_mask\"], dtype=torch.long))\n",
        "    return {\n",
        "        \"input_ids\": torch.stack(input_ids, dim=0),\n",
        "        \"attention_mask\": torch.stack(attention_masks, dim=0)\n",
        "    }\n",
        "\n",
        "##############################################################################\n",
        "# 7) Main Evaluation Code\n",
        "##############################################################################\n",
        "def main():\n",
        "\n",
        "    BASE_DIR = \"/content/drive/MyDrive/TeQAS V5/XLM_R\"\n",
        "    DATA_DIR = os.path.join(BASE_DIR, \"xlm_r_processed_telugu_squad_v2\")\n",
        "\n",
        "    # Adjust these paths:\n",
        "    MODEL_DIR = os.path.join(BASE_DIR, \"final_xlmr_2.0_tel_3\")\n",
        "    VAL_FILE  = os.path.join(DATA_DIR, \"val.pt\")\n",
        "    TEST_FILE = os.path.join(DATA_DIR, \"test.pt\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Load model\n",
        "    print(\"\\nLoading final model from:\", MODEL_DIR)\n",
        "    model = XLMRobertaForQuestionAnswering.from_pretrained(MODEL_DIR)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Load tokenizer if needed for any reason (optional)\n",
        "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")\n",
        "\n",
        "    # Function to run evaluation on a dataset (val or test)\n",
        "    def run_eval(data_file, split_name=\"val\"):\n",
        "        if not os.path.exists(data_file):\n",
        "            print(f\"[{split_name}] file not found: {data_file}\")\n",
        "            return None\n",
        "        print(f\"Loading [{split_name}] data from:\", data_file)\n",
        "        data_list = torch.load(data_file)\n",
        "        dataset = Dataset.from_list(data_list)\n",
        "        dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=squad_collate_eval)\n",
        "\n",
        "        all_start_logits = []\n",
        "        all_end_logits   = []\n",
        "\n",
        "        print(f\"Running inference on {split_name} set... (num_examples={len(dataset)})\")\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                start_logits = outputs.start_logits.detach().cpu().numpy()\n",
        "                end_logits   = outputs.end_logits.detach().cpu().numpy()\n",
        "\n",
        "                all_start_logits.append(start_logits)\n",
        "                all_end_logits.append(end_logits)\n",
        "\n",
        "        all_start_logits = np.concatenate(all_start_logits, axis=0)\n",
        "        all_end_logits   = np.concatenate(all_end_logits, axis=0)\n",
        "\n",
        "        # Evaluate with threshold=0.0 (typical default)\n",
        "        results, preds = evaluate_squad_v2(\n",
        "            data_list,\n",
        "            all_start_logits,\n",
        "            all_end_logits,\n",
        "            null_score_diff_threshold=0.0\n",
        "        )\n",
        "        print(f\"\\n[{split_name} Metrics]:\")\n",
        "        for k, v in results.items():\n",
        "            print(f\"  {k}: {v:.2f}\")\n",
        "        print(\"-\"*40)\n",
        "        return results\n",
        "\n",
        "    # Evaluate on validation\n",
        "    val_metrics = run_eval(VAL_FILE, \"val\")\n",
        "\n",
        "    # Evaluate on test\n",
        "    test_metrics = run_eval(TEST_FILE, \"test\")\n",
        "\n",
        "    print(\"\\nEvaluation script completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfEZ13wmjSMd"
      },
      "source": [
        "##### Comparision with Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926,
          "referenced_widgets": [
            "32bb69d567d443d0964d9f241cef5fa6",
            "8141ce556cc044559223665c5c8af949",
            "570756f1a4a7409684e8b3d807dc720d",
            "389ba94d32ab49138ec18e5c54624a07",
            "855ef79d647540b68edc1614be269f00",
            "f03012a1a5c6418ea2d7aa29a61d06e1",
            "fe6d5e8fd55b42f4ad0387b81496a41b",
            "a51e1c5f34df4626bbff99fc98ff10e5",
            "00b55bb8300f44b88f3f2e43766af549",
            "40e9381a4daa4b01b39f83a7d54f78bc",
            "9ee126bebaf74ab39eead9bf852e5fd4",
            "9a1c6ca6597f4e6eaf0458fd462d2737",
            "123e773fd6fe4378bb3ea7f968fe05b0",
            "ea7a04251d6a4d28bbf5da029b2ac1af",
            "23d769cd912148b392249d14d1b5075a",
            "2b37132873f147a89a5f6be5aa4e34c9",
            "785f01dd16184ca78b67c6e8605dc07c",
            "7c8e9323bb9d48d39cdd8d4db9c38ddc",
            "9808599abccf4c0e9bb5f4f2b9be3848",
            "b58f074f291a465c912cf0613a6fb4ff",
            "493853a37c054471b68b8828b3766884",
            "fbd25d72f17a4e80a24e9b2bbd6995e6",
            "cbebb69a4edc4c3d94477e5936694be3",
            "171476f8a223409d84687e4d46b44a8d",
            "01eff24caf954ccfb09e3fc0baa35f56",
            "9b59b2c7c4974f5790a4af6749252eba",
            "75170268a5974144a05d09d38d4cc9d1",
            "39eeda7f3fce444bad4fc0da17e97d2f",
            "83dcbb499e2449a4b919074ae548aa9f",
            "570b9024f826435bb13c8e6fd3fbc5f7",
            "c7e7ec7806a64f6abfe2e35e9e5b31e5",
            "8eda24f7574b4fe49710c3cbdd23a13f",
            "048bc030565046d1b3e0d401dd49dfd5",
            "b94310b5721e4731986c2f7bba5ccd44",
            "214a1b4b9a5a4766ba1f90b09cf3eb2b",
            "9ffb71c4f70b456cb033aa94f256b7ac",
            "90201913d38d485c9c72469d7b6e4b29",
            "0793416922c94bae8a00202e60808a59",
            "3b5a274cc7144cb2a442e700f944d9e3",
            "90217091c9b647ad84f873e999d640bb",
            "e05039015cb6411a9b3ecbcb42e9c057",
            "415f5c32d4be4adc82fd1beb7cd8d2ba",
            "cc2029179f9a4d3797f4170e0dc59bbf",
            "ff5823e48dbf42eea3918bcfeae9e887",
            "054a2a58d2fc451c9daed0574cfdeaaa",
            "d4b5b70bf99a46bdb9b9a807740a4e6f",
            "857770554a2a426c9b028a374859abaa",
            "a574fe56b27c45fbbed8af9e53e01040",
            "4ef116df03a940259a3117269c4c1099",
            "c15f6be65fdd4adfbf4ae679ad6eb45d",
            "d7da4b8e8f4f46518d30492d08b3e898",
            "e4449dc6c5be4b3ead863a6aecad3ecf",
            "a4eb4af54550473e9f1834c89c9c09d9",
            "684978beae0042dd9c2f3f41d93c2235",
            "d3e41812607440fbaf27e0382d474dd9"
          ]
        },
        "id": "ImsYPGsl0ziB",
        "outputId": "bba42f71-66fe-4d6a-c21e-43b6d4afd3b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading Baseline Model: xlm-roberta-large\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32bb69d567d443d0964d9f241cef5fa6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a1c6ca6597f4e6eaf0458fd462d2737",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbebb69a4edc4c3d94477e5936694be3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b94310b5721e4731986c2f7bba5ccd44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "054a2a58d2fc451c9daed0574cfdeaaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading Fine-Tuned Model: /content/drive/MyDrive/TeQAS V5/XLM_R/final_xlmr_2.0_tel_3\n",
            "\n",
            "Loading Datasets: /content/drive/MyDrive/TeQAS V5/XLM_R/xlm_r_processed_telugu_squad_v2/val.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-626dcdce201f>:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset_val = Dataset.from_list(torch.load(DATASET_VAL))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Compare baseline vs fine-tuned model performance with validation data\n",
            "\n",
            "Evaluating Baseline Model...\n",
            "\n",
            "Evaluating Fine-Tuned Model...\n",
            "\n",
            "Baseline Model Metrics:\n",
            "{'em': 0.0908773562950896, 'f1': 0.11998905071550492, 'is_impossible_acc': 39.147869674185465}\n",
            "\n",
            "Fine-Tuned Model Metrics:\n",
            "{'em': 0.5551547591342797, 'f1': 0.6958794826060655, 'is_impossible_acc': 87.41854636591478}\n",
            "\n",
            "Loading Datasets: /content/drive/MyDrive/TeQAS V5/XLM_R/xlm_r_processed_telugu_squad_v2/test.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-626dcdce201f>:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset_test = Dataset.from_list(torch.load(DATASET_TEST))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Compare baseline vs fine-tuned model performance with test data\n",
            "\n",
            "Evaluating Baseline Model...\n",
            "\n",
            "Evaluating Fine-Tuned Model...\n",
            "\n",
            "Baseline Model Metrics:\n",
            "{'em': 0.19629356444772267, 'f1': 0.21642521164105785, 'is_impossible_acc': 39.309453471196456}\n",
            "\n",
            "Fine-Tuned Model Metrics:\n",
            "{'em': 0.6113774663470404, 'f1': 0.7065033575269639, 'is_impossible_acc': 82.20088626292467}\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# eval_baseline_vs_finetuned.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    XLMRobertaForQuestionAnswering,\n",
        "    XLMRobertaTokenizerFast,\n",
        "    BertForQuestionAnswering,\n",
        "    BertTokenizerFast\n",
        ")\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "##########################################\n",
        "# 1) Normalize Telugu text for evaluation\n",
        "##########################################\n",
        "def normalize_text_telugu(s):\n",
        "    \"\"\"Minimal normalization for Telugu text.\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = re.sub(r\"[^\\u0C00-\\u0C7Fa-zA-Z0-9\\s]\", \"\", s)  # Keep Telugu, English, numbers\n",
        "    s = s.lower()\n",
        "    s = \" \".join(s.split())  # Remove extra spaces\n",
        "    return s\n",
        "\n",
        "##########################################\n",
        "# 2) Cleaning Predictions\n",
        "##########################################\n",
        "def clean_prediction(text):\n",
        "    \"\"\"Remove unwanted characters and extra spaces.\"\"\"\n",
        "    return text.strip().replace(\"\\n\", \" \")\n",
        "\n",
        "##########################################\n",
        "# 3) Postprocessing for SQuAD v2.0\n",
        "##########################################\n",
        "def postprocess_qa_predictions_squad_v2(\n",
        "    examples, start_logits, end_logits, cls_index=0, null_score_diff_threshold=0.0\n",
        "):\n",
        "    \"\"\"Postprocess predictions by selecting the best span or no-answer option.\"\"\"\n",
        "    preds = {}\n",
        "    for i, ex in enumerate(examples):\n",
        "        context = ex[\"context\"]\n",
        "        offsets = ex[\"offset_mapping\"]\n",
        "        ex_id   = ex[\"id\"]\n",
        "\n",
        "        best_start, best_end, best_score = 0, 0, float(\"-inf\")\n",
        "        cls_score = start_logits[i][cls_index] + end_logits[i][cls_index]  # No-answer score\n",
        "\n",
        "        for start_idx in np.argsort(start_logits[i])[-10:]:\n",
        "            for end_idx in np.argsort(end_logits[i])[-10:]:\n",
        "                if end_idx < start_idx or (end_idx - start_idx + 1) > 100:\n",
        "                    continue\n",
        "                score = start_logits[i][start_idx] + end_logits[i][end_idx]\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_start = start_idx\n",
        "                    best_end = end_idx\n",
        "\n",
        "        if best_score - cls_score < null_score_diff_threshold:\n",
        "            preds[ex_id] = \"\"\n",
        "        else:\n",
        "            start_char = offsets[best_start][0]\n",
        "            end_char   = offsets[best_end][1]\n",
        "            preds[ex_id] = clean_prediction(context[start_char:end_char])\n",
        "\n",
        "    return preds\n",
        "\n",
        "##########################################\n",
        "# 4) Metrics: EM, F1, Is Impossible Accuracy\n",
        "##########################################\n",
        "def exact_match(pred, gold):\n",
        "    return 1.0 if normalize_text_telugu(pred) == normalize_text_telugu(gold) else 0.0\n",
        "\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_text_telugu(pred).split()\n",
        "    gold_tokens = normalize_text_telugu(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same = len(common)\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return float(pred_tokens == gold_tokens)\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "def compute_is_impossible_accuracy(examples, predictions):\n",
        "    \"\"\"Compute accuracy for unanswerable questions.\"\"\"\n",
        "    total, correct = 0, 0\n",
        "    for ex in examples:\n",
        "        gold_empty = ex[\"gold_text\"].strip() == \"\"\n",
        "        pred_empty = predictions.get(ex[\"id\"], \"\").strip() == \"\"\n",
        "        if gold_empty:\n",
        "            total += 1\n",
        "            if pred_empty:\n",
        "                correct += 1\n",
        "    return (correct / total) * 100.0 if total > 0 else 100.0\n",
        "\n",
        "##########################################\n",
        "# 5) Master Evaluation Function\n",
        "##########################################\n",
        "def evaluate_squad_v2(examples, start_logits, end_logits, null_score_diff_threshold=0.0):\n",
        "    predictions = postprocess_qa_predictions_squad_v2(examples, start_logits, end_logits, null_score_diff_threshold=null_score_diff_threshold)\n",
        "\n",
        "    metrics = {\"em\": 0.0, \"f1\": 0.0, \"is_impossible_acc\": 0.0}\n",
        "    total = len(examples)\n",
        "\n",
        "    for ex in examples:\n",
        "        pred = predictions.get(ex[\"id\"], \"\")\n",
        "        gold = ex[\"gold_text\"]\n",
        "        metrics[\"em\"] += exact_match(pred, gold)\n",
        "        metrics[\"f1\"] += f1_score(pred, gold)\n",
        "\n",
        "    metrics[\"em\"] /= total\n",
        "    metrics[\"f1\"] /= total\n",
        "    metrics[\"is_impossible_acc\"] = compute_is_impossible_accuracy(examples, predictions)\n",
        "\n",
        "    return metrics, predictions\n",
        "\n",
        "##########################################\n",
        "# 6) Baseline vs Fine-Tuned Evaluation\n",
        "##########################################\n",
        "def run_model_evaluation(model, tokenizer, dataset, device):\n",
        "    \"\"\"Evaluate the given model on the dataset.\"\"\"\n",
        "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=lambda x: {\n",
        "        \"input_ids\": torch.stack([torch.tensor(f[\"input_ids\"]) for f in x]).to(device),\n",
        "        \"attention_mask\": torch.stack([torch.tensor(f[\"attention_mask\"]) for f in x]).to(device)\n",
        "    })\n",
        "\n",
        "    all_start_logits, all_end_logits = [], []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            outputs = model(**batch)\n",
        "            all_start_logits.append(outputs.start_logits.cpu().numpy())\n",
        "            all_end_logits.append(outputs.end_logits.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(all_start_logits), np.concatenate(all_end_logits)\n",
        "\n",
        "def compare_models(base_model, fine_tuned_model, tokenizer, dataset, device, data_ = \"validation\"):\n",
        "    \"\"\"Compare baseline vs fine-tuned model performance.\"\"\"\n",
        "    print(f\"=> Compare baseline vs fine-tuned model performance with {data_} data\")\n",
        "    print(\"\\nEvaluating Baseline Model...\")\n",
        "    base_start_logits, base_end_logits = run_model_evaluation(base_model, tokenizer, dataset, device)\n",
        "    base_metrics, _ = evaluate_squad_v2(dataset, base_start_logits, base_end_logits)\n",
        "\n",
        "    print(\"\\nEvaluating Fine-Tuned Model...\")\n",
        "    fine_tuned_start_logits, fine_tuned_end_logits = run_model_evaluation(fine_tuned_model, tokenizer, dataset, device)\n",
        "    fine_tuned_metrics, _ = evaluate_squad_v2(dataset, fine_tuned_start_logits, fine_tuned_end_logits)\n",
        "\n",
        "    print(\"\\nBaseline Model Metrics:\")\n",
        "    print(base_metrics)\n",
        "\n",
        "    print(\"\\nFine-Tuned Model Metrics:\")\n",
        "    print(fine_tuned_metrics)\n",
        "\n",
        "##########################################\n",
        "# 7) Main Execution\n",
        "##########################################\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    BASE_DIR = \"/content/drive/MyDrive/TeQAS V5/XLM_R\"\n",
        "    BASELINE_MODEL = \"xlm-roberta-large\"  # Pretrained (not fine-tuned)\n",
        "    FINETUNED_MODEL_PATH = os.path.join(BASE_DIR, \"final_xlmr_2.0_tel_3\")\n",
        "    DATASET_VAL = os.path.join(BASE_DIR, \"xlm_r_processed_telugu_squad_v2/val.pt\")\n",
        "    DATASET_TEST = os.path.join(BASE_DIR, \"xlm_r_processed_telugu_squad_v2/test.pt\")\n",
        "\n",
        "    print(\"\\nLoading Baseline Model:\", BASELINE_MODEL)\n",
        "    base_model = XLMRobertaForQuestionAnswering.from_pretrained(BASELINE_MODEL).to(device)\n",
        "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(BASELINE_MODEL)\n",
        "\n",
        "    print(\"\\nLoading Fine-Tuned Model:\", FINETUNED_MODEL_PATH)\n",
        "    fine_tuned_model = XLMRobertaForQuestionAnswering.from_pretrained(FINETUNED_MODEL_PATH).to(device)\n",
        "\n",
        "    print(\"\\nLoading Datasets:\", DATASET_VAL)\n",
        "    dataset_val = Dataset.from_list(torch.load(DATASET_VAL))\n",
        "    compare_models(base_model, fine_tuned_model, tokenizer, dataset_val, device, data_ = \"validation\")\n",
        "\n",
        "    print(\"\\nLoading Datasets:\", DATASET_TEST)\n",
        "    dataset_test = Dataset.from_list(torch.load(DATASET_TEST))\n",
        "    compare_models(base_model, fine_tuned_model, tokenizer, dataset_test, device, data_ = \"test\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMneyWLf7HsA"
      },
      "source": [
        "\n",
        "##  MuRIL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvXpfqBij1QV"
      },
      "source": [
        "#### 锔 FIne-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLnjowPn8jB2"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# fine_tune_stage_muril.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "# Import the correct MuRIL-based BERT classes:\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForQuestionAnswering,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "\n",
        "import re\n",
        "\n",
        "# If you have the following definitions from prior code,\n",
        "# ensure they are in the same file or properly imported:\n",
        "from functools import partial\n",
        "\n",
        "##########################################\n",
        "# 1) Post-processing: Normalization & EM/F1\n",
        "##########################################\n",
        "def normalize_text(s):\n",
        "    def remove_articles(txt):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", txt)\n",
        "    def remove_punc(txt):\n",
        "        return re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
        "    def white_space_fix(txt):\n",
        "        return \" \".join(txt.split())\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def exact_match(pred, gold):\n",
        "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
        "\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    gold_tokens = normalize_text(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same = len(common)\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall    = num_same / len(gold_tokens)\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
        "    \"\"\"\n",
        "    Because doc_stride can cause more features than original questions,\n",
        "    we clamp iteration by min(len(examples), len(start_logits)).\n",
        "    \"\"\"\n",
        "    preds = {}\n",
        "    limit = min(len(examples), len(start_logits))\n",
        "    for i in range(limit):\n",
        "        ex       = examples[i]\n",
        "        offsets  = ex[\"offset_mapping\"]\n",
        "        context  = ex[\"context\"]\n",
        "        ex_id    = ex[\"id\"]\n",
        "\n",
        "        s_idx = int(np.argmax(start_logits[i]))\n",
        "        e_idx = int(np.argmax(end_logits[i]))\n",
        "        if e_idx < s_idx:\n",
        "            s_idx, e_idx = e_idx, s_idx\n",
        "\n",
        "        if s_idx >= len(offsets):\n",
        "            preds[ex_id] = \"\"\n",
        "            continue\n",
        "        if e_idx >= len(offsets):\n",
        "            e_idx = len(offsets)-1\n",
        "\n",
        "        start_char = offsets[s_idx][0]\n",
        "        end_char   = offsets[e_idx][1]\n",
        "        pred_text  = context[start_char:end_char]\n",
        "        preds[ex_id] = pred_text\n",
        "    return preds\n",
        "\n",
        "def compute_metrics(eval_preds, raw_dataset):\n",
        "    \"\"\"\n",
        "    eval_preds => (start_logits, end_logits)\n",
        "    raw_dataset => the original list of dict with 'gold_text', 'offset_mapping', etc.\n",
        "    We'll decode => measure EM/F1.\n",
        "    \"\"\"\n",
        "    (start_logits, end_logits) = eval_preds\n",
        "    if isinstance(start_logits, torch.Tensor):\n",
        "        start_logits = start_logits.cpu().numpy()\n",
        "    if isinstance(end_logits, torch.Tensor):\n",
        "        end_logits   = end_logits.cpu().numpy()\n",
        "\n",
        "    preds_dict = postprocess_qa_predictions(raw_dataset, start_logits, end_logits)\n",
        "\n",
        "    total_em, total_f1, count = 0.0, 0.0, 0\n",
        "    for ex in raw_dataset:\n",
        "        ex_id = ex[\"id\"]\n",
        "        pred  = preds_dict.get(ex_id, \"\")\n",
        "        gold  = ex[\"gold_text\"]\n",
        "        total_em += exact_match(pred, gold)\n",
        "        total_f1 += f1_score(pred, gold)\n",
        "        count    += 1\n",
        "\n",
        "    em_val = (total_em / count)*100.0\n",
        "    f1_val = (total_f1 / count)*100.0\n",
        "    return {\"em\": em_val, \"f1\": f1_val}\n",
        "\n",
        "##########################################\n",
        "# 2) Collate => Provide start/end positions\n",
        "##########################################\n",
        "def squad_collate(features):\n",
        "    \"\"\"\n",
        "    Convert any lists => Tensors if needed, then stack.\n",
        "    Expects each feature to have \"input_ids\", \"attention_mask\",\n",
        "    \"start_positions\", \"end_positions\".\n",
        "    \"\"\"\n",
        "    for f in features:\n",
        "        if not isinstance(f[\"input_ids\"], torch.Tensor):\n",
        "            f[\"input_ids\"] = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
        "        if not isinstance(f[\"attention_mask\"], torch.Tensor):\n",
        "            f[\"attention_mask\"] = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
        "        if not isinstance(f[\"start_positions\"], torch.Tensor):\n",
        "            f[\"start_positions\"] = torch.tensor(f[\"start_positions\"], dtype=torch.long)\n",
        "        if not isinstance(f[\"end_positions\"], torch.Tensor):\n",
        "            f[\"end_positions\"] = torch.tensor(f[\"end_positions\"], dtype=torch.long)\n",
        "\n",
        "    input_ids      = torch.stack([f[\"input_ids\"] for f in features])\n",
        "    attention_mask = torch.stack([f[\"attention_mask\"] for f in features])\n",
        "    start_positions= torch.stack([f[\"start_positions\"] for f in features])\n",
        "    end_positions  = torch.stack([f[\"end_positions\"] for f in features])\n",
        "\n",
        "    return {\n",
        "        \"input_ids\":       input_ids,\n",
        "        \"attention_mask\":  attention_mask,\n",
        "        \"start_positions\": start_positions,\n",
        "        \"end_positions\":   end_positions,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMI3DXc-7RY2"
      },
      "source": [
        "##### English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-5PBA53huih"
      },
      "outputs": [],
      "source": [
        "##########################################\n",
        "# 3) 2-Stage Finetuning (English => Telugu)\n",
        "##########################################\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Base directory setup\n",
        "BASE_DIR = \"/content/drive/MyDrive/Te-QAS V07/MuRIL\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"muril_processed_english_squad_v2\")\n",
        "MODEL_NAME = \"google/muril-large-cased\"\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"checkpoints_muril_eng_1\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_DIR, \"final_muril_2.0_eng_1\")\n",
        "\n",
        "# Wandb configuration\n",
        "WANDB_PROJECT = \"Te-QAS 2.0 V-07\"\n",
        "RUN_NAME = \"muril_eng_run1\"\n",
        "\n",
        "def main():\n",
        "    # Stage 1: English Data\n",
        "    import sys\n",
        "\n",
        "    # Example:\n",
        "    train_list = torch.load(os.path.join(DATA_DIR, \"train.pt\"))\n",
        "    val_list   = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n",
        "    test_list   = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n",
        "\n",
        "    wandb.init(project=WANDB_PROJECT, name=RUN_NAME)\n",
        "\n",
        "    train_dataset = Dataset.from_list(train_list)\n",
        "    val_dataset   = Dataset.from_list(val_list)\n",
        "    test_dataset   = Dataset.from_list(test_list)\n",
        "\n",
        "    # 1) Use MuRIL's BERT-based classes\n",
        "\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "    model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # 2) Training args => 1 epoch\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir= OUTPUT_DIR,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"logs_muril\",\n",
        "        logging_steps=100,\n",
        "        report_to=\"wandb\"\n",
        "    )\n",
        "\n",
        "    from transformers.trainer_utils import EvalPrediction\n",
        "    def hf_compute_metrics_stage1(p: EvalPrediction):\n",
        "        return compute_metrics(p.predictions, val_list)\n",
        "\n",
        "    # Build trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=squad_collate,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=hf_compute_metrics_stage1\n",
        "    )\n",
        "\n",
        "    print(\"\\n======== STAGE 1: Fine-tuning on English data for 1 epoch ========\")\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(FINAL_MODEL_DIR)\n",
        "    print(f\"Done! Model saved to {FINAL_MODEL_DIR}\")\n",
        "\n",
        "    # # Evaluate on validation set\n",
        "    # print(\"\\n======== Evaluating on validation set ========\")\n",
        "    # final_val_metrics = trainer.evaluate()\n",
        "    # print(\"Stage 1 final val metrics:\", final_val_metrics)\n",
        "\n",
        "    # # Evaluate on test set\n",
        "    # print(\"\\n======== Evaluating on test set ========\")\n",
        "    # final_test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "    # print(\"Stage 1 final test metrics:\", final_test_metrics)\n",
        "    # print(\"Stage 1 done.\")\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1026,
          "referenced_widgets": [
            "fc65753410804c71ab3c70671cd4e21f",
            "59741346440543d3b350d0917bae2100",
            "b74d3ebc84254cedafc06fc68af74603",
            "229c26e8c11d4f509a30169722c45995",
            "39d0788d07c44c098707a278011074a6",
            "551cfd40a3ae4f919c01e961a2d62a39",
            "8c678547358a4e8db583e078f8e67d2d",
            "82f62d3650f04d2eb45a2818aec31154",
            "3a1aca689b6e4989b7c456a8d5960385",
            "abe91c30a89a4c8fa10f9cb15f155a87",
            "45d697da72a8414db99314c7e4545809"
          ]
        },
        "id": "8m5u8-VgwYdh",
        "outputId": "113be26a-6e2a-4ca9-f594-fdf5098dadb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading validation data ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-5cb64bdf12e7>:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  val_data_list = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading test data ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-5cb64bdf12e7>:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_data_list = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n",
            "<ipython-input-3-5cb64bdf12e7>:212: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running predictions on validation set (size=8605)...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250129_185519-uzbiswwd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/huggingface/runs/uzbiswwd' target=\"_blank\">inference_outputs_muril</a></strong> to <a href='https://wandb.ai/santhosh-rishi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/santhosh-rishi/huggingface' target=\"_blank\">https://wandb.ai/santhosh-rishi/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/santhosh-rishi/huggingface/runs/uzbiswwd' target=\"_blank\">https://wandb.ai/santhosh-rishi/huggingface/runs/uzbiswwd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing all metrics for validation set...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc65753410804c71ab3c70671cd4e21f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========= VALIDATION RESULTS =========\n",
            "em: 67.88\n",
            "f1: 79.08\n",
            "bleu: 32.16\n",
            "rouge1_f: 60.98\n",
            "rouge2_f: 38.82\n",
            "rougeL_f: 60.95\n",
            "==============================\n",
            "\n",
            "Running predictions on test set (size=11947)...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='98' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 98/374 04:35 < 13:05, 0.35 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing all metrics for test set...\n",
            "\n",
            "========= TEST RESULTS =========\n",
            "em: 67.75\n",
            "f1: 75.28\n",
            "bleu: 15.27\n",
            "rouge1_f: 38.45\n",
            "rouge2_f: 24.25\n",
            "rougeL_f: 38.39\n",
            "========================\n",
            "\n",
            "========= COMBINED SUMMARY =========\n",
            "Metric      | Validation | Test\n",
            "-----------------------------------\n",
            "em          |     67.88 |  67.75\n",
            "f1          |     79.08 |  75.28\n",
            "bleu        |     32.16 |  15.27\n",
            "rouge1_f    |     60.98 |  38.45\n",
            "rouge2_f    |     38.82 |  24.25\n",
            "rougeL_f    |     60.95 |  38.39\n",
            "===================================\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# test_inference_muril.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForQuestionAnswering,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import nltk  # for BLEU\n",
        "from evaluate import load as load_metric  # for ROUGE - Fixed import\n",
        "\n",
        "##########################################\n",
        "# 1) Basic text normalization + EM/F1\n",
        "##########################################\n",
        "def normalize_text(s):\n",
        "    def remove_articles(txt):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", txt)\n",
        "    def remove_punc(txt):\n",
        "        return re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
        "    def white_space_fix(txt):\n",
        "        return \" \".join(txt.split())\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def exact_match(pred, gold):\n",
        "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
        "\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    gold_tokens = normalize_text(gold).split()\n",
        "    common      = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same    = len(common)\n",
        "    if len(pred_tokens)==0 or len(gold_tokens)==0:\n",
        "        return 1.0 if pred_tokens==gold_tokens else 0.0\n",
        "    prec = num_same / len(pred_tokens)\n",
        "    rec  = num_same / len(gold_tokens)\n",
        "    if prec+rec==0:\n",
        "        return 0.0\n",
        "    return 2*prec*rec/(prec+rec)\n",
        "\n",
        "##########################################\n",
        "# 2) Decoding predictions => text answers\n",
        "##########################################\n",
        "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
        "    preds = {}\n",
        "    limit = min(len(examples), len(start_logits))\n",
        "    for i in range(limit):\n",
        "        ex       = examples[i]\n",
        "        offsets  = ex[\"offset_mapping\"]\n",
        "        context  = ex[\"context\"]\n",
        "        ex_id    = ex[\"id\"]\n",
        "\n",
        "        s_idx = int(np.argmax(start_logits[i]))\n",
        "        e_idx = int(np.argmax(end_logits[i]))\n",
        "        if e_idx < s_idx:\n",
        "            s_idx, e_idx = e_idx, s_idx\n",
        "\n",
        "        if s_idx >= len(offsets):\n",
        "            preds[ex_id] = \"\"\n",
        "            continue\n",
        "        if e_idx >= len(offsets):\n",
        "            e_idx = len(offsets)-1\n",
        "\n",
        "        start_char = offsets[s_idx][0]\n",
        "        end_char   = offsets[e_idx][1]\n",
        "        pred_text  = context[start_char:end_char]\n",
        "        preds[ex_id] = pred_text\n",
        "    return preds\n",
        "\n",
        "##########################################\n",
        "# 3) Evaluate => EM, F1, BLEU, ROUGE\n",
        "##########################################\n",
        "def compute_all_metrics(examples, start_logits, end_logits):\n",
        "    # Convert Tensors => numpy\n",
        "    if isinstance(start_logits, torch.Tensor):\n",
        "        start_logits = start_logits.cpu().numpy()\n",
        "    if isinstance(end_logits, torch.Tensor):\n",
        "        end_logits   = end_logits.cpu().numpy()\n",
        "\n",
        "    preds_dict = postprocess_qa_predictions(examples, start_logits, end_logits)\n",
        "\n",
        "    total_em, total_f1, count = 0.0, 0.0, 0\n",
        "    from nltk.translate.bleu_score import corpus_bleu\n",
        "    corpus_refs = []\n",
        "    corpus_hyps = []\n",
        "\n",
        "    pred_texts = []\n",
        "    gold_texts = []\n",
        "\n",
        "    for ex in examples:\n",
        "        ex_id = ex[\"id\"]\n",
        "        gold  = ex[\"gold_text\"]\n",
        "        pred  = preds_dict.get(ex_id, \"\")\n",
        "\n",
        "        # SQuAD EM/F1\n",
        "        total_em += exact_match(pred, gold)\n",
        "        total_f1 += f1_score(pred, gold)\n",
        "        count    += 1\n",
        "\n",
        "        # For BLEU/ROUGE\n",
        "        pred_texts.append(pred)\n",
        "        gold_texts.append(gold)\n",
        "\n",
        "        # corpus BLEU expects tokenized lists\n",
        "        gold_toks = normalize_text(gold).split()\n",
        "        pred_toks = normalize_text(pred).split()\n",
        "        corpus_refs.append([gold_toks])  # list of list\n",
        "        corpus_hyps.append(pred_toks)\n",
        "\n",
        "    # Final EM/F1\n",
        "    em_val = (total_em / count)*100.0\n",
        "    f1_val = (total_f1 / count)*100.0\n",
        "\n",
        "    # BLEU (corpus)\n",
        "    bleu_val = corpus_bleu(corpus_refs, corpus_hyps)*100.0\n",
        "\n",
        "    # ROUGE - Fixed metric calculation\n",
        "    rouge_metric = load_metric(\"rouge\")\n",
        "    results_rouge = rouge_metric.compute(\n",
        "        predictions=pred_texts,\n",
        "        references=gold_texts\n",
        "    )\n",
        "    r1_f = float(results_rouge['rouge1'])*100.0\n",
        "    r2_f = float(results_rouge['rouge2'])*100.0\n",
        "    rl_f = float(results_rouge['rougeL'])*100.0\n",
        "\n",
        "    return {\n",
        "        \"em\": em_val,\n",
        "        \"f1\": f1_val,\n",
        "        \"bleu\": bleu_val,\n",
        "        \"rouge1_f\": r1_f,\n",
        "        \"rouge2_f\": r2_f,\n",
        "        \"rougeL_f\": rl_f\n",
        "    }\n",
        "\n",
        "##########################################\n",
        "# 4) Custom collator => BERT\n",
        "##########################################\n",
        "def squad_collate(features):\n",
        "    for f in features:\n",
        "        if not isinstance(f[\"input_ids\"], torch.Tensor):\n",
        "            f[\"input_ids\"] = torch.tensor(f[\"input_ids\"], dtype=torch.long)\n",
        "        if not isinstance(f[\"attention_mask\"], torch.Tensor):\n",
        "            f[\"attention_mask\"] = torch.tensor(f[\"attention_mask\"], dtype=torch.long)\n",
        "        if not isinstance(f[\"start_positions\"], torch.Tensor):\n",
        "            f[\"start_positions\"] = torch.tensor(f[\"start_positions\"], dtype=torch.long)\n",
        "        if not isinstance(f[\"end_positions\"], torch.Tensor):\n",
        "            f[\"end_positions\"] = torch.tensor(f[\"end_positions\"], dtype=torch.long)\n",
        "\n",
        "    input_ids      = torch.stack([f[\"input_ids\"] for f in features])\n",
        "    attention_mask = torch.stack([f[\"attention_mask\"] for f in features])\n",
        "    start_positions= torch.stack([f[\"start_positions\"] for f in features])\n",
        "    end_positions  = torch.stack([f[\"end_positions\"] for f in features])\n",
        "\n",
        "    return {\n",
        "        \"input_ids\":       input_ids,\n",
        "        \"attention_mask\":  attention_mask,\n",
        "        \"start_positions\": start_positions,\n",
        "        \"end_positions\":   end_positions,\n",
        "    }\n",
        "\n",
        "##########################################\n",
        "# 5) Main function\n",
        "##########################################\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/Te-QAS V07/MuRIL\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"muril_processed_english_squad_v2\")\n",
        "MODEL_DIR = os.path.join(BASE_DIR, \"final_muril_2.0_eng_1\")\n",
        "\n",
        "def evaluate_data(trainer, data_list, data_name=\"\"):\n",
        "    print(f\"\\nRunning predictions on {data_name} set (size={len(data_list)})...\")\n",
        "    dataset = Dataset.from_list(data_list)\n",
        "    preds_out = trainer.predict(dataset)\n",
        "    start_logits, end_logits = preds_out.predictions\n",
        "\n",
        "    print(f\"Computing all metrics for {data_name} set...\")\n",
        "    scores = compute_all_metrics(data_list, start_logits, end_logits)\n",
        "\n",
        "    print(f\"\\n========= {data_name.upper()} RESULTS =========\")\n",
        "    for k,v in scores.items():\n",
        "        print(f\"{k}: {v:.2f}\")\n",
        "    print(\"=\" * (len(data_name) + 20))\n",
        "\n",
        "    return scores\n",
        "\n",
        "def main():\n",
        "    # Default values instead of argparser\n",
        "    batch_size = 32\n",
        "\n",
        "    # 1) Load validation and test data\n",
        "    print(\"Loading validation data ...\")\n",
        "    val_data_list = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n",
        "\n",
        "    print(\"Loading test data ...\")\n",
        "    test_data_list = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n",
        "\n",
        "    # 2) Load MuRIL BERT QA model\n",
        "    model = BertForQuestionAnswering.from_pretrained(MODEL_DIR)\n",
        "\n",
        "    # 3) Training args for inference\n",
        "    eval_args = TrainingArguments(\n",
        "        output_dir=\"inference_outputs_muril\",\n",
        "        per_device_eval_batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # 4) Build Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=eval_args,\n",
        "        data_collator=squad_collate,\n",
        "        tokenizer=None  # not necessary if no dynamic padding\n",
        "    )\n",
        "\n",
        "    # 5) Evaluate on validation set\n",
        "    val_scores = evaluate_data(trainer, val_data_list, \"validation\")\n",
        "\n",
        "    # 6) Evaluate on test set\n",
        "    test_scores = evaluate_data(trainer, test_data_list, \"test\")\n",
        "\n",
        "    # 7) Print combined summary\n",
        "    print(\"\\n========= COMBINED SUMMARY =========\")\n",
        "    print(\"Metric      | Validation | Test\")\n",
        "    print(\"-\" * 35)\n",
        "    for metric in val_scores.keys():\n",
        "        print(f\"{metric:<11} | {val_scores[metric]:>9.2f} | {test_scores[metric]:>6.2f}\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR_J0wn58x7A"
      },
      "source": [
        "##### 喟む喟侧喟啾"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "53UhtvxwtVcH",
        "outputId": "8214b522-16d5-4501-e1cb-ae47ce8e2952"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-8b3a154eeab6>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_list = torch.load(os.path.join(DATA_DIR, \"train.pt\"))\n",
            "<ipython-input-3-8b3a154eeab6>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  val_list   = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n",
            "<ipython-input-3-8b3a154eeab6>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_list   = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the Fine-Tuned Model from /content/drive/MyDrive/Te-QAS V07/MuRIL/final_muril_2.0_eng_1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-3-8b3a154eeab6>:59: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== STAGE 2: Fine-tuning on Telugu data for 3 epochs ========\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9466' max='10284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 9466/10284 5:08:31 < 26:40, 0.51 it/s, Epoch 2.76/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Em</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.752400</td>\n",
              "      <td>1.723241</td>\n",
              "      <td>56.928447</td>\n",
              "      <td>71.051366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.449300</td>\n",
              "      <td>1.679375</td>\n",
              "      <td>56.695753</td>\n",
              "      <td>71.429071</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10284' max='10284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10284/10284 5:38:14, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Em</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.752400</td>\n",
              "      <td>1.723241</td>\n",
              "      <td>56.928447</td>\n",
              "      <td>71.051366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.449300</td>\n",
              "      <td>1.679375</td>\n",
              "      <td>56.695753</td>\n",
              "      <td>71.429071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.198200</td>\n",
              "      <td>1.779184</td>\n",
              "      <td>56.649215</td>\n",
              "      <td>71.294583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Model saved to /content/drive/MyDrive/Te-QAS V07/MuRIL/final_muril_2.0_tel_3\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/em</td><td></td></tr><tr><td>eval/f1</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/em</td><td>56.64921</td></tr><tr><td>eval/f1</td><td>71.29458</td></tr><tr><td>eval/loss</td><td>1.77918</td></tr><tr><td>eval/runtime</td><td>166.0303</td></tr><tr><td>eval/samples_per_second</td><td>51.768</td></tr><tr><td>eval/steps_per_second</td><td>1.62</td></tr><tr><td>total_flos</td><td>3.055817486897971e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>10284</td></tr><tr><td>train/grad_norm</td><td>17.94786</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1982</td></tr><tr><td>train_loss</td><td>1.52642</td></tr><tr><td>train_runtime</td><td>20297.9045</td></tr><tr><td>train_samples_per_second</td><td>16.211</td></tr><tr><td>train_steps_per_second</td><td>0.507</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">muril_tel_run3</strong> at: <a href='https://wandb.ai/santhosh-rishi/Te-QAS%202.0%20V-07/runs/7caegvm9' target=\"_blank\">https://wandb.ai/santhosh-rishi/Te-QAS%202.0%20V-07/runs/7caegvm9</a><br> View project at: <a href='https://wandb.ai/santhosh-rishi/Te-QAS%202.0%20V-07' target=\"_blank\">https://wandb.ai/santhosh-rishi/Te-QAS%202.0%20V-07</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250127_120027-7caegvm9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "##########################################\n",
        "# 3) 2-Stage Finetuning (English => Telugu)\n",
        "##########################################\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Base directory setup\n",
        "BASE_DIR = \"/content/drive/MyDrive/Te-QAS V07/MuRIL\"\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"muril_processed_telugu_squad_v2\")\n",
        "MODEL_NAME = os.path.join(BASE_DIR, \"final_muril_2.0_eng_1\")\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"checkpoints_muril_tel_3\")\n",
        "FINAL_MODEL_DIR = os.path.join(BASE_DIR, \"final_muril_2.0_tel_3\")\n",
        "\n",
        "# Wandb configuration\n",
        "WANDB_PROJECT = \"Te-QAS 2.0 V-07\"\n",
        "RUN_NAME = \"muril_tel_run3\"\n",
        "\n",
        "def main():\n",
        "    # Stage 2: Telugu Data\n",
        "    import sys\n",
        "\n",
        "    # Example:\n",
        "    train_list = torch.load(os.path.join(DATA_DIR, \"train.pt\"))\n",
        "    val_list   = torch.load(os.path.join(DATA_DIR, \"val.pt\"))\n",
        "    test_list   = torch.load(os.path.join(DATA_DIR, \"test.pt\"))\n",
        "\n",
        "    wandb.init(project=WANDB_PROJECT, name=RUN_NAME)\n",
        "\n",
        "    train_dataset = Dataset.from_list(train_list)\n",
        "    val_dataset   = Dataset.from_list(val_list)\n",
        "    test_dataset   = Dataset.from_list(test_list)\n",
        "\n",
        "    # 1) Use Pre-Fine-tuned English Model\n",
        "\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "    model = BertForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
        "    print(f\"Loading the Fine-Tuned Model from {MODEL_NAME}\")\n",
        "\n",
        "    # 2) Training args => 3 epochs\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir= OUTPUT_DIR,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"logs_muril\",\n",
        "        logging_steps=100,\n",
        "        report_to=\"wandb\"\n",
        "    )\n",
        "\n",
        "    from transformers.trainer_utils import EvalPrediction\n",
        "    def hf_compute_metrics_stage1(p: EvalPrediction):\n",
        "        return compute_metrics(p.predictions, val_list)\n",
        "\n",
        "    # Build trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=squad_collate,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=hf_compute_metrics_stage1\n",
        "    )\n",
        "\n",
        "    print(\"\\n======== STAGE 2: Fine-tuning on Telugu data for 3 epochs ========\")\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(FINAL_MODEL_DIR)\n",
        "    print(f\"Done! Model saved to {FINAL_MODEL_DIR}\")\n",
        "\n",
        "    # # Evaluate on validation set\n",
        "    # print(\"\\n======== Evaluating on validation set ========\")\n",
        "    # final_val_metrics = trainer.evaluate()\n",
        "    # print(\"Stage 1 final val metrics:\", final_val_metrics)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUxlZXkA1TyG"
      },
      "source": [
        "#### 锔 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXQ2HW41lzZr",
        "outputId": "d4a21b49-e199-404d-8cec-931bfbcb4f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "\n",
            "Loading final MuRIL model from: /content/drive/MyDrive/Te-QAS V07/MuRIL/final_muril_2.0_tel_3\n",
            "Loading tokenizer from: /content/drive/MyDrive/Te-QAS V07/MuRIL/final_muril_2.0_tel_3\n",
            "Loading [val] data from: /content/drive/MyDrive/Te-QAS V07/MuRIL/muril_processed_telugu_squad_v2/val.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-38b2c7320bcd>:285: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_list = torch.load(data_file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running inference on val set... (num_examples=8595)\n",
            "\n",
            "[VAL SET METRICS]:\n",
            "  em: 57.08\n",
            "  f1: 71.84\n",
            "  bleu_unigram: 49.57\n",
            "  bleu_bigram: 37.35\n",
            "  rouge1: 14.75\n",
            "  rouge2: 1.69\n",
            "  rougeL: 14.75\n",
            "  is_impossible_acc: 80.80\n",
            "--------------------------------------------------\n",
            "Loading [test] data from: /content/drive/MyDrive/Te-QAS V07/MuRIL/muril_processed_telugu_squad_v2/test.pt\n",
            "Running inference on test set... (num_examples=10846)\n",
            "\n",
            "[TEST SET METRICS]:\n",
            "  em: 58.94\n",
            "  f1: 69.90\n",
            "  bleu_unigram: 29.40\n",
            "  bleu_bigram: 23.21\n",
            "  rouge1: 8.00\n",
            "  rouge2: 0.93\n",
            "  rougeL: 8.00\n",
            "  is_impossible_acc: 75.00\n",
            "--------------------------------------------------\n",
            "\n",
            "Evaluation script completed.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# eval_squad_v2_muril.py\n",
        "#\n",
        "# A separate evaluation script for your MuRIL QA model (SQuAD v2.0 style).\n",
        "# Adapts the same logic you used previously with XLM-R, but now for MuRIL.\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    BertForQuestionAnswering,\n",
        "    BertTokenizerFast\n",
        ")\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import re\n",
        "\n",
        "##############################################################################\n",
        "# 1) Telugu-Specific Normalization\n",
        "##############################################################################\n",
        "def normalize_text_telugu(s):\n",
        "    \"\"\"\n",
        "    Minimal Telugu-oriented normalization:\n",
        "      - Removes extra punctuation\n",
        "      - Lowercases any English letters\n",
        "      - Strips extra whitespace\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    # Remove everything not Telugu or alphanumeric. Adjust for your corpus if needed.\n",
        "    s = re.sub(r\"[^\\u0C00-\\u0C7Fa-zA-Z0-9\\s]\", \"\", s)\n",
        "    # Lowercase (affects only English letters)\n",
        "    s = s.lower()\n",
        "    # Remove multiple spaces\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "##############################################################################\n",
        "# 2) Basic Cleaning for Predicted Spans\n",
        "##############################################################################\n",
        "def clean_prediction(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "##############################################################################\n",
        "# 3) No-Answer Post-processing for SQuAD v2.0\n",
        "##############################################################################\n",
        "def postprocess_qa_predictions_squad_v2(\n",
        "    examples,\n",
        "    start_logits,\n",
        "    end_logits,\n",
        "    cls_index=0,\n",
        "    max_answer_length=100,\n",
        "    n_best_size=20,\n",
        "    null_score_diff_threshold=0.0\n",
        "):\n",
        "    \"\"\"\n",
        "    - examples: list of dicts with \"id\", \"context\", \"offset_mapping\", etc.\n",
        "    - start_logits[i], end_logits[i]: arrays of length [sequence_length].\n",
        "    - cls_index: index for [CLS] token logit (for MuRIL, typically token_id=101 is [CLS],\n",
        "      but in practice the position in the input is 0).\n",
        "    - null_score_diff_threshold: threshold for \"no answer\" decision.\n",
        "    \"\"\"\n",
        "    preds = {}\n",
        "\n",
        "    for i, ex in enumerate(examples):\n",
        "        context = ex[\"context\"]\n",
        "        offsets = ex[\"offset_mapping\"]\n",
        "        ex_id   = ex[\"id\"]\n",
        "\n",
        "        # 1) Find best span\n",
        "        best_score = float('-inf')\n",
        "        best_start, best_end = 0, 0\n",
        "\n",
        "        start_idxs = np.argsort(start_logits[i])[-n_best_size:].tolist()\n",
        "        end_idxs   = np.argsort(end_logits[i])[-n_best_size:].tolist()\n",
        "\n",
        "        for st in start_idxs:\n",
        "            for en in end_idxs:\n",
        "                if en < st:\n",
        "                    continue\n",
        "                if (en - st + 1) > max_answer_length:\n",
        "                    continue\n",
        "                if st >= len(offsets) or en >= len(offsets):\n",
        "                    continue\n",
        "\n",
        "                span_score = start_logits[i][st] + end_logits[i][en]\n",
        "                if span_score > best_score:\n",
        "                    best_score = span_score\n",
        "                    best_start = st\n",
        "                    best_end   = en\n",
        "\n",
        "        # 2) Calculate no-answer score from [CLS] token\n",
        "        cls_score = start_logits[i][cls_index] + end_logits[i][cls_index]\n",
        "\n",
        "        # 3) Decide if no-answer\n",
        "        score_diff = best_score - cls_score\n",
        "        if score_diff < null_score_diff_threshold:\n",
        "            preds[ex_id] = \"\"\n",
        "        else:\n",
        "            start_char = offsets[best_start][0]\n",
        "            end_char   = offsets[best_end][1]\n",
        "            text_span  = context[start_char:end_char]\n",
        "            preds[ex_id] = clean_prediction(text_span)\n",
        "\n",
        "    return preds\n",
        "\n",
        "##############################################################################\n",
        "# 4) Metrics: EM, F1, BLEU, ROUGE, plus is_impossible accuracy\n",
        "##############################################################################\n",
        "def exact_match(pred, gold):\n",
        "    return 1.0 if normalize_text_telugu(pred) == normalize_text_telugu(gold) else 0.0\n",
        "\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_text_telugu(pred).split()\n",
        "    gold_tokens = normalize_text_telugu(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same = len(common)\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return float(pred_tokens == gold_tokens)\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall    = num_same / len(gold_tokens)\n",
        "    if (precision + recall) == 0:\n",
        "        return 0.0\n",
        "    return (2.0 * precision * recall) / (precision + recall)\n",
        "\n",
        "def compute_bleu(pred, gold):\n",
        "    if not pred or not gold:\n",
        "        return {\"unigram\": 0.0, \"bigram\": 0.0}\n",
        "    pred_tokens = normalize_text_telugu(pred).split()\n",
        "    gold_tokens = normalize_text_telugu(gold).split()\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    unigram = sentence_bleu(\n",
        "        [gold_tokens],\n",
        "        pred_tokens,\n",
        "        weights=(1, 0, 0, 0),\n",
        "        smoothing_function=smoothing\n",
        "    )\n",
        "    bigram  = sentence_bleu(\n",
        "        [gold_tokens],\n",
        "        pred_tokens,\n",
        "        weights=(0.5, 0.5, 0, 0),\n",
        "        smoothing_function=smoothing\n",
        "    )\n",
        "    return {\"unigram\": unigram, \"bigram\": bigram}\n",
        "\n",
        "def compute_rouge(pred, gold):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=False)\n",
        "    if not pred or not gold:\n",
        "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
        "    pred_clean = normalize_text_telugu(pred)\n",
        "    gold_clean = normalize_text_telugu(gold)\n",
        "    scores = scorer.score(pred_clean, gold_clean)\n",
        "    return {\n",
        "        \"rouge1\": scores[\"rouge1\"].fmeasure,\n",
        "        \"rouge2\": scores[\"rouge2\"].fmeasure,\n",
        "        \"rougeL\": scores[\"rougeL\"].fmeasure\n",
        "    }\n",
        "\n",
        "def compute_is_impossible_accuracy(examples, predictions):\n",
        "    \"\"\"\n",
        "    For each example, if gold_text is empty => gold is_impossible.\n",
        "    We check if predicted text is also empty => predicted is_impossible.\n",
        "    Then compute accuracy over *only* the is_impossible subset.\n",
        "    \"\"\"\n",
        "    total_impossible = 0\n",
        "    correct_impossible = 0\n",
        "    for ex in examples:\n",
        "        gold = ex.get(\"gold_text\", \"\")\n",
        "        gold_impossible = (gold.strip() == \"\")\n",
        "        if gold_impossible:\n",
        "            total_impossible += 1\n",
        "            pred_text = predictions.get(ex[\"id\"], \"\")\n",
        "            if pred_text.strip() == \"\":\n",
        "                correct_impossible += 1\n",
        "\n",
        "    if total_impossible == 0:\n",
        "        return 100.0  # or 0.0, depending on your convention\n",
        "    return (correct_impossible / total_impossible) * 100.0\n",
        "\n",
        "##############################################################################\n",
        "# 5) Master Evaluation Function\n",
        "##############################################################################\n",
        "def evaluate_squad_v2(examples, start_logits, end_logits, null_score_diff_threshold=0.0):\n",
        "    \"\"\"\n",
        "    Returns a dict with EM, F1, BLEU, ROUGE, plus 'is_impossible_acc'.\n",
        "    \"\"\"\n",
        "    predictions = postprocess_qa_predictions_squad_v2(\n",
        "        examples,\n",
        "        start_logits,\n",
        "        end_logits,\n",
        "        cls_index=0,  # For MuRIL, position 0 is typically [CLS]\n",
        "        null_score_diff_threshold=null_score_diff_threshold\n",
        "    )\n",
        "\n",
        "    total = len(examples)\n",
        "    metrics = {\n",
        "        \"em\": 0.0,\n",
        "        \"f1\": 0.0,\n",
        "        \"bleu_unigram\": 0.0,\n",
        "        \"bleu_bigram\": 0.0,\n",
        "        \"rouge1\": 0.0,\n",
        "        \"rouge2\": 0.0,\n",
        "        \"rougeL\": 0.0\n",
        "    }\n",
        "\n",
        "    for ex in examples:\n",
        "        gold = ex.get(\"gold_text\", \"\")\n",
        "        pred = predictions.get(ex[\"id\"], \"\")\n",
        "\n",
        "        metrics[\"em\"] += exact_match(pred, gold)\n",
        "        metrics[\"f1\"] += f1_score(pred, gold)\n",
        "\n",
        "        bleu_scores = compute_bleu(pred, gold)\n",
        "        metrics[\"bleu_unigram\"] += bleu_scores[\"unigram\"]\n",
        "        metrics[\"bleu_bigram\"]  += bleu_scores[\"bigram\"]\n",
        "\n",
        "        rouge_scores = compute_rouge(pred, gold)\n",
        "        metrics[\"rouge1\"] += rouge_scores[\"rouge1\"]\n",
        "        metrics[\"rouge2\"] += rouge_scores[\"rouge2\"]\n",
        "        metrics[\"rougeL\"] += rouge_scores[\"rougeL\"]\n",
        "\n",
        "    # Average the main metrics\n",
        "    for key in metrics:\n",
        "        metrics[key] = (metrics[key] / total) * 100.0\n",
        "\n",
        "    # Calculate is_impossible accuracy\n",
        "    is_imp_acc = compute_is_impossible_accuracy(examples, predictions)\n",
        "    metrics[\"is_impossible_acc\"] = is_imp_acc\n",
        "\n",
        "    return metrics, predictions\n",
        "\n",
        "##############################################################################\n",
        "# 6) Minimal Collate for Eval\n",
        "##############################################################################\n",
        "def squad_collate_eval(features):\n",
        "    \"\"\"\n",
        "    For evaluation, we only need input_ids & attention_mask for the model.\n",
        "    Convert lists to Tensors.\n",
        "    \"\"\"\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for f in features:\n",
        "        input_ids.append(torch.tensor(f[\"input_ids\"], dtype=torch.long))\n",
        "        attention_masks.append(torch.tensor(f[\"attention_mask\"], dtype=torch.long))\n",
        "    return {\n",
        "        \"input_ids\": torch.stack(input_ids, dim=0),\n",
        "        \"attention_mask\": torch.stack(attention_masks, dim=0)\n",
        "    }\n",
        "\n",
        "##############################################################################\n",
        "# 7) Main Evaluation Code\n",
        "##############################################################################\n",
        "def main():\n",
        "    # Adjust these paths for your environment:\n",
        "    MODEL_DIR = \"/content/drive/MyDrive/Te-QAS V07/MuRIL/final_muril_2.0_tel_3\"\n",
        "    DATA_DIR  = \"/content/drive/MyDrive/Te-QAS V07/MuRIL/muril_processed_telugu_squad_v2\"\n",
        "    VAL_FILE  = os.path.join(DATA_DIR, \"val.pt\")\n",
        "    TEST_FILE = os.path.join(DATA_DIR, \"test.pt\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # Load model\n",
        "    print(\"\\nLoading final MuRIL model from:\", MODEL_DIR)\n",
        "    model = BertForQuestionAnswering.from_pretrained(MODEL_DIR)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Load tokenizer (optional for debugging or postprocessing, etc.)\n",
        "    print(\"Loading tokenizer from:\", MODEL_DIR)\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_DIR)\n",
        "\n",
        "    def run_eval(data_file, split_name=\"val\"):\n",
        "        if not os.path.exists(data_file):\n",
        "            print(f\"[{split_name}] file not found: {data_file}\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Loading [{split_name}] data from:\", data_file)\n",
        "        data_list = torch.load(data_file)\n",
        "        dataset = Dataset.from_list(data_list)\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=16,\n",
        "            shuffle=False,\n",
        "            collate_fn=squad_collate_eval\n",
        "        )\n",
        "\n",
        "        all_start_logits = []\n",
        "        all_end_logits   = []\n",
        "\n",
        "        print(f\"Running inference on {split_name} set... (num_examples={len(dataset)})\")\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                start_log = outputs.start_logits.detach().cpu().numpy()\n",
        "                end_log   = outputs.end_logits.detach().cpu().numpy()\n",
        "\n",
        "                all_start_logits.append(start_log)\n",
        "                all_end_logits.append(end_log)\n",
        "\n",
        "        # Combine\n",
        "        all_start_logits = np.concatenate(all_start_logits, axis=0)\n",
        "        all_end_logits   = np.concatenate(all_end_logits, axis=0)\n",
        "\n",
        "        # Evaluate with threshold=0.0 (typical default)\n",
        "        results, _ = evaluate_squad_v2(\n",
        "            data_list,\n",
        "            all_start_logits,\n",
        "            all_end_logits,\n",
        "            null_score_diff_threshold=0.0\n",
        "        )\n",
        "        print(f\"\\n[{split_name.upper()} SET METRICS]:\")\n",
        "        for k, v in results.items():\n",
        "            print(f\"  {k}: {v:.2f}\")\n",
        "        print(\"-\"*50)\n",
        "        return results\n",
        "\n",
        "    # Evaluate on validation\n",
        "    val_metrics = run_eval(VAL_FILE, split_name=\"val\")\n",
        "\n",
        "    # Evaluate on test\n",
        "    test_metrics = run_eval(TEST_FILE, split_name=\"test\")\n",
        "    print(\"\\nEvaluation script completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntw099tM6_4_"
      },
      "source": [
        "##### Comparision with Baseline Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JLDN5Mq67hK",
        "outputId": "d31a13c6-f222-49c9-a2d3-2f1d39bf75ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google/muril-large-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-5-1a6b925ea531>:168: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset_val = Dataset.from_list(torch.load(DATASET_VAL))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Compare baseline vs fine-tuned model performance with validation data\n",
            "\n",
            "Evaluating Baseline Model...\n",
            "\n",
            "Evaluating Fine-Tuned Model...\n",
            "\n",
            "Baseline Model Metrics:\n",
            "{'em': 0.12530541012216406, 'f1': 0.1459625570250129, 'is_impossible_acc': 53.68421052631579}\n",
            "\n",
            "Fine-Tuned Model Metrics:\n",
            "{'em': 0.5707969749854567, 'f1': 0.7183550698776989, 'is_impossible_acc': 80.85213032581454}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-1a6b925ea531>:171: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  dataset_test = Dataset.from_list(torch.load(DATASET_TEST))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Compare baseline vs fine-tuned model performance with test data\n",
            "\n",
            "Evaluating Baseline Model...\n",
            "\n",
            "Evaluating Fine-Tuned Model...\n",
            "\n",
            "Baseline Model Metrics:\n",
            "{'em': 0.2397197123363452, 'f1': 0.2573378939348231, 'is_impossible_acc': 47.34121122599705}\n",
            "\n",
            "Fine-Tuned Model Metrics:\n",
            "{'em': 0.5894338926793288, 'f1': 0.6989928720876525, 'is_impossible_acc': 75.01846381093058}\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# eval_baseline_vs_finetuned_muril.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
        "import re\n",
        "\n",
        "##########################################\n",
        "# 1) Normalize Telugu text for evaluation\n",
        "##########################################\n",
        "def normalize_text_telugu(s):\n",
        "    \"\"\"Minimal normalization for Telugu text.\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = re.sub(r\"[^\\u0C00-\\u0C7Fa-zA-Z0-9\\s]\", \"\", s)  # Keep Telugu, English, numbers\n",
        "    s = s.lower()\n",
        "    s = \" \".join(s.split())  # Remove extra spaces\n",
        "    return s\n",
        "\n",
        "##########################################\n",
        "# 2) Cleaning Predictions\n",
        "##########################################\n",
        "def clean_prediction(text):\n",
        "    \"\"\"Remove unwanted characters and extra spaces.\"\"\"\n",
        "    return text.strip().replace(\"\\n\", \" \")\n",
        "\n",
        "##########################################\n",
        "# 3) Postprocessing for SQuAD v2.0\n",
        "##########################################\n",
        "def postprocess_qa_predictions_squad_v2(examples, start_logits, end_logits, cls_index=0, null_score_diff_threshold=0.0):\n",
        "    \"\"\"Postprocess predictions by selecting the best span or no-answer option.\"\"\"\n",
        "    preds = {}\n",
        "    for i, ex in enumerate(examples):\n",
        "        context = ex[\"context\"]\n",
        "        offsets = ex[\"offset_mapping\"]\n",
        "        ex_id   = ex[\"id\"]\n",
        "\n",
        "        best_start, best_end, best_score = 0, 0, float(\"-inf\")\n",
        "        cls_score = start_logits[i][cls_index] + end_logits[i][cls_index]  # No-answer score\n",
        "\n",
        "        for start_idx in np.argsort(start_logits[i])[-10:]:\n",
        "            for end_idx in np.argsort(end_logits[i])[-10:]:\n",
        "                if end_idx < start_idx or (end_idx - start_idx + 1) > 100:\n",
        "                    continue\n",
        "                score = start_logits[i][start_idx] + end_logits[i][end_idx]\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_start = start_idx\n",
        "                    best_end = end_idx\n",
        "\n",
        "        if best_score - cls_score < null_score_diff_threshold:\n",
        "            preds[ex_id] = \"\"\n",
        "        else:\n",
        "            start_char = offsets[best_start][0]\n",
        "            end_char   = offsets[best_end][1]\n",
        "            preds[ex_id] = clean_prediction(context[start_char:end_char])\n",
        "\n",
        "    return preds\n",
        "\n",
        "##########################################\n",
        "# 4) Metrics: EM, F1, Is Impossible Accuracy\n",
        "##########################################\n",
        "def exact_match(pred, gold):\n",
        "    return 1.0 if normalize_text_telugu(pred) == normalize_text_telugu(gold) else 0.0\n",
        "\n",
        "def f1_score(pred, gold):\n",
        "    pred_tokens = normalize_text_telugu(pred).split()\n",
        "    gold_tokens = normalize_text_telugu(gold).split()\n",
        "    common = set(pred_tokens) & set(gold_tokens)\n",
        "    num_same = len(common)\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return float(pred_tokens == gold_tokens)\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "def compute_is_impossible_accuracy(examples, predictions):\n",
        "    \"\"\"Compute accuracy for unanswerable questions.\"\"\"\n",
        "    total, correct = 0, 0\n",
        "    for ex in examples:\n",
        "        gold_empty = ex[\"gold_text\"].strip() == \"\"\n",
        "        pred_empty = predictions.get(ex[\"id\"], \"\").strip() == \"\"\n",
        "        if gold_empty:\n",
        "            total += 1\n",
        "            if pred_empty:\n",
        "                correct += 1\n",
        "    return (correct / total) * 100.0 if total > 0 else 100.0\n",
        "\n",
        "##########################################\n",
        "# 5) Master Evaluation Function\n",
        "##########################################\n",
        "def evaluate_squad_v2(examples, start_logits, end_logits, null_score_diff_threshold=0.0):\n",
        "    predictions = postprocess_qa_predictions_squad_v2(examples, start_logits, end_logits, null_score_diff_threshold=null_score_diff_threshold)\n",
        "\n",
        "    metrics = {\"em\": 0.0, \"f1\": 0.0, \"is_impossible_acc\": 0.0}\n",
        "    total = len(examples)\n",
        "\n",
        "    for ex in examples:\n",
        "        pred = predictions.get(ex[\"id\"], \"\")\n",
        "        gold = ex[\"gold_text\"]\n",
        "        metrics[\"em\"] += exact_match(pred, gold)\n",
        "        metrics[\"f1\"] += f1_score(pred, gold)\n",
        "\n",
        "    metrics[\"em\"] /= total\n",
        "    metrics[\"f1\"] /= total\n",
        "    metrics[\"is_impossible_acc\"] = compute_is_impossible_accuracy(examples, predictions)\n",
        "\n",
        "    return metrics, predictions\n",
        "\n",
        "##########################################\n",
        "# 6) Baseline vs Fine-Tuned Evaluation\n",
        "##########################################\n",
        "def run_model_evaluation(model, tokenizer, dataset, device):\n",
        "    \"\"\"Evaluate the given model on the dataset.\"\"\"\n",
        "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=lambda x: {\n",
        "        \"input_ids\": torch.stack([torch.tensor(f[\"input_ids\"]) for f in x]).to(device),\n",
        "        \"attention_mask\": torch.stack([torch.tensor(f[\"attention_mask\"]) for f in x]).to(device)\n",
        "    })\n",
        "\n",
        "    all_start_logits, all_end_logits = [], []\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            outputs = model(**batch)\n",
        "            all_start_logits.append(outputs.start_logits.cpu().numpy())\n",
        "            all_end_logits.append(outputs.end_logits.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(all_start_logits), np.concatenate(all_end_logits)\n",
        "\n",
        "def compare_models(base_model, fine_tuned_model, tokenizer, dataset, device, data_=\"validation\"):\n",
        "    \"\"\"Compare baseline vs fine-tuned model performance.\"\"\"\n",
        "    print(f\"=> Compare baseline vs fine-tuned model performance with {data_} data\")\n",
        "    print(\"\\nEvaluating Baseline Model...\")\n",
        "    base_start_logits, base_end_logits = run_model_evaluation(base_model, tokenizer, dataset, device)\n",
        "    base_metrics, _ = evaluate_squad_v2(dataset, base_start_logits, base_end_logits)\n",
        "\n",
        "    print(\"\\nEvaluating Fine-Tuned Model...\")\n",
        "    fine_tuned_start_logits, fine_tuned_end_logits = run_model_evaluation(fine_tuned_model, tokenizer, dataset, device)\n",
        "    fine_tuned_metrics, _ = evaluate_squad_v2(dataset, fine_tuned_start_logits, fine_tuned_end_logits)\n",
        "\n",
        "    print(\"\\nBaseline Model Metrics:\")\n",
        "    print(base_metrics)\n",
        "\n",
        "    print(\"\\nFine-Tuned Model Metrics:\")\n",
        "    print(fine_tuned_metrics)\n",
        "\n",
        "##########################################\n",
        "# 7) Main Execution\n",
        "##########################################\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    BASE_DIR = \"/content/drive/MyDrive/Te-QAS V07/MuRIL\"\n",
        "    BASELINE_MODEL = \"google/muril-large-cased\"\n",
        "    FINETUNED_MODEL_PATH = os.path.join(BASE_DIR, \"final_muril_2.0_tel_3\")\n",
        "    DATASET_VAL = os.path.join(BASE_DIR, \"muril_processed_telugu_squad_v2/val.pt\")\n",
        "    DATASET_TEST = os.path.join(BASE_DIR, \"muril_processed_telugu_squad_v2/test.pt\")\n",
        "\n",
        "    base_model = BertForQuestionAnswering.from_pretrained(BASELINE_MODEL).to(device)\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(BASELINE_MODEL)\n",
        "    fine_tuned_model = BertForQuestionAnswering.from_pretrained(FINETUNED_MODEL_PATH).to(device)\n",
        "\n",
        "    dataset_val = Dataset.from_list(torch.load(DATASET_VAL))\n",
        "    compare_models(base_model, fine_tuned_model, tokenizer, dataset_val, device, data_=\"validation\")\n",
        "\n",
        "    dataset_test = Dataset.from_list(torch.load(DATASET_TEST))\n",
        "    compare_models(base_model, fine_tuned_model, tokenizer, dataset_test, device, data_=\"test\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epnYRocvRPw4"
      },
      "source": [
        "##  Comparision Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlip-slGRTpX",
        "outputId": "7d45250f-ca6f-45b8-a2ed-ebcc41ba5736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Table 1: XLM-R Full Data Performance (Validation & Test)\n",
            "==================================================================================================================\n",
            "| Metric                 | Baseline (Validation) | Fine-Tuned (Validation) | Baseline (Test) | Fine-Tuned (Test) |\n",
            "|------------------------|-----------------------|-------------------------|-----------------|-------------------|\n",
            "| Exact Match (EM)       | 0.09                  | 61.3                    | 0.2             | 61.14             |\n",
            "| F1 Score               | 0.12                  | 75.31                   | 0.22            | 70.65             |\n",
            "| Is Impossible Accuracy | 39.15                 | 91.43                   | 39.31           | 82.2              |\n",
            "==================================================================================================================\n",
            "\n",
            "\n",
            "Table 2: MuRIL Full Data Performance (Validation & Test)\n",
            "==================================================================================================================\n",
            "| Metric                 | Baseline (Validation) | Fine-Tuned (Validation) | Baseline (Test) | Fine-Tuned (Test) |\n",
            "|------------------------|-----------------------|-------------------------|-----------------|-------------------|\n",
            "| Exact Match (EM)       | 0.13                  | 57.08                   | 0.24            | 58.94             |\n",
            "| F1 Score               | 0.15                  | 71.84                   | 0.26            | 69.9              |\n",
            "| Is Impossible Accuracy | 53.68                 | 80.8                    | 47.34           | 75.0              |\n",
            "==================================================================================================================\n",
            "\n",
            "\n",
            "Table 3: XLM-R Answerable Only Performance\n",
            "============================================================================================================\n",
            "| Metric           | Baseline (Validation) | Fine-Tuned (Validation) | Baseline (Test) | Fine-Tuned (Test) |\n",
            "|------------------|-----------------------|-------------------------|-----------------|-------------------|\n",
            "| Exact Match (EM) | 0.0                   | 59.53                   | 0.0             | 51.23             |\n",
            "| F1 Score         | 3.13                  | 76.88                   | 3.49            | 71.66             |\n",
            "============================================================================================================\n",
            "\n",
            "\n",
            "Table 4: MuRIL Answerable Only Performance\n",
            "============================================================================================================\n",
            "| Metric           | Baseline (Validation) | Fine-Tuned (Validation) | Baseline (Test) | Fine-Tuned (Test) |\n",
            "|------------------|-----------------------|-------------------------|-----------------|-------------------|\n",
            "| Exact Match (EM) | 0.0                   | 60.89                   | 0.0             | 51.6              |\n",
            "| F1 Score         | 2.3                   | 78.19                   | 4.23            | 72.09             |\n",
            "============================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set display options for better formatting\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.precision', 2)\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x if isinstance(x, (int, float)) else str(x))\n",
        "\n",
        "def print_table_with_header(df, title):\n",
        "   # Get the maximum width of each column for formatting\n",
        "   col_widths = [max(len(str(x)) for x in df[col]) for col in df.columns]\n",
        "   col_widths = [max(len(col), width) for col, width in zip(df.columns, col_widths)]\n",
        "\n",
        "   # Create header separator\n",
        "   header_sep = \"=\" * (sum(col_widths) + (len(col_widths) - 1) * 3 + 4)\n",
        "\n",
        "   # Print title\n",
        "   print(f\"\\n{title}\")\n",
        "   print(header_sep)\n",
        "\n",
        "   # Format and print header\n",
        "   header = \" | \".join(f\"{col:{width}}\" for col, width in zip(df.columns, col_widths))\n",
        "   print(f\"| {header} |\")\n",
        "\n",
        "   # Print separator after header\n",
        "   print(\"|\" + \"|\".join(\"-\" * (width + 2) for width in col_widths) + \"|\")\n",
        "\n",
        "   # Print each row\n",
        "   for _, row in df.iterrows():\n",
        "       row_str = \" | \".join(f\"{str(val):{width}}\" for val, width in zip(row, col_widths))\n",
        "       print(f\"| {row_str} |\")\n",
        "\n",
        "   print(header_sep + \"\\n\")\n",
        "\n",
        "# XLM-R Full Data\n",
        "xlmr_full_val_test = pd.DataFrame({\n",
        "   \"Metric\": [\"Exact Match (EM)\", \"F1 Score\", \"Is Impossible Accuracy\"],\n",
        "   \"Baseline (Validation)\": [0.09, 0.12, 39.15],\n",
        "   \"Fine-Tuned (Validation)\": [61.30, 75.31, 91.43],\n",
        "   \"Baseline (Test)\": [0.20, 0.22, 39.31],\n",
        "   \"Fine-Tuned (Test)\": [61.14, 70.65, 82.20]\n",
        "})\n",
        "\n",
        "# XLM-R Answerable Only\n",
        "xlmr_answerable_val_test = pd.DataFrame({\n",
        "   \"Metric\": [\"Exact Match (EM)\", \"F1 Score\"],\n",
        "   \"Baseline (Validation)\": [0.00, 3.13],\n",
        "   \"Fine-Tuned (Validation)\": [59.53, 76.88],\n",
        "   \"Baseline (Test)\": [0.00, 3.49],\n",
        "   \"Fine-Tuned (Test)\": [51.23, 71.66]\n",
        "})\n",
        "\n",
        "# MuRIL Full Data\n",
        "muril_full_val_test = pd.DataFrame({\n",
        "   \"Metric\": [\"Exact Match (EM)\", \"F1 Score\", \"Is Impossible Accuracy\"],\n",
        "   \"Baseline (Validation)\": [0.13, 0.15, 53.68],\n",
        "   \"Fine-Tuned (Validation)\": [57.08, 71.84, 80.80],\n",
        "   \"Baseline (Test)\": [0.24, 0.26, 47.34],\n",
        "   \"Fine-Tuned (Test)\": [58.94, 69.90, 75.00]\n",
        "})\n",
        "\n",
        "# MuRIL Answerable Only\n",
        "muril_answerable_val_test = pd.DataFrame({\n",
        "   \"Metric\": [\"Exact Match (EM)\", \"F1 Score\"],\n",
        "   \"Baseline (Validation)\": [0.00, 2.30],\n",
        "   \"Fine-Tuned (Validation)\": [60.89, 78.19],\n",
        "   \"Baseline (Test)\": [0.00, 4.23],\n",
        "   \"Fine-Tuned (Test)\": [51.60, 72.09]\n",
        "})\n",
        "\n",
        "# Print all tables with proper formatting\n",
        "print_table_with_header(xlmr_full_val_test, \"Table 1: XLM-R Full Data Performance (Validation & Test)\")\n",
        "print_table_with_header(muril_full_val_test, \"Table 2: MuRIL Full Data Performance (Validation & Test)\")\n",
        "print_table_with_header(xlmr_answerable_val_test, \"Table 3: XLM-R Answerable Only Performance\")\n",
        "print_table_with_header(muril_answerable_val_test, \"Table 4: MuRIL Answerable Only Performance\")\n",
        "\n",
        "# Optionally save to CSV\n",
        "xlmr_full_val_test.to_csv('xlmr_full_performance.csv', index=False)\n",
        "xlmr_answerable_val_test.to_csv('xlmr_answerable_performance.csv', index=False)\n",
        "muril_full_val_test.to_csv('muril_full_performance.csv', index=False)\n",
        "muril_answerable_val_test.to_csv('muril_answerable_performance.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LB6fThqobhs"
      },
      "source": [
        " Above metrics are rounded off to 2 decimals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPSmSQQ1RlOv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Te-QAS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00b55bb8300f44b88f3f2e43766af549": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01eff24caf954ccfb09e3fc0baa35f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570b9024f826435bb13c8e6fd3fbc5f7",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7e7ec7806a64f6abfe2e35e9e5b31e5",
            "value": 25
          }
        },
        "048bc030565046d1b3e0d401dd49dfd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "054a2a58d2fc451c9daed0574cfdeaaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4b5b70bf99a46bdb9b9a807740a4e6f",
              "IPY_MODEL_857770554a2a426c9b028a374859abaa",
              "IPY_MODEL_a574fe56b27c45fbbed8af9e53e01040"
            ],
            "layout": "IPY_MODEL_4ef116df03a940259a3117269c4c1099"
          }
        },
        "0793416922c94bae8a00202e60808a59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "123e773fd6fe4378bb3ea7f968fe05b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_785f01dd16184ca78b67c6e8605dc07c",
            "placeholder": "",
            "style": "IPY_MODEL_7c8e9323bb9d48d39cdd8d4db9c38ddc",
            "value": "model.safetensors:100%"
          }
        },
        "171476f8a223409d84687e4d46b44a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39eeda7f3fce444bad4fc0da17e97d2f",
            "placeholder": "",
            "style": "IPY_MODEL_83dcbb499e2449a4b919074ae548aa9f",
            "value": "tokenizer_config.json:100%"
          }
        },
        "214a1b4b9a5a4766ba1f90b09cf3eb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5a274cc7144cb2a442e700f944d9e3",
            "placeholder": "",
            "style": "IPY_MODEL_90217091c9b647ad84f873e999d640bb",
            "value": "sentencepiece.bpe.model:100%"
          }
        },
        "229c26e8c11d4f509a30169722c45995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abe91c30a89a4c8fa10f9cb15f155a87",
            "placeholder": "",
            "style": "IPY_MODEL_45d697da72a8414db99314c7e4545809",
            "value": "6.27k/6.27k[00:00&lt;00:00,278kB/s]"
          }
        },
        "23d769cd912148b392249d14d1b5075a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_493853a37c054471b68b8828b3766884",
            "placeholder": "",
            "style": "IPY_MODEL_fbd25d72f17a4e80a24e9b2bbd6995e6",
            "value": "2.24G/2.24G[00:15&lt;00:00,244MB/s]"
          }
        },
        "2b37132873f147a89a5f6be5aa4e34c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32bb69d567d443d0964d9f241cef5fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8141ce556cc044559223665c5c8af949",
              "IPY_MODEL_570756f1a4a7409684e8b3d807dc720d",
              "IPY_MODEL_389ba94d32ab49138ec18e5c54624a07"
            ],
            "layout": "IPY_MODEL_855ef79d647540b68edc1614be269f00"
          }
        },
        "389ba94d32ab49138ec18e5c54624a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40e9381a4daa4b01b39f83a7d54f78bc",
            "placeholder": "",
            "style": "IPY_MODEL_9ee126bebaf74ab39eead9bf852e5fd4",
            "value": "616/616[00:00&lt;00:00,21.5kB/s]"
          }
        },
        "39d0788d07c44c098707a278011074a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39eeda7f3fce444bad4fc0da17e97d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a1aca689b6e4989b7c456a8d5960385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b5a274cc7144cb2a442e700f944d9e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40e9381a4daa4b01b39f83a7d54f78bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "415f5c32d4be4adc82fd1beb7cd8d2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45d697da72a8414db99314c7e4545809": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "493853a37c054471b68b8828b3766884": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ef116df03a940259a3117269c4c1099": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "551cfd40a3ae4f919c01e961a2d62a39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "570756f1a4a7409684e8b3d807dc720d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a51e1c5f34df4626bbff99fc98ff10e5",
            "max": 616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00b55bb8300f44b88f3f2e43766af549",
            "value": 616
          }
        },
        "570b9024f826435bb13c8e6fd3fbc5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59741346440543d3b350d0917bae2100": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_551cfd40a3ae4f919c01e961a2d62a39",
            "placeholder": "",
            "style": "IPY_MODEL_8c678547358a4e8db583e078f8e67d2d",
            "value": "Downloadingbuilderscript:100%"
          }
        },
        "684978beae0042dd9c2f3f41d93c2235": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75170268a5974144a05d09d38d4cc9d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "785f01dd16184ca78b67c6e8605dc07c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c8e9323bb9d48d39cdd8d4db9c38ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8141ce556cc044559223665c5c8af949": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f03012a1a5c6418ea2d7aa29a61d06e1",
            "placeholder": "",
            "style": "IPY_MODEL_fe6d5e8fd55b42f4ad0387b81496a41b",
            "value": "config.json:100%"
          }
        },
        "82f62d3650f04d2eb45a2818aec31154": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83dcbb499e2449a4b919074ae548aa9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "855ef79d647540b68edc1614be269f00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "857770554a2a426c9b028a374859abaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4449dc6c5be4b3ead863a6aecad3ecf",
            "max": 9096718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4eb4af54550473e9f1834c89c9c09d9",
            "value": 9096718
          }
        },
        "8c678547358a4e8db583e078f8e67d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eda24f7574b4fe49710c3cbdd23a13f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90201913d38d485c9c72469d7b6e4b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc2029179f9a4d3797f4170e0dc59bbf",
            "placeholder": "",
            "style": "IPY_MODEL_ff5823e48dbf42eea3918bcfeae9e887",
            "value": "5.07M/5.07M[00:00&lt;00:00,15.5MB/s]"
          }
        },
        "90217091c9b647ad84f873e999d640bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9808599abccf4c0e9bb5f4f2b9be3848": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a1c6ca6597f4e6eaf0458fd462d2737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_123e773fd6fe4378bb3ea7f968fe05b0",
              "IPY_MODEL_ea7a04251d6a4d28bbf5da029b2ac1af",
              "IPY_MODEL_23d769cd912148b392249d14d1b5075a"
            ],
            "layout": "IPY_MODEL_2b37132873f147a89a5f6be5aa4e34c9"
          }
        },
        "9b59b2c7c4974f5790a4af6749252eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eda24f7574b4fe49710c3cbdd23a13f",
            "placeholder": "",
            "style": "IPY_MODEL_048bc030565046d1b3e0d401dd49dfd5",
            "value": "25.0/25.0[00:00&lt;00:00,1.21kB/s]"
          }
        },
        "9ee126bebaf74ab39eead9bf852e5fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ffb71c4f70b456cb033aa94f256b7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05039015cb6411a9b3ecbcb42e9c057",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_415f5c32d4be4adc82fd1beb7cd8d2ba",
            "value": 5069051
          }
        },
        "a4eb4af54550473e9f1834c89c9c09d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a51e1c5f34df4626bbff99fc98ff10e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a574fe56b27c45fbbed8af9e53e01040": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_684978beae0042dd9c2f3f41d93c2235",
            "placeholder": "",
            "style": "IPY_MODEL_d3e41812607440fbaf27e0382d474dd9",
            "value": "9.10M/9.10M[00:00&lt;00:00,26.4MB/s]"
          }
        },
        "abe91c30a89a4c8fa10f9cb15f155a87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b58f074f291a465c912cf0613a6fb4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b74d3ebc84254cedafc06fc68af74603": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82f62d3650f04d2eb45a2818aec31154",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a1aca689b6e4989b7c456a8d5960385",
            "value": 6270
          }
        },
        "b94310b5721e4731986c2f7bba5ccd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_214a1b4b9a5a4766ba1f90b09cf3eb2b",
              "IPY_MODEL_9ffb71c4f70b456cb033aa94f256b7ac",
              "IPY_MODEL_90201913d38d485c9c72469d7b6e4b29"
            ],
            "layout": "IPY_MODEL_0793416922c94bae8a00202e60808a59"
          }
        },
        "c15f6be65fdd4adfbf4ae679ad6eb45d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e7ec7806a64f6abfe2e35e9e5b31e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbebb69a4edc4c3d94477e5936694be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_171476f8a223409d84687e4d46b44a8d",
              "IPY_MODEL_01eff24caf954ccfb09e3fc0baa35f56",
              "IPY_MODEL_9b59b2c7c4974f5790a4af6749252eba"
            ],
            "layout": "IPY_MODEL_75170268a5974144a05d09d38d4cc9d1"
          }
        },
        "cc2029179f9a4d3797f4170e0dc59bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e41812607440fbaf27e0382d474dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4b5b70bf99a46bdb9b9a807740a4e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c15f6be65fdd4adfbf4ae679ad6eb45d",
            "placeholder": "",
            "style": "IPY_MODEL_d7da4b8e8f4f46518d30492d08b3e898",
            "value": "tokenizer.json:100%"
          }
        },
        "d7da4b8e8f4f46518d30492d08b3e898": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e05039015cb6411a9b3ecbcb42e9c057": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4449dc6c5be4b3ead863a6aecad3ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea7a04251d6a4d28bbf5da029b2ac1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9808599abccf4c0e9bb5f4f2b9be3848",
            "max": 2244817354,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b58f074f291a465c912cf0613a6fb4ff",
            "value": 2244817354
          }
        },
        "f03012a1a5c6418ea2d7aa29a61d06e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbd25d72f17a4e80a24e9b2bbd6995e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc65753410804c71ab3c70671cd4e21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59741346440543d3b350d0917bae2100",
              "IPY_MODEL_b74d3ebc84254cedafc06fc68af74603",
              "IPY_MODEL_229c26e8c11d4f509a30169722c45995"
            ],
            "layout": "IPY_MODEL_39d0788d07c44c098707a278011074a6"
          }
        },
        "fe6d5e8fd55b42f4ad0387b81496a41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff5823e48dbf42eea3918bcfeae9e887": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
