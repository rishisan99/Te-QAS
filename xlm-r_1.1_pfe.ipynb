{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered answerable QAs saved to squad2.1_train.json\n",
      "Filtered answerable QAs saved to squad2.1_test.json\n",
      "Filtered answerable QAs saved to squad2.1_val.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def filter_answerable_squad(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Reads a SQuAD 2.0 file, keeps only QAs where is_impossible=False,\n",
    "    writes a new JSON file with the same structure but only answerable QAs.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    new_data = {\n",
    "        \"version\": data.get(\"version\", \"filtered\"),\n",
    "        \"data\": []\n",
    "    }\n",
    "\n",
    "    for article in data[\"data\"]:\n",
    "        new_paragraphs = []\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            new_qas = []\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                if not qa.get(\"is_impossible\", False):\n",
    "                    # keep only this QA\n",
    "                    new_qas.append(qa)\n",
    "            # If we found answerable QAs in this paragraph, keep them\n",
    "            if new_qas:\n",
    "                new_paragraphs.append({\n",
    "                    \"context\": context,\n",
    "                    \"qas\": new_qas\n",
    "                })\n",
    "\n",
    "        if new_paragraphs:\n",
    "            new_data[\"data\"].append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"paragraphs\": new_paragraphs\n",
    "            })\n",
    "\n",
    "    # Save\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def main():\n",
    "    # example usage \n",
    "    input_file  = \"../TeQAS 1.0/English Data/squad2.0_train.json\"\n",
    "    output_file = \"squad2.1_train.json\"\n",
    "    filter_answerable_squad(input_file, output_file)\n",
    "    print(f\"Filtered answerable QAs saved to {output_file}\")\n",
    "\n",
    "    # example usage\n",
    "    input_file  = \"../TeQAS 1.0/English Data/squad2.0_test.json\"\n",
    "    output_file = \"squad2.1_test.json\"\n",
    "    filter_answerable_squad(input_file, output_file)\n",
    "    print(f\"Filtered answerable QAs saved to {output_file}\")\n",
    "\n",
    "    # example usage\n",
    "    input_file  = \"../TeQAS 1.0/English Data/squad2.0_val.json\"\n",
    "    output_file = \"squad2.1_val.json\"\n",
    "    filter_answerable_squad(input_file, output_file)\n",
    "    print(f\"Filtered answerable QAs saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer: xlm-roberta-large\n",
      "Filtering + building train data (TELUGU) ...\n",
      "Train answerable size: 72039\n",
      "Filtering + building val data (TELUGU) ...\n",
      "Val answerable size: 6600\n",
      "Filtering + building test data (TELUGU) ...\n",
      "Test answerable size: 5430\n",
      "\n",
      "Saved final PT files to processed_telugu_answerable_data/\n",
      "Done! Telugu answerable preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# preprocess_answerable.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "# Adjust your paths here\n",
    "train_json = \"Telugu 2.1/squad2.1_telugu_train.json\"      # SQuAD 2.0 train with unanswerable\n",
    "val_json   = \"Telugu 2.1/squad2.1_telugu_val.json\"        # SQuAD 2.0 val\n",
    "test_json  = \"Telugu 2.1/squad2.1_telugu_test.json\"       # SQuAD 2.0 test\n",
    "\n",
    "out_dir    = \"processed_telugu_answerable_data\"     # where we'll write train.pt, val.pt, test.pt\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "max_length = 512\n",
    "model_tokenizer_name = \"xlm-roberta-large\"  # or \"xlm-roberta-base\" if GPU is limited\n",
    "\n",
    "###############################################\n",
    "# 2) Filter out unanswerable QAs\n",
    "###############################################\n",
    "def filter_answerable_squad(input_path):\n",
    "    \"\"\"\n",
    "    Returns a new SQuAD JSON dict containing only QAs where is_impossible=False\n",
    "    with at least one answer.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    new_data = {\n",
    "        \"version\": data.get(\"version\", \"filtered_telugu\"),\n",
    "        \"data\": []\n",
    "    }\n",
    "    for article in data[\"data\"]:\n",
    "        new_paragraphs = []\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            new_qas = []\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                if not qa.get(\"is_impossible\", False) and qa.get(\"answers\"):\n",
    "                    new_qas.append(qa)\n",
    "            if new_qas:\n",
    "                new_paragraphs.append({\n",
    "                    \"context\": context,\n",
    "                    \"qas\": new_qas\n",
    "                })\n",
    "        if new_paragraphs:\n",
    "            new_data[\"data\"].append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"paragraphs\": new_paragraphs\n",
    "            })\n",
    "    return new_data\n",
    "\n",
    "###############################################\n",
    "# 3) Build offset-based examples\n",
    "###############################################\n",
    "def build_answerable_examples(squad_data, tokenizer, max_length=384):\n",
    "    \"\"\"\n",
    "    For each answerable QA:\n",
    "      - tokenize question+context\n",
    "      - find start/end token indices\n",
    "      - store offset_mapping, context, gold_text, etc.\n",
    "    \"\"\"\n",
    "    examples_out = []\n",
    "    for article in squad_data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                ans = qa[\"answers\"][0]\n",
    "                ans_start = ans[\"answer_start\"]\n",
    "                ans_text  = ans[\"text\"]\n",
    "                ans_end   = ans_start + len(ans_text)\n",
    "\n",
    "                enc = tokenizer(\n",
    "                    qa[\"question\"],\n",
    "                    context,\n",
    "                    max_length=max_length,\n",
    "                    truncation=\"only_second\",\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "\n",
    "                input_ids      = enc[\"input_ids\"][0]\n",
    "                attention_mask = enc[\"attention_mask\"][0]\n",
    "                offset_mapping = enc[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "                # find start/end token indices\n",
    "                start_token = None\n",
    "                end_token   = None\n",
    "                for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "                    if start_char <= ans_start < end_char:\n",
    "                        start_token = i\n",
    "                    if start_char < ans_end <= end_char:\n",
    "                        end_token = i\n",
    "\n",
    "                # fallback if mismatch\n",
    "                if start_token is None or end_token is None or end_token < start_token:\n",
    "                    start_token = 0\n",
    "                    end_token   = 0\n",
    "\n",
    "                ex_item = {\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"start_positions\": torch.tensor(start_token, dtype=torch.long),\n",
    "                    \"end_positions\":   torch.tensor(end_token,   dtype=torch.long),\n",
    "                    \"offset_mapping\":  offset_mapping,\n",
    "                    \"context\":         context,\n",
    "                    \"gold_text\":       ans_text\n",
    "                }\n",
    "                examples_out.append(ex_item)\n",
    "    return examples_out\n",
    "\n",
    "def main():\n",
    "    print(f\"Using tokenizer: {model_tokenizer_name}\")\n",
    "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_tokenizer_name)\n",
    "\n",
    "    # 1) Filter + build train\n",
    "    print(\"Filtering + building train data (TELUGU) ...\")\n",
    "    train_data_raw = filter_answerable_squad(train_json)\n",
    "    train_list = build_answerable_examples(train_data_raw, tokenizer, max_length)\n",
    "    print(f\"Train answerable size: {len(train_list)}\")\n",
    "\n",
    "    # 2) Filter + build val\n",
    "    print(\"Filtering + building val data (TELUGU) ...\")\n",
    "    val_data_raw = filter_answerable_squad(val_json)\n",
    "    val_list = build_answerable_examples(val_data_raw, tokenizer, max_length)\n",
    "    print(f\"Val answerable size: {len(val_list)}\")\n",
    "\n",
    "    # 3) Filter + build test\n",
    "    print(\"Filtering + building test data (TELUGU) ...\")\n",
    "    test_data_raw = filter_answerable_squad(test_json)\n",
    "    test_list = build_answerable_examples(test_data_raw, tokenizer, max_length)\n",
    "    print(f\"Test answerable size: {len(test_list)}\")\n",
    "\n",
    "    # 4) Save as .pt\n",
    "    train_out = os.path.join(out_dir, \"train.pt\")\n",
    "    val_out   = os.path.join(out_dir, \"val.pt\")\n",
    "    test_out  = os.path.join(out_dir, \"test.pt\")\n",
    "\n",
    "    torch.save(train_list, train_out)\n",
    "    torch.save(val_list,   val_out)\n",
    "    torch.save(test_list,  test_out)\n",
    "\n",
    "    print(f\"\\nSaved final PT files to {out_dir}/\")\n",
    "    print(\"Done! Telugu answerable preprocessing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-27 18:05:12.404540: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_3266/4135308115.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_list = torch.load(os.path.join(data_dir, \"train.pt\"))\n",
      "/tmp/ipykernel_3266/4135308115.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
      "/tmp/ipykernel_3266/4135308115.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/data/TeQAS 1.1/wandb/run-20250127_182533-gdpu884d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1/runs/gdpu884d' target=\"_blank\">XLM-R Eng 1</a></strong> to <a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%201.1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1/runs/gdpu884d' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%201.1/runs/gdpu884d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5014' max='5014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5014/5014 10:55:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Em</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.773600</td>\n",
       "      <td>0.793180</td>\n",
       "      <td>71.075758</td>\n",
       "      <td>84.565608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.589000</td>\n",
       "      <td>0.748568</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>85.786440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Model + checkpoint saved.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# fine_tune_answerable_trainer.py\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    XLMRobertaForQuestionAnswering,\n",
    "    XLMRobertaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import wandb\n",
    "\n",
    "# We'll reuse some code for postprocessing\n",
    "############################\n",
    "# Postprocess + EM/F1\n",
    "############################\n",
    "def normalize_text(s):\n",
    "    def remove_articles(t):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", t)\n",
    "    def remove_punc(t):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    def white_space_fix(t):\n",
    "        return \" \".join(t.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens)==0 or len(gold_tokens)==0:\n",
    "        return 1.0 if pred_tokens==gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision+recall==0:\n",
    "        return 0.0\n",
    "    return 2*precision*recall/(precision+recall)\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"\n",
    "    examples: a list of dict, each with offset_mapping, context, gold_text, id, etc.\n",
    "    start_logits, end_logits: [num_samples, seq_len]\n",
    "\n",
    "    returns: dict { example_id: predicted_text }\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    for i, ex in enumerate(examples):\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "        # best start/end\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx   = int(np.argmax(end_logits[i]))\n",
    "        if end_idx < start_idx:\n",
    "            start_idx, end_idx = end_idx, start_idx\n",
    "\n",
    "        if start_idx >= len(offsets):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "        if end_idx >= len(offsets):\n",
    "            end_idx = len(offsets)-1\n",
    "\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char   = offsets[end_idx][1]\n",
    "        pred_text  = context[start_char:end_char]\n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, dataset):\n",
    "    \"\"\"\n",
    "    eval_preds => (start_logits, end_logits)\n",
    "    dataset => the raw examples with gold_text\n",
    "    We'll decode + compare\n",
    "    \"\"\"\n",
    "    (start_logits, end_logits) = eval_preds\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits   = end_logits.cpu().numpy()\n",
    "\n",
    "    preds_dict = postprocess_qa_predictions(dataset, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in dataset:\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred  = preds_dict[ex_id]\n",
    "        gold  = ex[\"gold_text\"]\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count    += 1\n",
    "\n",
    "    em_val = total_em / count * 100.0\n",
    "    f1_val = total_f1 / count * 100.0\n",
    "    return {\n",
    "        \"em\": em_val,\n",
    "        \"f1\": f1_val\n",
    "    }\n",
    "\n",
    "def main():    # 1) Load .pt files\n",
    "    data_dir = \"processed_english_answerable_data\"  # from previous script\n",
    "    train_list = torch.load(os.path.join(data_dir, \"train.pt\"))\n",
    "    val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
    "    test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
    "\n",
    "    # 2) Convert to huggingface Dataset\n",
    "    from datasets import Dataset\n",
    "    train_dataset = Dataset.from_list(train_list)\n",
    "    val_dataset   = Dataset.from_list(val_list)\n",
    "    test_dataset  = Dataset.from_list(test_list)\n",
    "\n",
    "    wandb.init(project=\"TeQAS 1.1\", name=\"XLM-R Eng 1\")\n",
    "\n",
    "    # 3) Initialize model (Large recommended)\n",
    "    model_name = \"xlm-roberta-large\"\n",
    "    model = XLMRobertaForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    # 4) Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"checkpoints_xlmr_eng_answerable_v2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,            # Try 2-3\n",
    "        per_device_train_batch_size=16,# or 32 if GPU memory allows\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"logs_answerable\",\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "    def hf_compute_metrics(p: EvalPrediction):\n",
    "        # p.predictions => (start_logits, end_logits)\n",
    "        # dataset => we have val_dataset\n",
    "        return compute_metrics(p.predictions, val_dataset)\n",
    "\n",
    "    # 6) Build Trainer\n",
    "\n",
    "    from transformers import Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=default_data_collator,  # works if each sample has same keys: input_ids, attn_mask, start/end\n",
    "        tokenizer=None,   \n",
    "        compute_metrics=hf_compute_metrics\n",
    "    )\n",
    "\n",
    "    # 7) Train\n",
    "    trainer.train()\n",
    "\n",
    "    # 10) Save final\n",
    "    trainer.save_model(\"final_xlmr_eng_answerable_v2\")\n",
    "    print(\"Done! Model + checkpoint saved.\")\n",
    "    \n",
    "    # # 8) Evaluate on val => see final\n",
    "    # def hf_compute_metrics_test(p: EvalPrediction):\n",
    "    #     return compute_metrics(p.predictions, test_dataset)\n",
    "\n",
    "    # test_metrics = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
    "    # # But note: we can't automatically change the compute_metrics. We can do a manual pass:\n",
    "    # test_preds = trainer.predict(test_dataset)\n",
    "\n",
    "    # # compute test set EM/F1\n",
    "    # final_test = compute_metrics(test_preds.predictions, test_list)\n",
    "    # print(\"Test set metrics:\", final_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3266/1323923500.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_list = torch.load(TEST_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./final_xlmr_eng_answerable_v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3266/1323923500.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"input_ids\": torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device),\n",
      "/tmp/ipykernel_3266/1323923500.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Metrics: {'exact_match': 69.12955465587044, 'f1': 83.49781599734129}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluate_model.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import XLMRobertaForQuestionAnswering, XLMRobertaTokenizerFast\n",
    "\n",
    "# Load preprocessed test dataset\n",
    "DATA_DIR = \"processed_english_answerable_data\"\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.pt\")\n",
    "\n",
    "print(\"\\nLoading test dataset...\")\n",
    "test_list = torch.load(TEST_PATH)\n",
    "test_dataset = Dataset.from_list(test_list)\n",
    "\n",
    "# Load trained model\n",
    "MODEL_PATH = \"./final_xlmr_eng_answerable_v2\"\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = XLMRobertaForQuestionAnswering.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Define Postprocessing & Evaluation\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"\n",
    "    Postprocesses model predictions to extract answer text.\n",
    "    - examples: List of dataset examples.\n",
    "    - start_logits, end_logits: Logits output by the model.\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    num_examples = len(examples)\n",
    "    num_logits = len(start_logits)\n",
    "\n",
    "    if num_logits != num_examples:\n",
    "        print(f\"⚠️ Warning: Logits size {num_logits} doesn't match dataset size {num_examples}. Truncating...\")\n",
    "        num_examples = min(num_examples, num_logits)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        ex = examples[i]\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "        \n",
    "        if len(start_logits[i]) == 0 or len(end_logits[i]) == 0:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        if start_idx >= len(offsets) or end_idx >= len(offsets) or start_idx > end_idx:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char = offsets[end_idx][1]\n",
    "        pred_text = context[start_char:end_char]\n",
    "        \n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, dataset):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics (EM, F1, BLEU, ROUGE).\n",
    "    \"\"\"\n",
    "    start_logits, end_logits = eval_preds\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    predictions = postprocess_qa_predictions(dataset, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in dataset:\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred = predictions.get(ex_id, \"\")\n",
    "        gold = ex[\"gold_text\"]\n",
    "\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count += 1\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": (total_em / count) * 100.0,\n",
    "        \"f1\": (total_f1 / count) * 100.0\n",
    "    }\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "def normalize_text(s):\n",
    "    def remove_articles(t):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", t)\n",
    "    def remove_punc(t):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    def white_space_fix(t):\n",
    "        return \" \".join(t.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "# Run Evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for example in test_list:\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device),\n",
    "            \"attention_mask\": torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device),\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        test_preds.append((outputs.start_logits.cpu().numpy(), outputs.end_logits.cpu().numpy()))\n",
    "\n",
    "# Compute Metrics\n",
    "test_start_logits = np.concatenate([p[0] for p in test_preds], axis=0)\n",
    "test_end_logits = np.concatenate([p[1] for p in test_preds], axis=0)\n",
    "test_metrics = compute_metrics((test_start_logits, test_end_logits), test_list)\n",
    "print(\"Final Test Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-28 08:24:25.542375: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_618687/1600972959.py:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_list = torch.load(os.path.join(data_dir, \"train.pt\"))\n",
      "/tmp/ipykernel_618687/1600972959.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
      "/tmp/ipykernel_618687/1600972959.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanthoshrishi9999\u001b[0m (\u001b[33msanthosh-rishi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/data/TeQAS 1.1/wandb/run-20250128_082753-of58vx7m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2/runs/of58vx7m' target=\"_blank\">XLM-R Tel 3</a></strong> to <a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2/runs/of58vx7m' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2/runs/of58vx7m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: final_xlmr_eng_answerable_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6756' max='6756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6756/6756 24:30:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Em</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.281000</td>\n",
       "      <td>1.246652</td>\n",
       "      <td>57.651515</td>\n",
       "      <td>75.059659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.043600</td>\n",
       "      <td>1.236136</td>\n",
       "      <td>59.075758</td>\n",
       "      <td>76.449596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.815600</td>\n",
       "      <td>1.306881</td>\n",
       "      <td>59.530303</td>\n",
       "      <td>76.880735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Model + checkpoint saved.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/em</td><td>▁▆█</td></tr><tr><td>eval/f1</td><td>▁▆█</td></tr><tr><td>eval/loss</td><td>▂▁█</td></tr><tr><td>eval/runtime</td><td>█▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁██</td></tr><tr><td>eval/steps_per_second</td><td>▁██</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▄▃█▄▃▄▂▃▃▂▅▃▂▃▂▄▃▄▂▃▂▂▃▂▃▃▃▄▃▄▅▂▁▅▂▃▃▃▃▂</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇█▇▇▇▆▆▇▆▆▆▆▆▅▄▄▃▄▄▄▃▃▄▃▄▃▃▃▁▂▂▂▂▂▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/em</td><td>59.5303</td></tr><tr><td>eval/f1</td><td>76.88073</td></tr><tr><td>eval/loss</td><td>1.30688</td></tr><tr><td>eval/runtime</td><td>533.6189</td></tr><tr><td>eval/samples_per_second</td><td>12.368</td></tr><tr><td>eval/steps_per_second</td><td>0.388</td></tr><tr><td>total_flos</td><td>2.0070936901772698e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>6756</td></tr><tr><td>train/grad_norm</td><td>16.10918</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8156</td></tr><tr><td>train_loss</td><td>1.09027</td></tr><tr><td>train_runtime</td><td>88266.3524</td></tr><tr><td>train_samples_per_second</td><td>2.448</td></tr><tr><td>train_steps_per_second</td><td>0.077</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">XLM-R Tel 3</strong> at: <a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2/runs/of58vx7m' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2/runs/of58vx7m</a><br> View project at: <a href='https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS%201.1%20v2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250128_082753-of58vx7m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# fine_tune_answerable_trainer.py\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    XLMRobertaForQuestionAnswering,\n",
    "    XLMRobertaConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# We'll reuse some code for postprocessing\n",
    "############################\n",
    "# Postprocess + EM/F1\n",
    "############################\n",
    "def normalize_text(s):\n",
    "    def remove_articles(t):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", t)\n",
    "    def remove_punc(t):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    def white_space_fix(t):\n",
    "        return \" \".join(t.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens)==0 or len(gold_tokens)==0:\n",
    "        return 1.0 if pred_tokens==gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision+recall==0:\n",
    "        return 0.0\n",
    "    return 2*precision*recall/(precision+recall)\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"\n",
    "    Postprocesses model predictions to extract answer text.\n",
    "    - examples: List of dataset examples.\n",
    "    - start_logits, end_logits: Logits output by the model.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary mapping example IDs to predicted answers.\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    \n",
    "    num_examples = len(examples)\n",
    "    num_logits = len(start_logits)\n",
    "\n",
    "    # ✅ Ensure logits and dataset sizes match\n",
    "    if num_logits != num_examples:\n",
    "        print(f\"⚠️ Warning: Logits size {num_logits} doesn't match dataset size {num_examples}. Truncating...\")\n",
    "        num_examples = min(num_examples, num_logits)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        ex = examples[i]\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        # ✅ Ensure `i` is within valid range\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "        \n",
    "        # ✅ Ensure logits shape is valid before calling `np.argmax`\n",
    "        if len(start_logits[i]) == 0 or len(end_logits[i]) == 0:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Get best start/end indices\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        if start_idx >= len(offsets) or end_idx >= len(offsets) or start_idx > end_idx:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Extract the predicted answer\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char = offsets[end_idx][1]\n",
    "        pred_text = context[start_char:end_char]\n",
    "        \n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, dataset):\n",
    "    \"\"\"\n",
    "    eval_preds => (start_logits, end_logits)\n",
    "    dataset => the raw examples with gold_text\n",
    "    We'll decode + compare\n",
    "    \"\"\"\n",
    "    (start_logits, end_logits) = eval_preds\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits   = end_logits.cpu().numpy()\n",
    "\n",
    "    preds_dict = postprocess_qa_predictions(dataset, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in dataset:\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred  = preds_dict[ex_id]\n",
    "        gold  = ex[\"gold_text\"]\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count    += 1\n",
    "\n",
    "    em_val = total_em / count * 100.0\n",
    "    f1_val = total_f1 / count * 100.0\n",
    "    return {\n",
    "        \"em\": em_val,\n",
    "        \"f1\": f1_val\n",
    "    }\n",
    "\n",
    "import wandb\n",
    "\n",
    "def main():\n",
    "    # 1) Load .pt files\n",
    "    data_dir = \"processed_telugu_answerable_data\"  # from previous script\n",
    "    train_list = torch.load(os.path.join(data_dir, \"train.pt\"))\n",
    "    val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
    "    test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
    "\n",
    "    # 2) Convert to huggingface Dataset\n",
    "    from datasets import Dataset\n",
    "    train_dataset = Dataset.from_list(train_list)\n",
    "    val_dataset   = Dataset.from_list(val_list)\n",
    "    test_dataset  = Dataset.from_list(test_list)\n",
    "\n",
    "    wandb.init(project=\"TeQAS 1.1 v2\", name=\"XLM-R Tel 3\")\n",
    "\n",
    "    # 3) Initialize model (Large recommended)\n",
    "    model_name = \"final_xlmr_eng_answerable_v2\"\n",
    "\n",
    "    # Load Model\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model = XLMRobertaForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # ✅ Move model explicitly to GPU\n",
    "\n",
    "    # Define Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"checkpoints_xlmr_tel_answerable_v2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,            # Try 2-3\n",
    "        per_device_train_batch_size=16,# or 32 if GPU memory allows\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"logs_answerable\",\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "    def hf_compute_metrics(p: EvalPrediction):\n",
    "        # p.predictions => (start_logits, end_logits)\n",
    "        # dataset => we have val_dataset\n",
    "        return compute_metrics(p.predictions, val_dataset)\n",
    "\n",
    "    # Trainer Initialization\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        tokenizer=None, \n",
    "        compute_metrics=hf_compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # 10) Save final\n",
    "    trainer.save_model(\"final_xlmr_tel_answerable_3_v2\")\n",
    "    print(\"Done! Model + checkpoint saved.\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_618687/1635882417.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_list = torch.load(TEST_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./final_xlmr_tel_answerable_3_v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_618687/1635882417.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"input_ids\": torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device),\n",
      "/tmp/ipykernel_618687/1635882417.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"attention_mask\": torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Metrics: {'exact_match': 51.23388581952118, 'f1': 71.65901983874353}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluate_model.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import XLMRobertaForQuestionAnswering, XLMRobertaTokenizerFast\n",
    "\n",
    "# ✅ Load preprocessed test dataset\n",
    "DATA_DIR = \"processed_telugu_answerable_data\"\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.pt\")\n",
    "\n",
    "print(\"\\nLoading test dataset...\")\n",
    "test_list = torch.load(TEST_PATH)\n",
    "test_dataset = Dataset.from_list(test_list)\n",
    "\n",
    "# ✅ Load trained model\n",
    "MODEL_PATH = \"./final_xlmr_tel_answerable_3_v2\"\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "model = XLMRobertaForQuestionAnswering.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ✅ Define Postprocessing & Evaluation\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"\n",
    "    Postprocesses model predictions to extract answer text.\n",
    "    - examples: List of dataset examples.\n",
    "    - start_logits, end_logits: Logits output by the model.\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    num_examples = len(examples)\n",
    "    num_logits = len(start_logits)\n",
    "\n",
    "    if num_logits != num_examples:\n",
    "        print(f\"⚠️ Warning: Logits size {num_logits} doesn't match dataset size {num_examples}. Truncating...\")\n",
    "        num_examples = min(num_examples, num_logits)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        ex = examples[i]\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "        \n",
    "        if len(start_logits[i]) == 0 or len(end_logits[i]) == 0:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        if start_idx >= len(offsets) or end_idx >= len(offsets) or start_idx > end_idx:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char = offsets[end_idx][1]\n",
    "        pred_text = context[start_char:end_char]\n",
    "        \n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, dataset):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics (EM, F1, BLEU, ROUGE).\n",
    "    \"\"\"\n",
    "    start_logits, end_logits = eval_preds\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    predictions = postprocess_qa_predictions(dataset, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in dataset:\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred = predictions.get(ex_id, \"\")\n",
    "        gold = ex[\"gold_text\"]\n",
    "\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count += 1\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": (total_em / count) * 100.0,\n",
    "        \"f1\": (total_f1 / count) * 100.0\n",
    "    }\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "\n",
    "def normalize_text(s):\n",
    "    def remove_articles(t):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", t)\n",
    "    def remove_punc(t):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    def white_space_fix(t):\n",
    "        return \" \".join(t.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "# ✅ Run Evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for example in test_list:\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device),\n",
    "            \"attention_mask\": torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device),\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        test_preds.append((outputs.start_logits.cpu().numpy(), outputs.end_logits.cpu().numpy()))\n",
    "\n",
    "# ✅ Compute Metrics\n",
    "test_start_logits = np.concatenate([p[0] for p in test_preds], axis=0)\n",
    "test_end_logits = np.concatenate([p[1] for p in test_preds], axis=0)\n",
    "\n",
    "test_metrics = compute_metrics((test_start_logits, test_end_logits), test_list)\n",
    "print(\"Final Test Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_618687/380611664.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
      "/tmp/ipykernel_618687/380611664.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/data/TeQAS 1.1/wandb/run-20250129_101041-cs3oi2b6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison/runs/cs3oi2b6' target=\"_blank\">Baseline_vs_FineTuned_XLMR</a></strong> to <a href='https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison/runs/cs3oi2b6' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison/runs/cs3oi2b6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: xlm-roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Baseline XLM-R (Val): {'em': 0.0, 'f1': 3.1289441386538477}\n",
      "Loading model: xlm-roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Baseline XLM-R (Test): {'em': 0.0, 'f1': 3.48595565464265}\n",
      "Loading model: ./final_xlmr_tel_answerable_3_v2\n",
      "Results for Fine-Tuned XLM-R (Val): {'em': 59.53030303030303, 'f1': 76.88073465726582}\n",
      "Loading model: ./final_xlmr_tel_answerable_3_v2\n",
      "Results for Fine-Tuned XLM-R (Test): {'em': 51.23388581952118, 'f1': 71.65901983874353}\n",
      "\n",
      "### Performance Comparison ###\n",
      "Metric              Baseline XLM-R      Fine-Tuned XLM-R\n",
      "------------------------------------------------------------\n",
      "Validation EM       0.00                59.53\n",
      "Validation F1       3.13                76.88\n",
      "Test EM             0.00                51.23\n",
      "Test F1             3.49                71.66\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Baseline EM (Test)</td><td>▁</td></tr><tr><td>Baseline EM (Val)</td><td>▁</td></tr><tr><td>Baseline F1 (Test)</td><td>▁</td></tr><tr><td>Baseline F1 (Val)</td><td>▁</td></tr><tr><td>Fine-Tuned EM (Test)</td><td>▁</td></tr><tr><td>Fine-Tuned EM (Val)</td><td>▁</td></tr><tr><td>Fine-Tuned F1 (Test)</td><td>▁</td></tr><tr><td>Fine-Tuned F1 (Val)</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Baseline EM (Test)</td><td>0</td></tr><tr><td>Baseline EM (Val)</td><td>0</td></tr><tr><td>Baseline F1 (Test)</td><td>3.48596</td></tr><tr><td>Baseline F1 (Val)</td><td>3.12894</td></tr><tr><td>Fine-Tuned EM (Test)</td><td>51.23389</td></tr><tr><td>Fine-Tuned EM (Val)</td><td>59.5303</td></tr><tr><td>Fine-Tuned F1 (Test)</td><td>71.65902</td></tr><tr><td>Fine-Tuned F1 (Val)</td><td>76.88073</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Baseline_vs_FineTuned_XLMR</strong> at: <a href='https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison/runs/cs3oi2b6' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison/runs/cs3oi2b6</a><br> View project at: <a href='https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_XLMR_Comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250129_101041-cs3oi2b6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Comparison completed!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# compare_xlmr_baseline_vs_finetuned.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import XLMRobertaForQuestionAnswering, XLMRobertaTokenizerFast\n",
    "import wandb\n",
    "\n",
    "############################\n",
    "# Postprocess + EM/F1\n",
    "############################\n",
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text for comparison.\"\"\"\n",
    "    def remove_articles(t):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", t)\n",
    "    def remove_punc(t):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    def white_space_fix(t):\n",
    "        return \" \".join(t.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    \"\"\"Exact match score.\"\"\"\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    \"\"\"F1-score computation.\"\"\"\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"Extract answer spans from model outputs.\"\"\"\n",
    "    preds = {}\n",
    "    for i, ex in enumerate(examples):\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        if start_idx >= len(offsets) or end_idx >= len(offsets) or start_idx > end_idx:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char   = offsets[end_idx][1]\n",
    "        pred_text  = context[start_char:end_char]\n",
    "        \n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, dataset):\n",
    "    \"\"\"Compute EM and F1 metrics.\"\"\"\n",
    "    (start_logits, end_logits) = eval_preds\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    preds_dict = postprocess_qa_predictions(dataset, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in dataset:\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred  = preds_dict.get(ex_id, \"\")\n",
    "        gold  = ex[\"gold_text\"]\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count    += 1\n",
    "\n",
    "    return {\"em\": total_em / count * 100.0, \"f1\": total_f1 / count * 100.0}\n",
    "\n",
    "############################\n",
    "# Load Data\n",
    "############################\n",
    "def load_data():\n",
    "    \"\"\"Load processed datasets.\"\"\"\n",
    "    data_dir = \"processed_telugu_answerable_data\"\n",
    "    val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
    "    test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
    "    val_dataset   = Dataset.from_list(val_list)\n",
    "    test_dataset  = Dataset.from_list(test_list)\n",
    "    return val_dataset, test_dataset\n",
    "\n",
    "############################\n",
    "# Model Evaluation\n",
    "############################\n",
    "def evaluate_model(model_name, dataset, model_alias):\n",
    "    \"\"\"Load a model and evaluate it on the given dataset.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model = XLMRobertaForQuestionAnswering.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "    # Run inference\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for example in dataset:\n",
    "            inputs = {\n",
    "                \"input_ids\": torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(\"cuda\"),\n",
    "                \"attention_mask\": torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(\"cuda\"),\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            test_preds.append((outputs.start_logits.cpu().numpy(), outputs.end_logits.cpu().numpy()))\n",
    "\n",
    "    # Compute metrics\n",
    "    start_logits = np.concatenate([p[0] for p in test_preds], axis=0)\n",
    "    end_logits = np.concatenate([p[1] for p in test_preds], axis=0)\n",
    "    results = compute_metrics((start_logits, end_logits), dataset)\n",
    "    \n",
    "    print(f\"Results for {model_alias}: {results}\")\n",
    "    return results\n",
    "\n",
    "############################\n",
    "# Main Function\n",
    "############################\n",
    "def main():\n",
    "    # Load datasets\n",
    "    val_dataset, test_dataset = load_data()\n",
    "\n",
    "    # Initialize W&B for logging\n",
    "    wandb.init(project=\"TeQAS_XLMR_Comparison\", name=\"Baseline_vs_FineTuned_XLMR\")\n",
    "\n",
    "    # Evaluate Baseline XLM-R\n",
    "    baseline_results_val  = evaluate_model(\"xlm-roberta-large\", val_dataset, \"Baseline XLM-R (Val)\")\n",
    "    baseline_results_test = evaluate_model(\"xlm-roberta-large\", test_dataset, \"Baseline XLM-R (Test)\")\n",
    "\n",
    "    # Evaluate Fine-Tuned XLM-R\n",
    "    finetuned_results_val  = evaluate_model(\"./final_xlmr_tel_answerable_3_v2\", val_dataset, \"Fine-Tuned XLM-R (Val)\")\n",
    "    finetuned_results_test = evaluate_model(\"./final_xlmr_tel_answerable_3_v2\", test_dataset, \"Fine-Tuned XLM-R (Test)\")\n",
    "\n",
    "    # Comparison Table\n",
    "    print(\"\\n### Performance Comparison ###\")\n",
    "    print(f\"{'Metric':<20}{'Baseline XLM-R':<20}{'Fine-Tuned XLM-R'}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Validation EM':<20}{baseline_results_val['em']:<20.2f}{finetuned_results_val['em']:.2f}\")\n",
    "    print(f\"{'Validation F1':<20}{baseline_results_val['f1']:<20.2f}{finetuned_results_val['f1']:.2f}\")\n",
    "    print(f\"{'Test EM':<20}{baseline_results_test['em']:<20.2f}{finetuned_results_test['em']:.2f}\")\n",
    "    print(f\"{'Test F1':<20}{baseline_results_test['f1']:<20.2f}{finetuned_results_test['f1']:.2f}\")\n",
    "\n",
    "    # Log results\n",
    "    wandb.log({\n",
    "        \"Baseline EM (Val)\": baseline_results_val[\"em\"],\n",
    "        \"Baseline F1 (Val)\": baseline_results_val[\"f1\"],\n",
    "        \"Fine-Tuned EM (Val)\": finetuned_results_val[\"em\"],\n",
    "        \"Fine-Tuned F1 (Val)\": finetuned_results_val[\"f1\"],\n",
    "        \"Baseline EM (Test)\": baseline_results_test[\"em\"],\n",
    "        \"Baseline F1 (Test)\": baseline_results_test[\"f1\"],\n",
    "        \"Fine-Tuned EM (Test)\": finetuned_results_test[\"em\"],\n",
    "        \"Fine-Tuned F1 (Test)\": finetuned_results_test[\"f1\"],\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"✅ Comparison completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
