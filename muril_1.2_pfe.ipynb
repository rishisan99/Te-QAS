{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer: google/muril-large-cased\n",
      "Filtering + building train data (English) ...\n",
      "Train answerable size: 80221\n",
      "Filtering + building val data (English) ...\n",
      "Val answerable size: 6600\n",
      "Filtering + building test data (English) ...\n",
      "Test answerable size: 5928\n",
      "\n",
      "Saved final PT files to processed_english_answerable_data_muril/\n",
      "Done! English answerable preprocessing completed with MuRIL Large.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# preprocess_answerable_muril.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer  # We'll use AutoTokenizer for MuRIL\n",
    "\n",
    "# Adjust your paths here\n",
    "train_json = \"English 2.1/squad2.1_train.json\"    # SQuAD 2.0 train with unanswerable\n",
    "val_json   = \"English 2.1/squad2.1_val.json\"      # SQuAD 2.0 val\n",
    "test_json  = \"English 2.1/squad2.1_test.json\"     # SQuAD 2.0 test\n",
    "\n",
    "out_dir    = \"processed_english_answerable_data_muril\"   # where we'll write train.pt, val.pt, test.pt\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "max_length = 512\n",
    "model_tokenizer_name = \"google/muril-large-cased\"  # MuRIL Large\n",
    "\n",
    "###############################################\n",
    "# 1) Filter out unanswerable QAs\n",
    "###############################################\n",
    "def filter_answerable_squad(input_path):\n",
    "    \"\"\"\n",
    "    Returns a new SQuAD JSON dict containing only QAs where is_impossible=False\n",
    "    with at least one answer.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    new_data = {\n",
    "        \"version\": data.get(\"version\", \"filtered_English\"),\n",
    "        \"data\": []\n",
    "    }\n",
    "    for article in data[\"data\"]:\n",
    "        new_paragraphs = []\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            new_qas = []\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                if not qa.get(\"is_impossible\", False) and qa.get(\"answers\"):\n",
    "                    new_qas.append(qa)\n",
    "            if new_qas:\n",
    "                new_paragraphs.append({\n",
    "                    \"context\": context,\n",
    "                    \"qas\": new_qas\n",
    "                })\n",
    "        if new_paragraphs:\n",
    "            new_data[\"data\"].append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"paragraphs\": new_paragraphs\n",
    "            })\n",
    "    return new_data\n",
    "\n",
    "###############################################\n",
    "# 2) Build offset-based examples\n",
    "###############################################\n",
    "def build_answerable_examples(squad_data, tokenizer, max_length=384):\n",
    "    \"\"\"\n",
    "    For each answerable QA:\n",
    "      - tokenize question+context\n",
    "      - find start/end token indices\n",
    "      - store offset_mapping, context, gold_text, etc.\n",
    "    \"\"\"\n",
    "    examples_out = []\n",
    "    for article in squad_data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                ans = qa[\"answers\"][0]\n",
    "                ans_start = ans[\"answer_start\"]\n",
    "                ans_text  = ans[\"text\"]\n",
    "                ans_end   = ans_start + len(ans_text)\n",
    "\n",
    "                enc = tokenizer(\n",
    "                    qa[\"question\"],\n",
    "                    context,\n",
    "                    max_length=max_length,\n",
    "                    truncation=\"only_second\",  # Usually for Q&A we truncate the context\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "\n",
    "                input_ids      = enc[\"input_ids\"][0]\n",
    "                attention_mask = enc[\"attention_mask\"][0]\n",
    "                offset_mapping = enc[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "                # find start/end token indices\n",
    "                start_token = None\n",
    "                end_token   = None\n",
    "                for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "                    if start_char <= ans_start < end_char:\n",
    "                        start_token = i\n",
    "                    if start_char < ans_end <= end_char:\n",
    "                        end_token = i\n",
    "\n",
    "                # fallback if mismatch\n",
    "                if start_token is None or end_token is None or end_token < start_token:\n",
    "                    start_token = 0\n",
    "                    end_token   = 0\n",
    "\n",
    "                ex_item = {\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"start_positions\": torch.tensor(start_token, dtype=torch.long),\n",
    "                    \"end_positions\":   torch.tensor(end_token,   dtype=torch.long),\n",
    "                    \"offset_mapping\":  offset_mapping,\n",
    "                    \"context\":         context,\n",
    "                    \"gold_text\":       ans_text\n",
    "                }\n",
    "                examples_out.append(ex_item)\n",
    "    return examples_out\n",
    "\n",
    "def main():\n",
    "    print(f\"Using tokenizer: {model_tokenizer_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_tokenizer_name)  # MuRIL Large tokenizer\n",
    "\n",
    "    # 1) Filter + build train\n",
    "    print(\"Filtering + building train data (English) ...\")\n",
    "    train_data_raw = filter_answerable_squad(train_json)\n",
    "    train_list = build_answerable_examples(train_data_raw, tokenizer, max_length)\n",
    "    print(f\"Train answerable size: {len(train_list)}\")\n",
    "\n",
    "    # 2) Filter + build val\n",
    "    print(\"Filtering + building val data (English) ...\")\n",
    "    val_data_raw = filter_answerable_squad(val_json)\n",
    "    val_list = build_answerable_examples(val_data_raw, tokenizer, max_length)\n",
    "    print(f\"Val answerable size: {len(val_list)}\")\n",
    "\n",
    "    # 3) Filter + build test\n",
    "    print(\"Filtering + building test data (English) ...\")\n",
    "    test_data_raw = filter_answerable_squad(test_json)\n",
    "    test_list = build_answerable_examples(test_data_raw, tokenizer, max_length)\n",
    "    print(f\"Test answerable size: {len(test_list)}\")\n",
    "\n",
    "    # 4) Save as .pt\n",
    "    train_out = os.path.join(out_dir, \"train.pt\")\n",
    "    val_out   = os.path.join(out_dir, \"val.pt\")\n",
    "    test_out  = os.path.join(out_dir, \"test.pt\")\n",
    "\n",
    "    torch.save(train_list, train_out)\n",
    "    torch.save(val_list,   val_out)\n",
    "    torch.save(test_list,  test_out)\n",
    "\n",
    "    print(f\"\\nSaved final PT files to {out_dir}/\")\n",
    "    print(\"Done! English answerable preprocessing completed with MuRIL Large.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer: google/muril-large-cased\n",
      "Filtering + building train data (TELUGU) ...\n",
      "Train answerable size: 72039\n",
      "Filtering + building val data (TELUGU) ...\n",
      "Val answerable size: 6600\n",
      "Filtering + building test data (TELUGU) ...\n",
      "Test answerable size: 5430\n",
      "\n",
      "Saved final PT files to processed_telugu_answerable_data_muril/\n",
      "Done! Telugu answerable preprocessing completed with MuRIL Large.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# preprocess_answerable_muril.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer  # We'll use AutoTokenizer for MuRIL\n",
    "\n",
    "# Adjust your paths here\n",
    "train_json = \"Telugu 2.1/squad2.1_telugu_train.json\"    # SQuAD 2.0 train with unanswerable\n",
    "val_json   = \"Telugu 2.1/squad2.1_telugu_val.json\"      # SQuAD 2.0 val\n",
    "test_json  = \"Telugu 2.1/squad2.1_telugu_test.json\"     # SQuAD 2.0 test\n",
    "\n",
    "out_dir    = \"processed_telugu_answerable_data_muril\"   # where we'll write train.pt, val.pt, test.pt\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "max_length = 512\n",
    "model_tokenizer_name = \"google/muril-large-cased\"  # MuRIL Large\n",
    "\n",
    "###############################################\n",
    "# 1) Filter out unanswerable QAs\n",
    "###############################################\n",
    "def filter_answerable_squad(input_path):\n",
    "    \"\"\"\n",
    "    Returns a new SQuAD JSON dict containing only QAs where is_impossible=False\n",
    "    with at least one answer.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    new_data = {\n",
    "        \"version\": data.get(\"version\", \"filtered_telugu\"),\n",
    "        \"data\": []\n",
    "    }\n",
    "    for article in data[\"data\"]:\n",
    "        new_paragraphs = []\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            new_qas = []\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                if not qa.get(\"is_impossible\", False) and qa.get(\"answers\"):\n",
    "                    new_qas.append(qa)\n",
    "            if new_qas:\n",
    "                new_paragraphs.append({\n",
    "                    \"context\": context,\n",
    "                    \"qas\": new_qas\n",
    "                })\n",
    "        if new_paragraphs:\n",
    "            new_data[\"data\"].append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"paragraphs\": new_paragraphs\n",
    "            })\n",
    "    return new_data\n",
    "\n",
    "###############################################\n",
    "# 2) Build offset-based examples\n",
    "###############################################\n",
    "def build_answerable_examples(squad_data, tokenizer, max_length=384):\n",
    "    \"\"\"\n",
    "    For each answerable QA:\n",
    "      - tokenize question+context\n",
    "      - find start/end token indices\n",
    "      - store offset_mapping, context, gold_text, etc.\n",
    "    \"\"\"\n",
    "    examples_out = []\n",
    "    for article in squad_data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                ans = qa[\"answers\"][0]\n",
    "                ans_start = ans[\"answer_start\"]\n",
    "                ans_text  = ans[\"text\"]\n",
    "                ans_end   = ans_start + len(ans_text)\n",
    "\n",
    "                enc = tokenizer(\n",
    "                    qa[\"question\"],\n",
    "                    context,\n",
    "                    max_length=max_length,\n",
    "                    truncation=\"only_second\",  # Usually for Q&A we truncate the context\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "\n",
    "                input_ids      = enc[\"input_ids\"][0]\n",
    "                attention_mask = enc[\"attention_mask\"][0]\n",
    "                offset_mapping = enc[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "                # find start/end token indices\n",
    "                start_token = None\n",
    "                end_token   = None\n",
    "                for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "                    if start_char <= ans_start < end_char:\n",
    "                        start_token = i\n",
    "                    if start_char < ans_end <= end_char:\n",
    "                        end_token = i\n",
    "\n",
    "                # fallback if mismatch\n",
    "                if start_token is None or end_token is None or end_token < start_token:\n",
    "                    start_token = 0\n",
    "                    end_token   = 0\n",
    "\n",
    "                ex_item = {\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"start_positions\": torch.tensor(start_token, dtype=torch.long),\n",
    "                    \"end_positions\":   torch.tensor(end_token,   dtype=torch.long),\n",
    "                    \"offset_mapping\":  offset_mapping,\n",
    "                    \"context\":         context,\n",
    "                    \"gold_text\":       ans_text\n",
    "                }\n",
    "                examples_out.append(ex_item)\n",
    "    return examples_out\n",
    "\n",
    "def main():\n",
    "    print(f\"Using tokenizer: {model_tokenizer_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_tokenizer_name)  # MuRIL Large tokenizer\n",
    "\n",
    "    # 1) Filter + build train\n",
    "    print(\"Filtering + building train data (TELUGU) ...\")\n",
    "    train_data_raw = filter_answerable_squad(train_json)\n",
    "    train_list = build_answerable_examples(train_data_raw, tokenizer, max_length)\n",
    "    print(f\"Train answerable size: {len(train_list)}\")\n",
    "\n",
    "    # 2) Filter + build val\n",
    "    print(\"Filtering + building val data (TELUGU) ...\")\n",
    "    val_data_raw = filter_answerable_squad(val_json)\n",
    "    val_list = build_answerable_examples(val_data_raw, tokenizer, max_length)\n",
    "    print(f\"Val answerable size: {len(val_list)}\")\n",
    "\n",
    "    # 3) Filter + build test\n",
    "    print(\"Filtering + building test data (TELUGU) ...\")\n",
    "    test_data_raw = filter_answerable_squad(test_json)\n",
    "    test_list = build_answerable_examples(test_data_raw, tokenizer, max_length)\n",
    "    print(f\"Test answerable size: {len(test_list)}\")\n",
    "\n",
    "    # 4) Save as .pt\n",
    "    train_out = os.path.join(out_dir, \"train.pt\")\n",
    "    val_out   = os.path.join(out_dir, \"val.pt\")\n",
    "    test_out  = os.path.join(out_dir, \"test.pt\")\n",
    "\n",
    "    torch.save(train_list, train_out)\n",
    "    torch.save(val_list,   val_out)\n",
    "    torch.save(test_list,  test_out)\n",
    "\n",
    "    print(f\"\\nSaved final PT files to {out_dir}/\")\n",
    "    print(\"Done! Telugu answerable preprocessing completed with MuRIL Large.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-27 18:08:18.073976: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_2854/1575471374.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_list = torch.load(os.path.join(data_dir, \"train.pt\"))\n",
      "/tmp/ipykernel_2854/1575471374.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
      "/tmp/ipykernel_2854/1575471374.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/data/TeQAS 1.2/wandb/run-20250127_182521-2ip9l5mv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2/runs/2ip9l5mv' target=\"_blank\">MuRIL_Eng_1</a></strong> to <a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_1.2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2/runs/2ip9l5mv' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_1.2/runs/2ip9l5mv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/muril-large-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google/muril-large-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1738002437.166616] [101f5ae8f935:2854 :f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5014' max='5014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5014/5014 13:46:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Em</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.750400</td>\n",
       "      <td>0.777438</td>\n",
       "      <td>71.181818</td>\n",
       "      <td>84.391236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.589800</td>\n",
       "      <td>0.754629</td>\n",
       "      <td>72.090909</td>\n",
       "      <td>85.245737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Model + checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final val metrics: {'eval_loss': 0.7546290755271912, 'eval_em': 72.0909090909091, 'eval_f1': 85.24573695226366, 'eval_runtime': 600.5605, 'eval_samples_per_second': 10.99, 'eval_steps_per_second': 0.345, 'epoch': 2.0}\n",
      "⚠️ Warning: Logits size 5928 doesn't match dataset size 6600. Truncating...\n",
      "Test set metrics: {'em': 69.77058029689609, 'f1': 83.99227358899536}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# fine_tune_answerable_muril.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "from datasets import Dataset\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "import wandb\n",
    "\n",
    "############################\n",
    "# Postprocess + EM/F1\n",
    "############################\n",
    "def normalize_text(s):\n",
    "    def remove_articles(t):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", t)\n",
    "    def remove_punc(t):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    def white_space_fix(t):\n",
    "        return \" \".join(t.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"\n",
    "    Postprocesses model predictions to extract answer text.\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    num_examples = len(examples)\n",
    "    num_logits = len(start_logits)\n",
    "\n",
    "    # Safety check\n",
    "    if num_logits != num_examples:\n",
    "        print(f\"⚠️ Warning: Logits size {num_logits} doesn't match dataset size {num_examples}. Truncating...\")\n",
    "        num_examples = min(num_examples, num_logits)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        ex = examples[i]\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        if len(start_logits[i]) == 0 or len(end_logits[i]) == 0:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Get best start/end indices\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        if start_idx >= len(offsets) or end_idx >= len(offsets) or start_idx > end_idx:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Extract the predicted answer span\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char   = offsets[end_idx][1]\n",
    "        pred_text  = context[start_char:end_char]\n",
    "        \n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, dataset):\n",
    "    \"\"\"\n",
    "    eval_preds => (start_logits, end_logits)\n",
    "    dataset => the raw examples with gold_text\n",
    "    \"\"\"\n",
    "    (start_logits, end_logits) = eval_preds\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    preds_dict = postprocess_qa_predictions(dataset, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in dataset:\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred  = preds_dict.get(ex_id, \"\")\n",
    "        gold  = ex[\"gold_text\"]\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count    += 1\n",
    "\n",
    "    em_val = total_em / count * 100.0\n",
    "    f1_val = total_f1 / count * 100.0\n",
    "    return {\n",
    "        \"em\": em_val,\n",
    "        \"f1\": f1_val\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # 1) Load .pt files\n",
    "    data_dir = \"processed_english_answerable_data_muril\"  # from previous script\n",
    "    train_list = torch.load(os.path.join(data_dir, \"train.pt\"))\n",
    "    val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
    "    test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
    "\n",
    "    # 2) Convert to huggingface Datasets\n",
    "    train_dataset = Dataset.from_list(train_list)\n",
    "    val_dataset   = Dataset.from_list(val_list)\n",
    "    test_dataset  = Dataset.from_list(test_list)\n",
    "\n",
    "    # Initialize Weights & Biases (optional)\n",
    "    wandb.init(project=\"TeQAS_1.2\", name=\"MuRIL_Eng_1\")\n",
    "\n",
    "    # 3) Initialize model (MuRIL Large)\n",
    "    model_name = \"google/muril-large-cased\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"checkpoints_muril_eng_answerable_v2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,            # Adjust as needed\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"logs_muril_answerable\",\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    def hf_compute_metrics(p: EvalPrediction):\n",
    "        # p.predictions => (start_logits, end_logits)\n",
    "        return compute_metrics(p.predictions, val_dataset)\n",
    "\n",
    "    # Trainer Initialization\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        tokenizer=None,  # Not strictly needed since we don't do on-the-fly tokenization\n",
    "        compute_metrics=hf_compute_metrics\n",
    "    )\n",
    "\n",
    "    # 4) Train\n",
    "    trainer.train()\n",
    "\n",
    "    # 5) Save final model\n",
    "    trainer.save_model(\"final_muril_eng_answerable_v2\")\n",
    "    print(\"Done! Model + checkpoint saved.\")\n",
    "\n",
    "    # Evaluate on val => see final\n",
    "    final_val_metrics = trainer.evaluate()\n",
    "    print(\"Final val metrics:\", final_val_metrics)\n",
    "\n",
    "    # Evaluate on test set => final test metrics\n",
    "    test_preds = trainer.predict(test_dataset)\n",
    "    final_test_metrics = compute_metrics(test_preds.predictions, test_list)\n",
    "    print(\"Test set metrics:\", final_test_metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telugu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-28 08:37:41.048232: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_625614/1253372809.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_list = torch.load(os.path.join(data_dir, \"train.pt\"))\n",
      "/tmp/ipykernel_625614/1253372809.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
      "/tmp/ipykernel_625614/1253372809.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msanthoshrishi9999\u001b[0m (\u001b[33msanthosh-rishi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/data/TeQAS 1.2/wandb/run-20250128_084102-6678u00r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2/runs/6678u00r' target=\"_blank\">MuRIL_Telugu_3</a></strong> to <a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_1.2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2/runs/6678u00r' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_1.2/runs/6678u00r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: ./final_muril_eng_answerable_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6756' max='6756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6756/6756 15:20:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Em</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.184500</td>\n",
       "      <td>1.169649</td>\n",
       "      <td>60.196970</td>\n",
       "      <td>77.287197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>1.184296</td>\n",
       "      <td>60.318182</td>\n",
       "      <td>77.901823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>1.254607</td>\n",
       "      <td>60.893939</td>\n",
       "      <td>78.192110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Model + checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Logits size 5430 doesn't match dataset size 6600. Truncating...\n",
      "Test set metrics: {'em': 51.60220994475138, 'f1': 72.08715583242966}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/em</td><td>▁▂█</td></tr><tr><td>eval/f1</td><td>▁▆█</td></tr><tr><td>eval/loss</td><td>▁▂█</td></tr><tr><td>eval/runtime</td><td>▆▁█</td></tr><tr><td>eval/samples_per_second</td><td>▃█▁</td></tr><tr><td>eval/steps_per_second</td><td>▃█▁</td></tr><tr><td>test/em</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▂▂▃▂▁▂▃█▂▂▂▃▁▂▄▁▁▂▁▂▃▂▂▄▁▂▃▅▃▃▂▁▃▂▂▃▂▂▂</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▇▆▆▆▆▆▆▆▅▅▆▅▃▃▃▄▃▃▃▃▃▃▄▃▃▂▁▁▂▁▂▁▁▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/em</td><td>60.89394</td></tr><tr><td>eval/f1</td><td>78.19211</td></tr><tr><td>eval/loss</td><td>1.25461</td></tr><tr><td>eval/runtime</td><td>628.2416</td></tr><tr><td>eval/samples_per_second</td><td>10.506</td></tr><tr><td>eval/steps_per_second</td><td>0.329</td></tr><tr><td>test/em</td><td>0.04545</td></tr><tr><td>test/f1</td><td>2.4011</td></tr><tr><td>test/loss</td><td>1.66918</td></tr><tr><td>test/runtime</td><td>507.0632</td></tr><tr><td>test/samples_per_second</td><td>10.709</td></tr><tr><td>test/steps_per_second</td><td>0.335</td></tr><tr><td>total_flos</td><td>2.0070936901772698e+17</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>6756</td></tr><tr><td>train/grad_norm</td><td>7.15012</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.726</td></tr><tr><td>train_loss</td><td>0.99468</td></tr><tr><td>train_runtime</td><td>55260.5214</td></tr><tr><td>train_samples_per_second</td><td>3.911</td></tr><tr><td>train_steps_per_second</td><td>0.122</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MuRIL_Telugu_3</strong> at: <a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2/runs/6678u00r' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_1.2/runs/6678u00r</a><br> View project at: <a href='https://wandb.ai/santhosh-rishi/TeQAS_1.2' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_1.2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250128_084102-6678u00r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# fine_tune_answerable_muril.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "from datasets import Dataset\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "import wandb\n",
    "\n",
    "############################\n",
    "# Postprocess + EM/F1\n",
    "############################\n",
    "def normalize_text(s):\n",
    "    def remove_articles(t):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", t)\n",
    "    def remove_punc(t):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    def white_space_fix(t):\n",
    "        return \" \".join(t.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"\n",
    "    Postprocesses model predictions to extract answer text.\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    num_examples = len(examples)\n",
    "    num_logits = len(start_logits)\n",
    "\n",
    "    # Safety check\n",
    "    if num_logits != num_examples:\n",
    "        print(f\"⚠️ Warning: Logits size {num_logits} doesn't match dataset size {num_examples}. Truncating...\")\n",
    "        num_examples = min(num_examples, num_logits)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        ex = examples[i]\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        if len(start_logits[i]) == 0 or len(end_logits[i]) == 0:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Get best start/end indices\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        if start_idx >= len(offsets) or end_idx >= len(offsets) or start_idx > end_idx:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Extract the predicted answer span\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char   = offsets[end_idx][1]\n",
    "        pred_text  = context[start_char:end_char]\n",
    "        \n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, dataset):\n",
    "    \"\"\"\n",
    "    eval_preds => (start_logits, end_logits)\n",
    "    dataset => the raw examples with gold_text\n",
    "    \"\"\"\n",
    "    (start_logits, end_logits) = eval_preds\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    preds_dict = postprocess_qa_predictions(dataset, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in dataset:\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred  = preds_dict.get(ex_id, \"\")\n",
    "        gold  = ex[\"gold_text\"]\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count    += 1\n",
    "\n",
    "    em_val = total_em / count * 100.0\n",
    "    f1_val = total_f1 / count * 100.0\n",
    "    return {\n",
    "        \"em\": em_val,\n",
    "        \"f1\": f1_val\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # 1) Load .pt files\n",
    "    data_dir = \"processed_telugu_answerable_data_muril\"  # from previous script\n",
    "    train_list = torch.load(os.path.join(data_dir, \"train.pt\"))\n",
    "    val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
    "    test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
    "\n",
    "    # 2) Convert to huggingface Datasets\n",
    "    train_dataset = Dataset.from_list(train_list)\n",
    "    val_dataset   = Dataset.from_list(val_list)\n",
    "    test_dataset  = Dataset.from_list(test_list)\n",
    "\n",
    "    # Initialize Weights & Biases (optional)\n",
    "    wandb.init(project=\"TeQAS_1.2\", name=\"MuRIL_Telugu_3\")\n",
    "\n",
    "    # 3) Initialize model (MuRIL Large)\n",
    "    model_name = \"./final_muril_eng_answerable_v2\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"checkpoints_muril_tel_answerable_v2\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,            # Adjust as needed\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"logs_muril_answerable\",\n",
    "        logging_steps=100\n",
    "    )\n",
    "\n",
    "    def hf_compute_metrics(p: EvalPrediction):\n",
    "        # p.predictions => (start_logits, end_logits)\n",
    "        return compute_metrics(p.predictions, val_dataset)\n",
    "\n",
    "    # Trainer Initialization\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        tokenizer=None,  # Not strictly needed since we don't do on-the-fly tokenization\n",
    "        compute_metrics=hf_compute_metrics\n",
    "    )\n",
    "\n",
    "    # 4) Train\n",
    "    trainer.train()\n",
    "\n",
    "    # 5) Save final model\n",
    "    trainer.save_model(\"final_muril_tel_answerable_v2\")\n",
    "    print(\"Done! Model + checkpoint saved.\")\n",
    "\n",
    "    # # Evaluate on val => see final\n",
    "    # final_val_metrics = trainer.evaluate()\n",
    "    # print(\"Final val metrics:\", final_val_metrics)\n",
    "\n",
    "    # Evaluate on test set => final test metrics\n",
    "    test_preds = trainer.predict(test_dataset)\n",
    "    final_test_metrics = compute_metrics(test_preds.predictions, test_list)\n",
    "    print(\"Test set metrics:\", final_test_metrics)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_625614/535255819.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
      "/tmp/ipykernel_625614/535255819.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/data/TeQAS 1.2/wandb/run-20250129_051146-gl5j9qlw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/santhosh-rishi/TeQAS_Comparison/runs/gl5j9qlw' target=\"_blank\">MuRIL_Baseline_vs_FineTuned</a></strong> to <a href='https://wandb.ai/santhosh-rishi/TeQAS_Comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/santhosh-rishi/TeQAS_Comparison' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_Comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/santhosh-rishi/TeQAS_Comparison/runs/gl5j9qlw' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_Comparison/runs/gl5j9qlw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/muril-large-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google/muril-large-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 09:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Baseline MuRIL (Val): {'eval_loss': 6.258428573608398, 'eval_em': 0.0, 'eval_f1': 2.3042203328654196, 'eval_runtime': 643.545, 'eval_samples_per_second': 10.256, 'eval_steps_per_second': 0.322}\n",
      "Loading model: google/muril-large-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google/muril-large-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 07:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Baseline MuRIL (Test): {'eval_loss': 6.2398481369018555, 'eval_em': 0.0, 'eval_f1': 4.231857038797818, 'eval_runtime': 499.9708, 'eval_samples_per_second': 10.861, 'eval_steps_per_second': 0.34}\n",
      "Loading model: ./final_muril_tel_answerable_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 08:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Fine-Tuned MuRIL (Val): {'eval_loss': 1.2546072006225586, 'eval_em': 60.89393939393939, 'eval_f1': 78.19210983489383, 'eval_runtime': 592.3048, 'eval_samples_per_second': 11.143, 'eval_steps_per_second': 0.349}\n",
      "Loading model: ./final_muril_tel_answerable_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 08:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Fine-Tuned MuRIL (Test): {'eval_loss': 1.6691776514053345, 'eval_em': 51.60220994475138, 'eval_f1': 72.08715583242966, 'eval_runtime': 530.972, 'eval_samples_per_second': 10.227, 'eval_steps_per_second': 0.32}\n",
      "\n",
      "### Performance Comparison ###\n",
      "Metric              Baseline MuRIL      Fine-Tuned MuRIL\n",
      "------------------------------------------------------------\n",
      "Validation EM       0.00                60.89\n",
      "Validation F1       2.30                78.19\n",
      "Test EM             0.00                51.60\n",
      "Test F1             4.23                72.09\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Baseline EM (Test)</td><td>▁</td></tr><tr><td>Baseline EM (Val)</td><td>▁</td></tr><tr><td>Baseline F1 (Test)</td><td>▁</td></tr><tr><td>Baseline F1 (Val)</td><td>▁</td></tr><tr><td>Fine-Tuned EM (Test)</td><td>▁</td></tr><tr><td>Fine-Tuned EM (Val)</td><td>▁</td></tr><tr><td>Fine-Tuned F1 (Test)</td><td>▁</td></tr><tr><td>Fine-Tuned F1 (Val)</td><td>▁</td></tr><tr><td>eval/em</td><td>▁▁█▇</td></tr><tr><td>eval/f1</td><td>▁▁█▇</td></tr><tr><td>eval/loss</td><td>██▁▂</td></tr><tr><td>eval/runtime</td><td>█▁▆▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▆█▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▆█▁</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Baseline EM (Test)</td><td>0</td></tr><tr><td>Baseline EM (Val)</td><td>0</td></tr><tr><td>Baseline F1 (Test)</td><td>4.23186</td></tr><tr><td>Baseline F1 (Val)</td><td>2.30422</td></tr><tr><td>Fine-Tuned EM (Test)</td><td>51.60221</td></tr><tr><td>Fine-Tuned EM (Val)</td><td>60.89394</td></tr><tr><td>Fine-Tuned F1 (Test)</td><td>72.08716</td></tr><tr><td>Fine-Tuned F1 (Val)</td><td>78.19211</td></tr><tr><td>eval/em</td><td>51.60221</td></tr><tr><td>eval/f1</td><td>72.08716</td></tr><tr><td>eval/loss</td><td>1.66918</td></tr><tr><td>eval/runtime</td><td>530.972</td></tr><tr><td>eval/samples_per_second</td><td>10.227</td></tr><tr><td>eval/steps_per_second</td><td>0.32</td></tr><tr><td>train/global_step</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MuRIL_Baseline_vs_FineTuned</strong> at: <a href='https://wandb.ai/santhosh-rishi/TeQAS_Comparison/runs/gl5j9qlw' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_Comparison/runs/gl5j9qlw</a><br> View project at: <a href='https://wandb.ai/santhosh-rishi/TeQAS_Comparison' target=\"_blank\">https://wandb.ai/santhosh-rishi/TeQAS_Comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250129_051146-gl5j9qlw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Comparison completed!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# compare_muril_baseline_vs_finetuned.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "from datasets import Dataset\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "import wandb\n",
    "\n",
    "############################\n",
    "# Postprocess + EM/F1\n",
    "############################\n",
    "def normalize_text(s):\n",
    "    \"\"\"Normalize text for comparison.\"\"\"\n",
    "    def remove_articles(t):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", t)\n",
    "    def remove_punc(t):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", t)\n",
    "    def white_space_fix(t):\n",
    "        return \" \".join(t.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    \"\"\"Exact match score.\"\"\"\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    \"\"\"F1-score computation.\"\"\"\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"Extract answer spans from model outputs.\"\"\"\n",
    "    preds = {}\n",
    "    for i, ex in enumerate(examples):\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Get best start/end indices\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        if start_idx >= len(offsets) or end_idx >= len(offsets) or start_idx > end_idx:\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Extract the predicted answer span\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char   = offsets[end_idx][1]\n",
    "        pred_text  = context[start_char:end_char]\n",
    "        \n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, dataset):\n",
    "    \"\"\"Compute EM and F1 metrics.\"\"\"\n",
    "    (start_logits, end_logits) = eval_preds\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    preds_dict = postprocess_qa_predictions(dataset, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in dataset:\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred  = preds_dict.get(ex_id, \"\")\n",
    "        gold  = ex[\"gold_text\"]\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count    += 1\n",
    "\n",
    "    em_val = total_em / count * 100.0\n",
    "    f1_val = total_f1 / count * 100.0\n",
    "    return {\"em\": em_val, \"f1\": f1_val}\n",
    "\n",
    "############################\n",
    "# Load Data\n",
    "############################\n",
    "def load_data():\n",
    "    \"\"\"Load processed datasets.\"\"\"\n",
    "    data_dir = \"processed_telugu_answerable_data_muril\"\n",
    "    val_list   = torch.load(os.path.join(data_dir, \"val.pt\"))\n",
    "    test_list  = torch.load(os.path.join(data_dir, \"test.pt\"))\n",
    "    val_dataset   = Dataset.from_list(val_list)\n",
    "    test_dataset  = Dataset.from_list(test_list)\n",
    "    return val_dataset, test_dataset\n",
    "\n",
    "############################\n",
    "# Model Evaluation\n",
    "############################\n",
    "def evaluate_model(model_name, dataset, model_alias):\n",
    "    \"\"\"Load a model and evaluate it on the given dataset.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define Training Arguments (for evaluation)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"checkpoints_muril_comparison\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        do_train=False,\n",
    "        do_eval=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics(p.predictions, dataset)\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Results for {model_alias}: {results}\")\n",
    "    return results\n",
    "\n",
    "############################\n",
    "# Main Function\n",
    "############################\n",
    "def main():\n",
    "    # Load datasets\n",
    "    val_dataset, test_dataset = load_data()\n",
    "\n",
    "    # Initialize W&B for logging\n",
    "    wandb.init(project=\"TeQAS_Comparison\", name=\"MuRIL_Baseline_vs_FineTuned\")\n",
    "\n",
    "    # Evaluate Baseline MuRIL\n",
    "    baseline_results_val  = evaluate_model(\"google/muril-large-cased\", val_dataset, \"Baseline MuRIL (Val)\")\n",
    "    baseline_results_test = evaluate_model(\"google/muril-large-cased\", test_dataset, \"Baseline MuRIL (Test)\")\n",
    "\n",
    "    # Evaluate Fine-Tuned MuRIL\n",
    "    finetuned_results_val  = evaluate_model(\"./final_muril_tel_answerable_v2\", val_dataset, \"Fine-Tuned MuRIL (Val)\")\n",
    "    finetuned_results_test = evaluate_model(\"./final_muril_tel_answerable_v2\", test_dataset, \"Fine-Tuned MuRIL (Test)\")\n",
    "\n",
    "    # Comparison Table\n",
    "    print(\"\\n### Performance Comparison ###\")\n",
    "    print(f\"{'Metric':<20}{'Baseline MuRIL':<20}{'Fine-Tuned MuRIL'}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Validation EM':<20}{baseline_results_val['eval_em']:<20.2f}{finetuned_results_val['eval_em']:.2f}\")\n",
    "    print(f\"{'Validation F1':<20}{baseline_results_val['eval_f1']:<20.2f}{finetuned_results_val['eval_f1']:.2f}\")\n",
    "    print(f\"{'Test EM':<20}{baseline_results_test['eval_em']:<20.2f}{finetuned_results_test['eval_em']:.2f}\")\n",
    "    print(f\"{'Test F1':<20}{baseline_results_test['eval_f1']:<20.2f}{finetuned_results_test['eval_f1']:.2f}\")\n",
    "\n",
    "    # Log results\n",
    "    wandb.log({\n",
    "        \"Baseline EM (Val)\": baseline_results_val[\"eval_em\"],\n",
    "        \"Baseline F1 (Val)\": baseline_results_val[\"eval_f1\"],\n",
    "        \"Fine-Tuned EM (Val)\": finetuned_results_val[\"eval_em\"],\n",
    "        \"Fine-Tuned F1 (Val)\": finetuned_results_val[\"eval_f1\"],\n",
    "        \"Baseline EM (Test)\": baseline_results_test[\"eval_em\"],\n",
    "        \"Baseline F1 (Test)\": baseline_results_test[\"eval_f1\"],\n",
    "        \"Fine-Tuned EM (Test)\": finetuned_results_test[\"eval_em\"],\n",
    "        \"Fine-Tuned F1 (Test)\": finetuned_results_test[\"eval_f1\"],\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"✅ Comparison completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table 1: XLM-R Full Data Performance (Validation & Test)\n",
      "==================================================================================================================\n",
      "| Metric                 | Baseline (Validation) | Fine-Tuned (Validation) | Baseline (Test) | Fine-Tuned (Test) |\n",
      "|------------------------|-----------------------|-------------------------|-----------------|-------------------|\n",
      "| Exact Match (EM)       | 0.09                  | 55.52                   | 0.2             | 61.14             |\n",
      "| F1 Score               | 0.12                  | 69.59                   | 0.22            | 70.65             |\n",
      "| Is Impossible Accuracy | 39.15                 | 87.42                   | 39.31           | 82.2              |\n",
      "==================================================================================================================\n",
      "\n",
      "\n",
      "Table 2: MuRIL Full Data Performance (Validation & Test)\n",
      "==================================================================================================================\n",
      "| Metric                 | Baseline (Validation) | Fine-Tuned (Validation) | Baseline (Test) | Fine-Tuned (Test) |\n",
      "|------------------------|-----------------------|-------------------------|-----------------|-------------------|\n",
      "| Exact Match (EM)       | 0.13                  | 57.08                   | 0.24            | 58.94             |\n",
      "| F1 Score               | 0.15                  | 71.84                   | 0.26            | 69.9              |\n",
      "| Is Impossible Accuracy | 53.68                 | 80.8                    | 47.34           | 75.0              |\n",
      "==================================================================================================================\n",
      "\n",
      "\n",
      "Table 3: XLM-R Answerable Only Performance\n",
      "============================================================================================================\n",
      "| Metric           | Baseline (Validation) | Fine-Tuned (Validation) | Baseline (Test) | Fine-Tuned (Test) |\n",
      "|------------------|-----------------------|-------------------------|-----------------|-------------------|\n",
      "| Exact Match (EM) | 0.0                   | 59.53                   | 0.0             | 51.23             |\n",
      "| F1 Score         | 3.13                  | 76.88                   | 3.49            | 71.66             |\n",
      "============================================================================================================\n",
      "\n",
      "\n",
      "Table 4: MuRIL Answerable Only Performance\n",
      "============================================================================================================\n",
      "| Metric           | Baseline (Validation) | Fine-Tuned (Validation) | Baseline (Test) | Fine-Tuned (Test) |\n",
      "|------------------|-----------------------|-------------------------|-----------------|-------------------|\n",
      "| Exact Match (EM) | 0.0                   | 60.89                   | 0.0             | 51.6              |\n",
      "| F1 Score         | 2.3                   | 78.19                   | 4.23            | 72.09             |\n",
      "============================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set display options for better formatting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x if isinstance(x, (int, float)) else str(x))\n",
    "\n",
    "def print_table_with_header(df, title):\n",
    "   # Get the maximum width of each column for formatting\n",
    "   col_widths = [max(len(str(x)) for x in df[col]) for col in df.columns]\n",
    "   col_widths = [max(len(col), width) for col, width in zip(df.columns, col_widths)]\n",
    "   \n",
    "   # Create header separator\n",
    "   header_sep = \"=\" * (sum(col_widths) + (len(col_widths) - 1) * 3 + 4)\n",
    "   \n",
    "   # Print title\n",
    "   print(f\"\\n{title}\")\n",
    "   print(header_sep)\n",
    "   \n",
    "   # Format and print header\n",
    "   header = \" | \".join(f\"{col:{width}}\" for col, width in zip(df.columns, col_widths))\n",
    "   print(f\"| {header} |\")\n",
    "   \n",
    "   # Print separator after header\n",
    "   print(\"|\" + \"|\".join(\"-\" * (width + 2) for width in col_widths) + \"|\")\n",
    "   \n",
    "   # Print each row\n",
    "   for _, row in df.iterrows():\n",
    "       row_str = \" | \".join(f\"{str(val):{width}}\" for val, width in zip(row, col_widths))\n",
    "       print(f\"| {row_str} |\")\n",
    "   \n",
    "   print(header_sep + \"\\n\")\n",
    "\n",
    "# XLM-R Full Data\n",
    "xlmr_full_val_test = pd.DataFrame({\n",
    "   \"Metric\": [\"Exact Match (EM)\", \"F1 Score\", \"Is Impossible Accuracy\"],\n",
    "   \"Baseline (Validation)\": [0.09, 0.12, 39.15],\n",
    "   \"Fine-Tuned (Validation)\": [55.52, 69.59, 87.42],\n",
    "   \"Baseline (Test)\": [0.20, 0.22, 39.31],\n",
    "   \"Fine-Tuned (Test)\": [61.14, 70.65, 82.20]\n",
    "})\n",
    "\n",
    "# XLM-R Answerable Only\n",
    "xlmr_answerable_val_test = pd.DataFrame({\n",
    "   \"Metric\": [\"Exact Match (EM)\", \"F1 Score\"],\n",
    "   \"Baseline (Validation)\": [0.00, 3.13],\n",
    "   \"Fine-Tuned (Validation)\": [59.53, 76.88],\n",
    "   \"Baseline (Test)\": [0.00, 3.49],\n",
    "   \"Fine-Tuned (Test)\": [51.23, 71.66]\n",
    "})\n",
    "\n",
    "# MuRIL Full Data\n",
    "muril_full_val_test = pd.DataFrame({\n",
    "   \"Metric\": [\"Exact Match (EM)\", \"F1 Score\", \"Is Impossible Accuracy\"],\n",
    "   \"Baseline (Validation)\": [0.13, 0.15, 53.68],\n",
    "   \"Fine-Tuned (Validation)\": [57.08, 71.84, 80.80],\n",
    "   \"Baseline (Test)\": [0.24, 0.26, 47.34],\n",
    "   \"Fine-Tuned (Test)\": [58.94, 69.90, 75.00]\n",
    "})\n",
    "\n",
    "# MuRIL Answerable Only\n",
    "muril_answerable_val_test = pd.DataFrame({\n",
    "   \"Metric\": [\"Exact Match (EM)\", \"F1 Score\"],\n",
    "   \"Baseline (Validation)\": [0.00, 2.30],\n",
    "   \"Fine-Tuned (Validation)\": [60.89, 78.19],\n",
    "   \"Baseline (Test)\": [0.00, 4.23],\n",
    "   \"Fine-Tuned (Test)\": [51.60, 72.09]\n",
    "})\n",
    "\n",
    "# Print all tables with proper formatting\n",
    "print_table_with_header(xlmr_full_val_test, \"Table 1: XLM-R Full Data Performance (Validation & Test)\")\n",
    "print_table_with_header(muril_full_val_test, \"Table 2: MuRIL Full Data Performance (Validation & Test)\")\n",
    "print_table_with_header(xlmr_answerable_val_test, \"Table 3: XLM-R Answerable Only Performance\")\n",
    "print_table_with_header(muril_answerable_val_test, \"Table 4: MuRIL Answerable Only Performance\")\n",
    "\n",
    "# Optionally save to CSV\n",
    "xlmr_full_val_test.to_csv('xlmr_full_performance.csv', index=False)\n",
    "xlmr_answerable_val_test.to_csv('xlmr_answerable_performance.csv', index=False)\n",
    "muril_full_val_test.to_csv('muril_full_performance.csv', index=False)\n",
    "muril_answerable_val_test.to_csv('muril_answerable_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
