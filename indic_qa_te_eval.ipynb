{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_squad_format(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    squad_data = {\n",
    "        \"version\": \"v2.0\",\n",
    "        \"data\": []\n",
    "    }\n",
    "\n",
    "    for article in data[\"data\"]:\n",
    "        article_data = {\n",
    "            \"title\": article.get(\"title\", \"\"),\n",
    "            \"paragraphs\": []\n",
    "        }\n",
    "\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            paragraph_data = {\n",
    "                \"context\": paragraph[\"context\"],\n",
    "                \"qas\": []\n",
    "            }\n",
    "\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                # Check if the question is impossible (i.e., has an empty answer)\n",
    "                if qa[\"answers\"][0][\"text\"] == \"\":\n",
    "                    # Impossible question\n",
    "                    qas = {\n",
    "                        \"id\": qa[\"id\"],\n",
    "                        \"question\": qa[\"question\"],\n",
    "                        \"answers\": [{\"text\": \"\", \"answer_start\": None}],\n",
    "                        \"is_impossible\": True\n",
    "                    }\n",
    "                else:\n",
    "                    # Answerable question\n",
    "                    qas = {\n",
    "                        \"id\": qa[\"id\"],\n",
    "                        \"question\": qa[\"question\"],\n",
    "                        \"answers\": [{\"text\": qa[\"answers\"][0][\"text\"], \"answer_start\": qa[\"answers\"][0][\"answer_start\"]}],\n",
    "                        \"is_impossible\": False\n",
    "                    }\n",
    "                \n",
    "                paragraph_data[\"qas\"].append(qas)\n",
    "            article_data[\"paragraphs\"].append(paragraph_data)\n",
    "\n",
    "        squad_data[\"data\"].append(article_data)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(squad_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage:\n",
    "input_file = \"indicqa.te.json\"  # Path to the original IndicQA.TE file\n",
    "output_file = \"indicqa_squad_format.json\"  # Path to save the converted SQuAD 2.0 file\n",
    "convert_to_squad_format(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Assigning Context IDs\n",
      "==================================================\n",
      "Reading input file: indicqa_squad_format.json\n",
      "Assigning context IDs...\n",
      "Writing contexts CSV to contexts.csv\n",
      "Writing modified JSON to indicqa_squad_with_ids.json\n",
      "Processed 250 contexts\n",
      "\n",
      "Step 2: Creating Context Splits\n",
      "==================================================\n",
      "Reading JSON with context IDs: indicqa_squad_with_ids.json\n",
      "Creating context splits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 18203.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing splits to context_splits.json\n",
      "\n",
      "Processing complete!\n",
      "Total contexts processed: 250\n",
      "Total sub-contexts created: 1896\n",
      "Average splits per context: 7.6\n",
      "\n",
      "All steps completed successfully!\n",
      "1. JSON with context IDs: indicqa_squad_with_ids.json\n",
      "2. Contexts CSV: contexts.csv\n",
      "3. Context splits JSON: context_splits.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "OVERLAP_SIZE = 100  # Characters of overlap between chunks\n",
    "\n",
    "def assign_context_ids(input_path, output_path, contexts_csv_path):\n",
    "    \"\"\"\n",
    "    Step 1: Assign unique IDs to contexts and save both modified JSON and CSV\n",
    "    \"\"\"\n",
    "    print(f\"Reading input file: {input_path}\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    csv_rows = []\n",
    "    context_id = 1\n",
    "    modified_data = {\"version\": data[\"version\"], \"data\": []}\n",
    "    \n",
    "    print(\"Assigning context IDs...\")\n",
    "    for article in data[\"data\"]:\n",
    "        new_article = {\n",
    "            \"title\": article.get(\"title\", \"\"),\n",
    "            \"paragraphs\": []\n",
    "        }\n",
    "        \n",
    "        for para in article[\"paragraphs\"]:\n",
    "            cid = f\"c_{context_id:03d}\"\n",
    "            \n",
    "            csv_rows.append({\n",
    "                \"context_id\": cid,\n",
    "                \"context\": para[\"context\"]\n",
    "            })\n",
    "            \n",
    "            new_para = {\n",
    "                \"context\": para[\"context\"],\n",
    "                \"context_id\": cid,\n",
    "                \"qas\": para[\"qas\"]\n",
    "            }\n",
    "            new_article[\"paragraphs\"].append(new_para)\n",
    "            context_id += 1\n",
    "        \n",
    "        modified_data[\"data\"].append(new_article)\n",
    "    \n",
    "    print(f\"Writing contexts CSV to {contexts_csv_path}\")\n",
    "    with open(contexts_csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"context_id\", \"context\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_rows)\n",
    "    \n",
    "    print(f\"Writing modified JSON to {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(modified_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Processed {context_id-1} contexts\")\n",
    "    return modified_data\n",
    "\n",
    "def find_word_boundary(text, pos, forward=True):\n",
    "    \"\"\"\n",
    "    Find the nearest word boundary from pos, moving either forward or backward\n",
    "    \"\"\"\n",
    "    if forward:\n",
    "        space_pos = text.find(' ', pos)\n",
    "        return len(text) if space_pos == -1 else space_pos\n",
    "    else:\n",
    "        space_pos = text.rfind(' ', 0, pos)\n",
    "        return 0 if space_pos == -1 else space_pos + 1\n",
    "\n",
    "def split_context(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP_SIZE):\n",
    "    \"\"\"\n",
    "    Split context into overlapping chunks at word boundaries\n",
    "    \"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [(0, text)]\n",
    "    \n",
    "    chunks = []\n",
    "    pos = 0\n",
    "    \n",
    "    while pos < len(text):\n",
    "        # Find end of current chunk (with overlap)\n",
    "        chunk_end = min(pos + chunk_size, len(text))\n",
    "        \n",
    "        # Extend to word boundary\n",
    "        if chunk_end < len(text):\n",
    "            chunk_end = find_word_boundary(text, chunk_end)\n",
    "        \n",
    "        # Extract chunk\n",
    "        chunk = text[pos:chunk_end]\n",
    "        chunks.append((pos, chunk))\n",
    "        \n",
    "        # Move to next position (with overlap)\n",
    "        if chunk_end >= len(text):\n",
    "            break\n",
    "            \n",
    "        # Start next chunk at a word boundary before current chunk end\n",
    "        pos = find_word_boundary(text, chunk_end - overlap, forward=False)\n",
    "        \n",
    "        # Prevent infinite loop\n",
    "        if pos >= chunk_end:\n",
    "            pos = chunk_end\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_context_splits(json_with_ids_path, splits_output_path):\n",
    "    \"\"\"\n",
    "    Step 2: Create context splits JSON with sub-context IDs\n",
    "    \"\"\"\n",
    "    print(f\"Reading JSON with context IDs: {json_with_ids_path}\")\n",
    "    with open(json_with_ids_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    splits_map = {}\n",
    "    \n",
    "    print(\"Creating context splits...\")\n",
    "    for article in tqdm(data[\"data\"]):\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            cid = para[\"context_id\"]\n",
    "            \n",
    "            # Split context into overlapping chunks\n",
    "            chunks = split_context(context)\n",
    "            \n",
    "            # Create sub-contexts with IDs\n",
    "            sub_contexts = []\n",
    "            for i, (start_pos, chunk_text) in enumerate(chunks, 1):\n",
    "                sub_id = f\"{cid}_{i:02d}\"\n",
    "                sub_contexts.append({\n",
    "                    \"id\": sub_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"start_idx\": start_pos,\n",
    "                    \"original_context\": context\n",
    "                })\n",
    "            \n",
    "            splits_map[cid] = sub_contexts\n",
    "    \n",
    "    print(f\"Writing splits to {splits_output_path}\")\n",
    "    with open(splits_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(splits_map, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    total_contexts = len(splits_map)\n",
    "    total_subcontexts = sum(len(splits) for splits in splits_map.values())\n",
    "    avg_splits = total_subcontexts / total_contexts if total_contexts > 0 else 0\n",
    "    \n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Total contexts processed: {total_contexts}\")\n",
    "    print(f\"Total sub-contexts created: {total_subcontexts}\")\n",
    "    print(f\"Average splits per context: {avg_splits:.1f}\")\n",
    "\n",
    "def main():\n",
    "    input_json = \"indicqa_squad_format.json\"\n",
    "    json_with_ids = \"indicqa_squad_with_ids.json\"\n",
    "    contexts_csv = \"contexts.csv\"\n",
    "    splits_json = \"context_splits.json\"\n",
    "    \n",
    "    print(\"\\nStep 1: Assigning Context IDs\")\n",
    "    print(\"=\" * 50)\n",
    "    assign_context_ids(input_json, json_with_ids, contexts_csv)\n",
    "    \n",
    "    print(\"\\nStep 2: Creating Context Splits\")\n",
    "    print(\"=\" * 50)\n",
    "    create_context_splits(json_with_ids, splits_json)\n",
    "    \n",
    "    print(\"\\nAll steps completed successfully!\")\n",
    "    print(f\"1. JSON with context IDs: {json_with_ids}\")\n",
    "    print(f\"2. Contexts CSV: {contexts_csv}\")\n",
    "    print(f\"3. Context splits JSON: {splits_json}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original data from indicqa_squad_with_ids.json\n",
      "Loading sub-contexts from context_splits.json\n",
      "Processing QA pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 9377.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing processed dataset to indicqa_squad_window.json\n",
      "Writing lost questions report to lost_questions.json\n",
      "\n",
      "Processing complete!\n",
      "Total questions: 1734\n",
      "Questions placed: 1728\n",
      "Questions lost: 6\n",
      "Success rate: 99.7%\n",
      "\n",
      "Warning: Some questions could not be placed.\n",
      "Check lost_questions.json for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def find_answer_in_context(text, answer_text):\n",
    "    \"\"\"Find all occurrences of answer_text in text\"\"\"\n",
    "    positions = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        pos = text.find(answer_text, start)\n",
    "        if pos == -1:\n",
    "            break\n",
    "        positions.append(pos)\n",
    "        start = pos + 1\n",
    "    return positions\n",
    "\n",
    "def find_subcontext_for_answer(sub_contexts, current_idx, answer_text):\n",
    "    \"\"\"\n",
    "    Try to find answer in current, next, and previous sub-contexts.\n",
    "    Returns (sub_context_id, new_answer_start) or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    # First check current context\n",
    "    current = sub_contexts[current_idx]\n",
    "    positions = find_answer_in_context(current['text'], answer_text)\n",
    "    if positions:\n",
    "        return current['id'], positions[0]\n",
    "    \n",
    "    # Check next context if available\n",
    "    if current_idx + 1 < len(sub_contexts):\n",
    "        next_ctx = sub_contexts[current_idx + 1]\n",
    "        positions = find_answer_in_context(next_ctx['text'], answer_text)\n",
    "        if positions:\n",
    "            return next_ctx['id'], positions[0]\n",
    "    \n",
    "    # Check previous context if available\n",
    "    if current_idx > 0:\n",
    "        prev_ctx = sub_contexts[current_idx - 1]\n",
    "        positions = find_answer_in_context(prev_ctx['text'], answer_text)\n",
    "        if positions:\n",
    "            return prev_ctx['id'], positions[0]\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def construct_qa_dataset(original_squad_path, subcontexts_json_path, output_path):\n",
    "    \"\"\"\n",
    "    Construct QA dataset checking adjacent contexts for answers.\n",
    "    \"\"\"\n",
    "    print(f\"Loading original data from {original_squad_path}\")\n",
    "    with open(original_squad_path, 'r', encoding='utf-8') as f:\n",
    "        original_data = json.load(f)\n",
    "    \n",
    "    print(f\"Loading sub-contexts from {subcontexts_json_path}\")\n",
    "    with open(subcontexts_json_path, 'r', encoding='utf-8') as f:\n",
    "        contexts_map = json.load(f)\n",
    "    \n",
    "    new_data = {\n",
    "        \"version\": \"v2.0_processed\",\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    total_questions = 0\n",
    "    questions_placed = 0\n",
    "    lost_questions = []\n",
    "    \n",
    "    print(\"Processing QA pairs...\")\n",
    "    for article in tqdm(original_data['data']):\n",
    "        new_article = {\n",
    "            \"title\": article.get('title', ''),\n",
    "            \"paragraphs\": []\n",
    "        }\n",
    "        \n",
    "        for para in article['paragraphs']:\n",
    "            context = para['context']\n",
    "            qas = para['qas']\n",
    "            total_questions += len(qas)\n",
    "            \n",
    "            cid = para.get('context_id')\n",
    "            if not cid or cid not in contexts_map:\n",
    "                continue\n",
    "            \n",
    "            sub_contexts = contexts_map[cid]\n",
    "            subcontext_qas = {}\n",
    "            \n",
    "            # Calculate rough position for each sub-context\n",
    "            sub_context_positions = []\n",
    "            current_pos = 0\n",
    "            for sub_ctx in sub_contexts:\n",
    "                sub_context_positions.append(current_pos)\n",
    "                current_pos += len(sub_ctx['text'])\n",
    "            \n",
    "            for qa in qas:\n",
    "                if qa.get('is_impossible', False):\n",
    "                    # Add unanswerable questions to first sub-context\n",
    "                    sc_id = sub_contexts[0]['id']\n",
    "                    if sc_id not in subcontext_qas:\n",
    "                        subcontext_qas[sc_id] = []\n",
    "                    new_qa = qa.copy()\n",
    "                    new_qa['answers'] = []\n",
    "                    subcontext_qas[sc_id].append(new_qa)\n",
    "                    questions_placed += 1\n",
    "                    continue\n",
    "                \n",
    "                if not qa.get('answers'):\n",
    "                    continue\n",
    "                \n",
    "                answer = qa['answers'][0]\n",
    "                ans_start = answer.get('answer_start')\n",
    "                ans_text = answer.get('text')\n",
    "                \n",
    "                if ans_start is None or ans_text is None:\n",
    "                    continue\n",
    "                \n",
    "                # Find which sub-context is closest to the answer position\n",
    "                closest_idx = 0\n",
    "                closest_dist = float('inf')\n",
    "                for idx, pos in enumerate(sub_context_positions):\n",
    "                    dist = abs(ans_start - pos)\n",
    "                    if dist < closest_dist:\n",
    "                        closest_dist = dist\n",
    "                        closest_idx = idx\n",
    "                \n",
    "                # Try to find answer in current/adjacent sub-contexts\n",
    "                sc_id, new_start = find_subcontext_for_answer(\n",
    "                    sub_contexts, closest_idx, ans_text\n",
    "                )\n",
    "                \n",
    "                if sc_id is not None:\n",
    "                    if sc_id not in subcontext_qas:\n",
    "                        subcontext_qas[sc_id] = []\n",
    "                    \n",
    "                    new_qa = qa.copy()\n",
    "                    new_qa['answers'] = [answer.copy()]\n",
    "                    new_qa['answers'][0]['answer_start'] = new_start\n",
    "                    subcontext_qas[sc_id].append(new_qa)\n",
    "                    questions_placed += 1\n",
    "                else:\n",
    "                    # If still not found, try all sub-contexts\n",
    "                    found = False\n",
    "                    for idx in range(len(sub_contexts)):\n",
    "                        sc_id, new_start = find_subcontext_for_answer(\n",
    "                            sub_contexts, idx, ans_text\n",
    "                        )\n",
    "                        if sc_id is not None:\n",
    "                            if sc_id not in subcontext_qas:\n",
    "                                subcontext_qas[sc_id] = []\n",
    "                            new_qa = qa.copy()\n",
    "                            new_qa['answers'] = [answer.copy()]\n",
    "                            new_qa['answers'][0]['answer_start'] = new_start\n",
    "                            subcontext_qas[sc_id].append(new_qa)\n",
    "                            questions_placed += 1\n",
    "                            found = True\n",
    "                            break\n",
    "                    \n",
    "                    if not found:\n",
    "                        lost_questions.append({\n",
    "                            'id': qa['id'],\n",
    "                            'context_id': cid,\n",
    "                            'answer_start': ans_start,\n",
    "                            'answer_text': ans_text,\n",
    "                            'question': qa['question']\n",
    "                        })\n",
    "            \n",
    "            # Create new paragraphs for each sub-context that has questions\n",
    "            for sub_ctx in sub_contexts:\n",
    "                sc_id = sub_ctx['id']\n",
    "                if sc_id in subcontext_qas:\n",
    "                    new_article['paragraphs'].append({\n",
    "                        \"context\": sub_ctx['text'],\n",
    "                        \"context_id\": sc_id,\n",
    "                        \"qas\": subcontext_qas[sc_id]\n",
    "                    })\n",
    "        \n",
    "        if new_article['paragraphs']:\n",
    "            new_data['data'].append(new_article)\n",
    "    \n",
    "    print(f\"Writing processed dataset to {output_path}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    if lost_questions:\n",
    "        lost_questions_path = 'lost_questions.json'\n",
    "        print(f\"Writing lost questions report to {lost_questions_path}\")\n",
    "        with open(lost_questions_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(lost_questions, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Total questions: {total_questions}\")\n",
    "    print(f\"Questions placed: {questions_placed}\")\n",
    "    print(f\"Questions lost: {len(lost_questions)}\")\n",
    "    print(f\"Success rate: {questions_placed/total_questions*100:.1f}%\")\n",
    "    \n",
    "    if lost_questions:\n",
    "        print(\"\\nWarning: Some questions could not be placed.\")\n",
    "        print(\"Check lost_questions.json for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    original_squad_path = \"indicqa_squad_with_ids.json\"\n",
    "    subcontexts_json_path = \"context_splits.json\"\n",
    "    output_path = \"indicqa_squad_window.json\"\n",
    "    \n",
    "    construct_qa_dataset(original_squad_path, subcontexts_json_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input file: indicqa_squad_window.json\n",
      "Validating answer spans...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 40209.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation complete!\n",
      "Total questions: 1728\n",
      "Valid spans: 1728\n",
      "Invalid spans: 0\n",
      "Validity rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validate_answer_span(context, answer_text, answer_start):\n",
    "    \"\"\"\n",
    "    Validate if the answer text matches at the given position in context.\n",
    "    Returns (is_valid, details) tuple.\n",
    "    \"\"\"\n",
    "    # Basic validation\n",
    "    if answer_start < 0 or answer_start >= len(context):\n",
    "        return False, \"Answer start position out of context bounds\"\n",
    "    \n",
    "    if answer_start + len(answer_text) > len(context):\n",
    "        return False, \"Answer text extends beyond context bounds\"\n",
    "    \n",
    "    # Extract text at the specified position\n",
    "    text_at_position = context[answer_start:answer_start + len(answer_text)]\n",
    "    \n",
    "    # Check if extracted text matches answer text\n",
    "    if text_at_position != answer_text:\n",
    "        # Search for the answer text in the context\n",
    "        real_pos = context.find(answer_text)\n",
    "        if real_pos != -1:\n",
    "            return False, f\"Answer text found at position {real_pos}, not at {answer_start}\"\n",
    "        return False, f\"Answer text not found. At position {answer_start} found '{text_at_position}' instead of '{answer_text}'\"\n",
    "    \n",
    "    return True, \"Valid answer span\"\n",
    "\n",
    "def validate_squad_file(input_path):\n",
    "    \"\"\"\n",
    "    Validate all answer spans in a SQuAD format file.\n",
    "    \"\"\"\n",
    "    print(f\"Reading input file: {input_path}\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Statistics\n",
    "    total_questions = 0\n",
    "    valid_spans = 0\n",
    "    invalid_spans = []\n",
    "    \n",
    "    print(\"Validating answer spans...\")\n",
    "    for article in tqdm(data['data']):\n",
    "        for para in article['paragraphs']:\n",
    "            context = para['context']\n",
    "            for qa in para['qas']:\n",
    "                total_questions += 1\n",
    "                \n",
    "                if qa.get('is_impossible', False):\n",
    "                    valid_spans += 1\n",
    "                    continue\n",
    "                \n",
    "                if not qa.get('answers'):\n",
    "                    continue\n",
    "                \n",
    "                answer = qa['answers'][0]\n",
    "                ans_start = answer.get('answer_start')\n",
    "                ans_text = answer.get('text')\n",
    "                \n",
    "                if ans_start is None or ans_text is None:\n",
    "                    invalid_spans.append({\n",
    "                        'id': qa['id'],\n",
    "                        'context_id': para.get('context_id', 'unknown'),\n",
    "                        'error': 'Missing answer start or text',\n",
    "                        'question': qa['question']\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                is_valid, details = validate_answer_span(context, ans_text, ans_start)\n",
    "                \n",
    "                if is_valid:\n",
    "                    valid_spans += 1\n",
    "                else:\n",
    "                    invalid_spans.append({\n",
    "                        'id': qa['id'],\n",
    "                        'context_id': para.get('context_id', 'unknown'),\n",
    "                        'question': qa['question'],\n",
    "                        'answer_text': ans_text,\n",
    "                        'answer_start': ans_start,\n",
    "                        'error': details,\n",
    "                        'context_snippet': context[max(0, ans_start-50):min(len(context), ans_start + len(ans_text) + 50)]\n",
    "                    })\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nValidation complete!\")\n",
    "    print(f\"Total questions: {total_questions}\")\n",
    "    print(f\"Valid spans: {valid_spans}\")\n",
    "    print(f\"Invalid spans: {len(invalid_spans)}\")\n",
    "    print(f\"Validity rate: {valid_spans/total_questions*100:.1f}%\")\n",
    "    \n",
    "    if invalid_spans:\n",
    "        output_path = 'invalid_spans.json'\n",
    "        print(f\"\\nWriting invalid spans to {output_path}\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(invalid_spans, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Print some examples of invalid spans\n",
    "        print(\"\\nExample invalid spans:\")\n",
    "        for span in invalid_spans[:3]:  # Show first 3 examples\n",
    "            print(f\"\\nQuestion ID: {span['id']}\")\n",
    "            print(f\"Context ID: {span['context_id']}\")\n",
    "            print(f\"Question: {span['question']}\")\n",
    "            print(f\"Answer text: {span['answer_text']}\")\n",
    "            print(f\"Error: {span['error']}\")\n",
    "            print(f\"Context snippet: ...{span['context_snippet']}...\")\n",
    "\n",
    "def check_specific_qa(context, qa):\n",
    "    \"\"\"\n",
    "    Check a specific QA pair and print detailed analysis.\n",
    "    \"\"\"\n",
    "    if qa.get('is_impossible', False):\n",
    "        print(\"This is marked as an impossible question\")\n",
    "        return\n",
    "    \n",
    "    answer = qa['answers'][0]\n",
    "    ans_start = answer['answer_start']\n",
    "    ans_text = answer['text']\n",
    "    \n",
    "    print(f\"\\nAnalyzing answer span for question ID: {qa['id']}\")\n",
    "    print(f\"Question: {qa['question']}\")\n",
    "    print(f\"Expected answer: '{ans_text}' at position {ans_start}\")\n",
    "    \n",
    "    # Validate the span\n",
    "    is_valid, details = validate_answer_span(context, ans_text, ans_start)\n",
    "    print(f\"\\nValidation result: {'✓ Valid' if is_valid else '✗ Invalid'}\")\n",
    "    print(f\"Details: {details}\")\n",
    "    \n",
    "    # Show context snippet\n",
    "    start_idx = max(0, ans_start - 50)\n",
    "    end_idx = min(len(context), ans_start + len(ans_text) + 50)\n",
    "    print(\"\\nContext snippet:\")\n",
    "    print(f\"...{context[start_idx:end_idx]}...\")\n",
    "    \n",
    "    # If invalid, try to find the correct position\n",
    "    if not is_valid:\n",
    "        positions = []\n",
    "        start = 0\n",
    "        while True:\n",
    "            pos = context.find(ans_text, start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "            positions.append(pos)\n",
    "            start = pos + 1\n",
    "        \n",
    "        if positions:\n",
    "            print(\"\\nFound answer text at positions:\", positions)\n",
    "            for pos in positions:\n",
    "                print(f\"\\nSnippet for position {pos}:\")\n",
    "                print(f\"...{context[max(0, pos-20):min(len(context), pos+len(ans_text)+20)]}...\")\n",
    "        else:\n",
    "            print(\"\\nAnswer text not found in context\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For validating entire file\n",
    "    validate_squad_file(\"indicqa_squad_window.json\")\n",
    "    \n",
    "    # For checking specific QA pair (from your example)\n",
    "    # example_context = \"చేసుకున్నాడ్య్. ఉన్నత పాఠశాల విద్య ముగించాక, నారాయణ్ విశ్వవిద్యాలయ ప్రవేశ పరీక్ష తప్పి, ఇంటిలోనే చదువుకుంటూ, రాసుకుంటూ ఒక సంవత్సరం గడిపి, తర్వాత 1926 సంవత్సరము పరీక్షలో ఉత్తీర్ణుడై మైసూరు మహారాజ కళాశాలలో చేరాడు. బేచలర్ పట్టా పొందడానికి నారాయణ్ నాలుగు సంవత్సరాలు తీసుకున్నాడు. ఇది మామూలు కంటే ఒక ఏడాది ఎక్కువ. మాస్టర్ డిగ్రీ (M. A. ) చదవడం వల్ల సాహిత్యంలో ఉన్న ఆసక్తి తగ్గిపోతుందని ఒక మిత్రుడు చెప్పడంతో, కొంత కాలం ఇతడు ఒక పాఠశాల ఉపాధ్యాయునిగా ఉద్యోగం చేసాడు. అయితే, ప్రధానోపాధ్యాయుడు ఇతడిని వ్యాయామ ఉపాధ్యాయుని\"\n",
    "    # example_qa = {\n",
    "    #     \"id\": 0,\n",
    "    #     \"question\": \"మాస్టర్ డిగ్రీ (M. A. ) చదవడం వల్ల సాహిత్యంలో ఉన్న ఆసక్తి తగ్గిపోతుందని మిత్రుడు చెప్పడంతో, కొంత కాలం నారాయణ్ ఏ ఉద్యోగం చేసాడు?\",\n",
    "    #     \"answers\": [\n",
    "    #         {\n",
    "    #             \"text\": \"ఉపాధ్యాయునిగా\",\n",
    "    #             \"answer_start\": 429\n",
    "    #         }\n",
    "    #     ],\n",
    "    #     \"is_impossible\": False\n",
    "    # }\n",
    "    \n",
    "    # check_specific_qa(example_context, example_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using tokenizer: xlm-roberta-large\n",
      "[INFO] Processing QA data from indicqa_squad_window.json...\n",
      "Loading data from indicqa_squad_window.json\n",
      "Filtering and verifying QA pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 41588.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total QAs: 1728\n",
      "Valid QAs: 1392\n",
      "Filtered out: 336\n",
      "[INFO] Building tokenized examples...\n",
      "Building tokenized examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:06<00:00, 37.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total examples built: 1392\n",
      "[INFO] Total examples: 1392\n",
      "[INFO] Saving to processed_indicqa/indicqa_windowed.pt\n",
      "[DONE] Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------\n",
    "# Adjust paths here\n",
    "# ------------------\n",
    "input_json = \"indicqa_squad_window.json\"\n",
    "out_dir = \"processed_indicqa\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "output_pt = os.path.join(out_dir, \"indicqa_windowed.pt\")\n",
    "\n",
    "# ------------------\n",
    "# Hyperparameters\n",
    "# ------------------\n",
    "max_length = 512\n",
    "model_tokenizer_name = \"xlm-roberta-large\"\n",
    "\n",
    "def filter_and_verify_squad(input_path):\n",
    "    \"\"\"\n",
    "    Returns a new SQuAD JSON dict containing only QAs where:\n",
    "    - is_impossible=False\n",
    "    - at least one answer exists\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {input_path}\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    new_data = {\n",
    "        \"version\": data.get(\"version\", \"windowed_indicqa\"),\n",
    "        \"data\": []\n",
    "    }\n",
    "    \n",
    "    total_qas = 0\n",
    "    valid_qas = 0\n",
    "    \n",
    "    print(\"Filtering and verifying QA pairs...\")\n",
    "    for article in tqdm(data[\"data\"]):\n",
    "        new_paragraphs = []\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            new_qas = []\n",
    "            \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                total_qas += 1\n",
    "                # Skip impossible questions\n",
    "                if qa.get(\"is_impossible\", True):\n",
    "                    continue\n",
    "                    \n",
    "                # Must have answers\n",
    "                if not qa.get(\"answers\"):\n",
    "                    continue\n",
    "                \n",
    "                answer = qa[\"answers\"][0]\n",
    "                ans_text = answer.get(\"text\")\n",
    "                \n",
    "                # Only verify answer text exists somewhere in context\n",
    "                if ans_text and ans_text in context:\n",
    "                    new_qas.append(qa)\n",
    "                    valid_qas += 1\n",
    "            \n",
    "            if new_qas:\n",
    "                new_paragraphs.append({\n",
    "                    \"context\": context,\n",
    "                    \"context_id\": paragraph.get(\"context_id\", \"\"),\n",
    "                    \"qas\": new_qas\n",
    "                })\n",
    "        \n",
    "        if new_paragraphs:\n",
    "            new_data[\"data\"].append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"paragraphs\": new_paragraphs\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nTotal QAs: {total_qas}\")\n",
    "    print(f\"Valid QAs: {valid_qas}\")\n",
    "    print(f\"Filtered out: {total_qas - valid_qas}\")\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def build_examples(squad_data, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Build tokenized examples for each QA pair.\n",
    "    More lenient handling of answer spans for evaluation purposes.\n",
    "    \"\"\"\n",
    "    examples_out = []\n",
    "    print(\"Building tokenized examples...\")\n",
    "    \n",
    "    for article in tqdm(squad_data[\"data\"]):\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            context_id = paragraph.get(\"context_id\", \"\")\n",
    "            \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                ans = qa[\"answers\"][0]\n",
    "                ans_text = ans[\"text\"]\n",
    "                ans_start = ans.get(\"answer_start\", context.find(ans_text))  # Fallback to first occurrence\n",
    "                \n",
    "                # If answer_start is invalid, find the first occurrence\n",
    "                if ans_start < 0 or ans_start >= len(context) or context[ans_start:ans_start + len(ans_text)] != ans_text:\n",
    "                    ans_start = context.find(ans_text)\n",
    "                \n",
    "                # If still can't find the answer, skip this example\n",
    "                if ans_start == -1:\n",
    "                    continue\n",
    "                    \n",
    "                ans_end = ans_start + len(ans_text)\n",
    "                \n",
    "                # Tokenize\n",
    "                enc = tokenizer(\n",
    "                    qa[\"question\"],\n",
    "                    context,\n",
    "                    max_length=max_length,\n",
    "                    truncation=\"only_second\",\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "                \n",
    "                input_ids = enc[\"input_ids\"][0]\n",
    "                attention_mask = enc[\"attention_mask\"][0]\n",
    "                offset_mapping = enc[\"offset_mapping\"][0].tolist()\n",
    "                \n",
    "                # Find token indices for answer span\n",
    "                start_token, end_token = None, None\n",
    "                for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "                    if start_char <= ans_start < end_char:\n",
    "                        start_token = i\n",
    "                    if start_char < ans_end <= end_char:\n",
    "                        end_token = i\n",
    "                \n",
    "                # If can't find exact token spans, use approximation\n",
    "                if start_token is None or end_token is None or end_token < start_token:\n",
    "                    # Find the best approximate token positions\n",
    "                    for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "                        if end_char > 0:  # Skip special tokens\n",
    "                            start_token = i\n",
    "                            break\n",
    "                    \n",
    "                    for i, (start_char, end_char) in enumerate(offset_mapping[start_token:], start_token):\n",
    "                        if i >= len(offset_mapping) - 1 or offset_mapping[i+1][0] > ans_start + len(ans_text):\n",
    "                            end_token = i\n",
    "                            break\n",
    "                    \n",
    "                    if end_token is None:\n",
    "                        end_token = start_token\n",
    "                \n",
    "                # Create example\n",
    "                ex_item = {\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"context_id\": context_id,\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"start_positions\": torch.tensor(start_token, dtype=torch.long),\n",
    "                    \"end_positions\": torch.tensor(end_token, dtype=torch.long),\n",
    "                    \"offset_mapping\": offset_mapping,\n",
    "                    \"context\": context,\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"gold_text\": ans_text,\n",
    "                    \"answer_start\": ans_start  # Keep original position for evaluation\n",
    "                }\n",
    "                examples_out.append(ex_item)\n",
    "    \n",
    "    print(f\"\\nTotal examples built: {len(examples_out)}\")\n",
    "    return examples_out\n",
    "\n",
    "def main():\n",
    "    print(f\"[INFO] Using tokenizer: {model_tokenizer_name}\")\n",
    "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_tokenizer_name)\n",
    "\n",
    "    print(f\"[INFO] Processing QA data from {input_json}...\")\n",
    "    filtered_data = filter_and_verify_squad(input_json)\n",
    "\n",
    "    print(\"[INFO] Building tokenized examples...\")\n",
    "    examples = build_examples(filtered_data, tokenizer, max_length)\n",
    "    print(f\"[INFO] Total examples: {len(examples)}\")\n",
    "\n",
    "    print(f\"[INFO] Saving to {output_pt}\")\n",
    "    torch.save(examples, output_pt)\n",
    "\n",
    "    print(\"[DONE] Preprocessing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "336 Questions are filtered out due to the impossibility factor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loading processed dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_263263/1680665109.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_list = torch.load(DATA_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from ../TeQAS 1.1/final_xlmr_tel_answerable_3_v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Running inference on all examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_263263/1680665109.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids      = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
      "/tmp/ipykernel_263263/1680665109.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Computing metrics...\n",
      "\n",
      "===== IndicQATe Final Evaluation Metrics =====\n",
      "exact_match: 53.23\n",
      "f1: 70.82\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluate_tydiqa_telugu.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    XLMRobertaForQuestionAnswering,\n",
    "    XLMRobertaTokenizerFast\n",
    ")\n",
    "\n",
    "# ------------------\n",
    "# Adjust paths here\n",
    "# ------------------\n",
    "\n",
    "DATA_PATH = \"processed_indicqa/indicqa_windowed.pt\"\n",
    "\n",
    "MODEL_PATH = \"../TeQAS 1.1/final_xlmr_tel_answerable_3_v2\"  # Path to your fine-tuned QA model\n",
    "\n",
    "print(\"\\n[INFO] Loading processed dataset...\")\n",
    "data_list = torch.load(DATA_PATH)\n",
    "dataset = Dataset.from_list(data_list)\n",
    "\n",
    "print(f\"[INFO] Loading model from {MODEL_PATH}...\")\n",
    "model = XLMRobertaForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"\n",
    "    Convert model logits into text predictions by:\n",
    "      - Finding best start/end\n",
    "      - Using offset_mapping to slice the original context\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    num_examples = len(examples)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        ex = examples[i]\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        # If mismatch in array sizes, skip\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx   = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        # Check valid indices\n",
    "        if (\n",
    "            start_idx >= len(offsets) or\n",
    "            end_idx   >= len(offsets) or\n",
    "            start_idx > end_idx\n",
    "        ):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char   = offsets[end_idx][1]\n",
    "        pred_text  = context[start_char:end_char]\n",
    "\n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, examples):\n",
    "    \"\"\"\n",
    "    Compute EM and F1 on the predictions vs. gold_text.\n",
    "    \"\"\"\n",
    "    start_logits, end_logits = eval_preds\n",
    "\n",
    "    # Convert any torch.Tensors to numpy\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    predictions = postprocess_qa_predictions(examples, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1 = 0.0, 0.0\n",
    "    for ex_idx, ex in enumerate(examples):\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred  = predictions.get(ex_id, \"\")\n",
    "        gold  = ex[\"gold_text\"]\n",
    "\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "\n",
    "    count = len(examples)\n",
    "    return {\n",
    "        \"exact_match\": 100.0 * total_em / count,\n",
    "        \"f1\":          100.0 * total_f1 / count\n",
    "    }\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "\n",
    "    common   = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"\n",
    "    Lower text and remove punctuation, articles, extra whitespace.\n",
    "    \"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "    def remove_punc(text):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    s = s.lower()\n",
    "    s = remove_articles(s)\n",
    "    s = remove_punc(s)\n",
    "    s = white_space_fix(s)\n",
    "    return s\n",
    "\n",
    "print(\"\\n[INFO] Running inference on all examples...\")\n",
    "start_logits_list = []\n",
    "end_logits_list   = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in data_list:\n",
    "        input_ids      = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits_list.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits_list.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "# Concatenate to get final arrays\n",
    "start_logits = np.concatenate(start_logits_list, axis=0)\n",
    "end_logits   = np.concatenate(end_logits_list,   axis=0)\n",
    "\n",
    "print(\"[INFO] Computing metrics...\")\n",
    "metrics = compute_metrics((start_logits, end_logits), data_list)\n",
    "\n",
    "print(\"\\n===== IndicQATe Final Evaluation Metrics =====\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using tokenizer: google/muril-large-cased\n",
      "[INFO] Processing QA data from indicqa_squad_window.json...\n",
      "Loading data from indicqa_squad_window.json\n",
      "Processing QA pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 36191.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total QAs: 1728\n",
      "Valid QAs: 1392\n",
      "Filtered out: 336\n",
      "[INFO] Building tokenized examples...\n",
      "Building tokenized examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:04<00:00, 53.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total examples built: 1392\n",
      "[INFO] Total examples: 1392\n",
      "[INFO] Saving to processed_indicqa_muril/indicqa_windowed_muril.pt\n",
      "[DONE] Preprocessing completed for MuRIL.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# -------------------------\n",
    "#  Adjust paths and params\n",
    "# -------------------------\n",
    "input_json = \"indicqa_squad_window.json\"  # Your windowed data\n",
    "out_dir = \"processed_indicqa_muril\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "output_pt = os.path.join(out_dir, \"indicqa_windowed_muril.pt\")\n",
    "\n",
    "max_length = 512\n",
    "model_tokenizer_name = \"google/muril-large-cased\"  # or \"google/muril-base-cased\"\n",
    "\n",
    "def filter_and_verify_squad(input_path):\n",
    "    \"\"\"\n",
    "    Loads and verifies answerable QAs from windowed data.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {input_path}\")\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    new_data = {\n",
    "        \"version\": data.get(\"version\", \"windowed_indicqa\"),\n",
    "        \"data\": []\n",
    "    }\n",
    "\n",
    "    total_qas = 0\n",
    "    valid_qas = 0\n",
    "    \n",
    "    print(\"Processing QA pairs...\")\n",
    "    for article in tqdm(data[\"data\"]):\n",
    "        new_paragraphs = []\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            new_qas = []\n",
    "            \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                total_qas += 1\n",
    "                \n",
    "                # Skip impossible questions\n",
    "                if qa.get(\"is_impossible\", True):\n",
    "                    continue\n",
    "                \n",
    "                # Must have answers\n",
    "                if not qa.get(\"answers\"):\n",
    "                    continue\n",
    "                \n",
    "                answer = qa[\"answers\"][0]\n",
    "                ans_text = answer.get(\"text\")\n",
    "                \n",
    "                # Only check if answer exists in context\n",
    "                if ans_text and ans_text in context:\n",
    "                    new_qas.append(qa)\n",
    "                    valid_qas += 1\n",
    "            \n",
    "            if new_qas:\n",
    "                new_paragraphs.append({\n",
    "                    \"context\": context,\n",
    "                    \"context_id\": paragraph.get(\"context_id\", \"\"),\n",
    "                    \"qas\": new_qas\n",
    "                })\n",
    "        \n",
    "        if new_paragraphs:\n",
    "            new_data[\"data\"].append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"paragraphs\": new_paragraphs\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nTotal QAs: {total_qas}\")\n",
    "    print(f\"Valid QAs: {valid_qas}\")\n",
    "    print(f\"Filtered out: {total_qas - valid_qas}\")\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def build_examples(squad_data, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Build tokenized examples using MuRIL tokenizer.\n",
    "    \"\"\"\n",
    "    examples_out = []\n",
    "    print(\"Building tokenized examples...\")\n",
    "    \n",
    "    for article in tqdm(squad_data[\"data\"]):\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            context_id = paragraph.get(\"context_id\", \"\")\n",
    "            \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                ans = qa[\"answers\"][0]\n",
    "                ans_text = ans[\"text\"]\n",
    "                ans_start = ans.get(\"answer_start\", context.find(ans_text))\n",
    "                \n",
    "                # If answer_start is invalid, find first occurrence\n",
    "                if ans_start < 0 or ans_start >= len(context) or context[ans_start:ans_start + len(ans_text)] != ans_text:\n",
    "                    ans_start = context.find(ans_text)\n",
    "                \n",
    "                # Skip if answer can't be found\n",
    "                if ans_start == -1:\n",
    "                    continue\n",
    "                \n",
    "                ans_end = ans_start + len(ans_text)\n",
    "                \n",
    "                # Tokenize\n",
    "                encoding = tokenizer(\n",
    "                    qa[\"question\"],\n",
    "                    context,\n",
    "                    max_length=max_length,\n",
    "                    truncation=\"only_second\",\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "                \n",
    "                input_ids = encoding[\"input_ids\"][0]\n",
    "                attention_mask = encoding[\"attention_mask\"][0]\n",
    "                offset_mapping = encoding[\"offset_mapping\"][0].tolist()\n",
    "                \n",
    "                # Find token indices for answer span\n",
    "                start_token, end_token = None, None\n",
    "                for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "                    if start_char <= ans_start < end_char:\n",
    "                        start_token = i\n",
    "                    if start_char < ans_end <= end_char:\n",
    "                        end_token = i\n",
    "                \n",
    "                # If exact mapping fails, use approximation\n",
    "                if start_token is None or end_token is None or end_token < start_token:\n",
    "                    # Find best approximate positions\n",
    "                    for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "                        if end_char > 0:  # Skip special tokens\n",
    "                            start_token = i\n",
    "                            break\n",
    "                    \n",
    "                    for i, (start_char, end_char) in enumerate(offset_mapping[start_token:], start_token):\n",
    "                        if i >= len(offset_mapping) - 1 or offset_mapping[i+1][0] > ans_start + len(ans_text):\n",
    "                            end_token = i\n",
    "                            break\n",
    "                    \n",
    "                    if end_token is None:\n",
    "                        end_token = start_token\n",
    "                \n",
    "                # Create example\n",
    "                ex_item = {\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"context_id\": context_id,\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"start_positions\": torch.tensor(start_token, dtype=torch.long),\n",
    "                    \"end_positions\": torch.tensor(end_token, dtype=torch.long),\n",
    "                    \"offset_mapping\": offset_mapping,\n",
    "                    \"context\": context,\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"gold_text\": ans_text,\n",
    "                    \"answer_start\": ans_start\n",
    "                }\n",
    "                examples_out.append(ex_item)\n",
    "    \n",
    "    print(f\"\\nTotal examples built: {len(examples_out)}\")\n",
    "    return examples_out\n",
    "\n",
    "def main():\n",
    "    print(f\"[INFO] Using tokenizer: {model_tokenizer_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_tokenizer_name)\n",
    "\n",
    "    print(f\"[INFO] Processing QA data from {input_json}...\")\n",
    "    filtered_data = filter_and_verify_squad(input_json)\n",
    "\n",
    "    print(\"[INFO] Building tokenized examples...\")\n",
    "    examples = build_examples(filtered_data, tokenizer, max_length)\n",
    "    print(f\"[INFO] Total examples: {len(examples)}\")\n",
    "\n",
    "    print(f\"[INFO] Saving to {output_pt}\")\n",
    "    torch.save(examples, output_pt)\n",
    "    print(\"[DONE] Preprocessing completed for MuRIL.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading processed dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_263263/3972129100.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  examples_list = torch.load(DATA_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 1392 total examples for evaluation.\n",
      "[INFO] Loading fine-tuned model from ../TeQAS 1.2/final_muril_tel_answerable_v2...\n",
      "[INFO] Running inference on each example...\n",
      "[INFO] Computing final metrics...\n",
      "\n",
      "===== IndicQA (Telugu) - MuRIL Evaluation =====\n",
      "Exact Match (EM): 53.66\n",
      "F1 Score:         71.00\n",
      "================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluate_tydiqa_telugu_muril.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "############################################\n",
    "# 1) Adjust paths and parameters\n",
    "############################################\n",
    "DATA_DIR   = \"processed_indicqa_muril\"\n",
    "DATA_PATH  = os.path.join(DATA_DIR, \"indicqa_windowed_muril.pt\")\n",
    "MODEL_PATH = \"../TeQAS 1.2/final_muril_tel_answerable_v2\"  # your fine-tuned MuRIL QA model folder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use the same MuRIL model name as in preprocessing\n",
    "MURIL_TOKENIZER = \"google/muril-large-cased\"\n",
    "\n",
    "############################################\n",
    "# 2) Load Data & Model\n",
    "############################################\n",
    "print(\"[INFO] Loading processed dataset...\")\n",
    "examples_list = torch.load(DATA_PATH)\n",
    "dataset = Dataset.from_list(examples_list)\n",
    "print(f\"[INFO] Loaded {len(dataset)} total examples for evaluation.\")\n",
    "\n",
    "print(f\"[INFO] Loading fine-tuned model from {MODEL_PATH}...\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MURIL_TOKENIZER)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "############################################\n",
    "# 3) Define Postprocessing & Metrics\n",
    "############################################\n",
    "def normalize_text(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "    common      = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same    = len(common)\n",
    "\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"Convert model logits into final text predictions.\"\"\"\n",
    "    predictions = {}\n",
    "    for i, ex in enumerate(examples):\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            predictions[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        # Identify best start/end\n",
    "        best_start = int(np.argmax(start_logits[i]))\n",
    "        best_end   = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        # Validate token indices\n",
    "        if best_start >= len(offsets) or best_end >= len(offsets) or best_start > best_end:\n",
    "            predictions[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Convert token offsets to char positions, then slice context\n",
    "        start_char = offsets[best_start][0]\n",
    "        end_char   = offsets[best_end][1]\n",
    "        pred_text  = context[start_char:end_char]\n",
    "\n",
    "        predictions[ex[\"id\"]] = pred_text\n",
    "    return predictions\n",
    "\n",
    "def compute_metrics(logits_tuple, examples):\n",
    "    \"\"\"\n",
    "    logits_tuple => (start_logits, end_logits)\n",
    "    examples => the raw examples with gold_text\n",
    "    \"\"\"\n",
    "    start_logits, end_logits = logits_tuple\n",
    "\n",
    "    # Convert to numpy if still Tensors\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    # Postprocess predictions\n",
    "    preds = postprocess_qa_predictions(examples, start_logits, end_logits)\n",
    "\n",
    "    # Compute EM / F1\n",
    "    total_em, total_f1, count = 0.0, 0.0, 0\n",
    "    for ex in examples:\n",
    "        gold = ex[\"gold_text\"]\n",
    "        pred = preds.get(ex[\"id\"], \"\")\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "        count += 1\n",
    "\n",
    "    em = 100.0 * total_em / count\n",
    "    f1 = 100.0 * total_f1 / count\n",
    "    return {\"exact_match\": em, \"f1\": f1}\n",
    "\n",
    "############################################\n",
    "# 4) Inference / Evaluation\n",
    "############################################\n",
    "print(\"[INFO] Running inference on each example...\")\n",
    "start_logits_list, end_logits_list = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ex in examples_list:\n",
    "        input_ids      = ex[\"input_ids\"].unsqueeze(0).to(device)\n",
    "        attention_mask = ex[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits_list.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits_list.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "start_logits_all = np.concatenate(start_logits_list, axis=0)\n",
    "end_logits_all   = np.concatenate(end_logits_list,   axis=0)\n",
    "\n",
    "print(\"[INFO] Computing final metrics...\")\n",
    "metrics = compute_metrics((start_logits_all, end_logits_all), examples_list)\n",
    "\n",
    "############################################\n",
    "# 5) Print results\n",
    "############################################\n",
    "print(\"\\n===== IndicQA (Telugu) - MuRIL Evaluation =====\")\n",
    "print(f\"Exact Match (EM): {metrics['exact_match']:.2f}\")\n",
    "print(f\"F1 Score:         {metrics['f1']:.2f}\")\n",
    "print(\"================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
