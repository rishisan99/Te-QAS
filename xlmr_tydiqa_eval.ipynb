{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'context', 'question', 'answers']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the TyDi QA dataset (validation split)\n",
    "tydi_dataset = load_dataset(\"tydiqa\", \"secondary_task\", split=\"validation\")\n",
    "\n",
    "# Print dataset keys\n",
    "print(tydi_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5077/5077 [00:00<00:00, 14699.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'telugu--2245295572008910947-0', 'title': 'మహాసముద్రం', 'context': \"మహా సముద్రం లేదా మహాసాగరం (Ocean), భూగోళం యొక్క జలావరణంలో ప్రధాన భాగం. ఉప్పు నీటితో నిండిన ఈ మహా సముద్రాలు భూమి ఉపరితలము పై 71% పైగా విస్తరించి ఉన్నాయి. వీటి మొత్తం వైశాల్యం 36.1 కోట్ల చదరపు కిలో మీటర్లు. ప్రపంచం సముద్ర జలాలలో దాదాపు సగ భాగము 3,000 మీటర్లు (9,800 అడుగులు) ఫైగా లోతైనవి. సరాసరి మహాసముద్రాల 'సెలైనిటీ' (ఉప్పదనం) దాదాపు మిలియనుకు 35 వంతులు (3.5%). దాదాపు అన్ని సముద్ర జలాల సెలైనిటీ మిలియనుకు 31 నుండి 38 వంతులు ఉంటుంది. మహాసాగరాలన్నీ కలిసి ఉన్నా గాని వ్యావహారికంగా ఐదు వేరు వేరు మహాసాగరాలుగా గుర్తిస్తారు. అవి పసిఫిక్ మహాసముద్రం, అట్లాంటిక్ మహాసముద్రం, హిందూ మహాసముద్రం, ఆర్కిటిక్ మహాసముద్రం మరియు దక్షిణ మహాసముద్రం.\", 'question': 'మహా సముద్రాలు ఎన్ని ఉన్నాయి?', 'answers': {'text': ['ఐదు'], 'answer_start': [479]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load TyDi QA validation dataset\n",
    "tydi_dataset = load_dataset(\"tydiqa\", \"secondary_task\", split=\"validation\")\n",
    "\n",
    "# Function to detect Telugu text using Unicode range (U+0C00–U+0C7F)\n",
    "def is_telugu(text):\n",
    "    return bool(re.search(r\"[\\u0C00-\\u0C7F]\", text))\n",
    "\n",
    "# Filter for Telugu language samples based on \"context\"\n",
    "telugu_data = tydi_dataset.filter(lambda x: is_telugu(x[\"context\"]))\n",
    "\n",
    "# Print the first sample from the Telugu subset\n",
    "print(telugu_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered Telugu TyDi QA dataset saved as 'tydiqa_telugu_dev.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Convert dataset to a list of dicts\n",
    "telugu_samples = telugu_data.to_list()\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"tydiqa_telugu_dev.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(telugu_samples, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ Filtered Telugu TyDi QA dataset saved as 'tydiqa_telugu_dev.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Answerable Question:\n",
      "{'id': 'telugu--2245295572008910947-0', 'title': 'మహాసముద్రం', 'context': \"మహా సముద్రం లేదా మహాసాగరం (Ocean), భూగోళం యొక్క జలావరణంలో ప్రధాన భాగం. ఉప్పు నీటితో నిండిన ఈ మహా సముద్రాలు భూమి ఉపరితలము పై 71% పైగా విస్తరించి ఉన్నాయి. వీటి మొత్తం వైశాల్యం 36.1 కోట్ల చదరపు కిలో మీటర్లు. ప్రపంచం సముద్ర జలాలలో దాదాపు సగ భాగము 3,000 మీటర్లు (9,800 అడుగులు) ఫైగా లోతైనవి. సరాసరి మహాసముద్రాల 'సెలైనిటీ' (ఉప్పదనం) దాదాపు మిలియనుకు 35 వంతులు (3.5%). దాదాపు అన్ని సముద్ర జలాల సెలైనిటీ మిలియనుకు 31 నుండి 38 వంతులు ఉంటుంది. మహాసాగరాలన్నీ కలిసి ఉన్నా గాని వ్యావహారికంగా ఐదు వేరు వేరు మహాసాగరాలుగా గుర్తిస్తారు. అవి పసిఫిక్ మహాసముద్రం, అట్లాంటిక్ మహాసముద్రం, హిందూ మహాసముద్రం, ఆర్కిటిక్ మహాసముద్రం మరియు దక్షిణ మహాసముద్రం.\", 'question': 'మహా సముద్రాలు ఎన్ని ఉన్నాయి?', 'answers': {'text': ['ఐదు'], 'answer_start': [479]}}\n",
      "\n",
      "No impossible questions found.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load TyDi QA validation dataset\n",
    "tydi_dataset = load_dataset(\"tydiqa\", \"secondary_task\", split=\"validation\")\n",
    "\n",
    "# Function to detect Telugu text using Unicode range (U+0C00–U+0C7F)\n",
    "def is_telugu(text):\n",
    "    return bool(re.search(r\"[\\u0C00-\\u0C7F]\", text))\n",
    "\n",
    "# Filter for Telugu language samples based on \"context\"\n",
    "telugu_data = tydi_dataset.filter(lambda x: is_telugu(x[\"context\"]))\n",
    "\n",
    "# Function to check if a question has no answers (impossible question)\n",
    "def is_impossible_example(example):\n",
    "    return len(example['answers']['text']) == 0  # If there are no answers, it's impossible\n",
    "\n",
    "# Function to check if a question has answers (answerable question)\n",
    "def is_answerable_example(example):\n",
    "    return len(example['answers']['text']) > 0  # If there are answers, it's answerable\n",
    "\n",
    "# Filter impossible questions\n",
    "impossible_questions = telugu_data.filter(is_impossible_example)\n",
    "\n",
    "# Filter answerable questions\n",
    "answerable_questions = telugu_data.filter(is_answerable_example)\n",
    "\n",
    "# Print a sample of answerable questions\n",
    "print(\"Sample Answerable Question:\")\n",
    "print(answerable_questions[0])\n",
    "\n",
    "# Check if there are any impossible questions and print a sample\n",
    "if len(impossible_questions) > 0:\n",
    "    print(\"\\nSample Impossible Question:\")\n",
    "    print(impossible_questions[0])\n",
    "else:\n",
    "    print(\"\\nNo impossible questions found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TyDi QA Telugu converted to SQuAD format and saved as 'tydiqa_telugu_squad_format.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the TyDi QA Telugu dataset\n",
    "with open(\"tydiqa_telugu_dev.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tydi_telugu_data = json.load(f)\n",
    "\n",
    "# Convert to SQuAD format\n",
    "squad_format = {\"version\": \"TyDi-QA-Telugu\", \"data\": []}\n",
    "\n",
    "article_dict = {}  # To group by title\n",
    "\n",
    "for entry in tydi_telugu_data:\n",
    "    title = entry[\"title\"]\n",
    "    context = entry[\"context\"]\n",
    "    question = entry[\"question\"]\n",
    "    answers = entry[\"answers\"][\"text\"]\n",
    "    answer_starts = entry[\"answers\"][\"answer_start\"]\n",
    "\n",
    "    # If this title doesn't exist, create a new article\n",
    "    if title not in article_dict:\n",
    "        article_dict[title] = {\n",
    "            \"title\": title,\n",
    "            \"paragraphs\": []\n",
    "        }\n",
    "\n",
    "    # Prepare the Q&A format\n",
    "    qas_entry = {\n",
    "        \"question\": question,\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"is_impossible\": False,  # Since all have answers\n",
    "        \"answers\": [\n",
    "            {\"text\": ans, \"answer_start\": start}\n",
    "            for ans, start in zip(answers, answer_starts)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Check if this context already exists\n",
    "    existing_paragraph = next((p for p in article_dict[title][\"paragraphs\"] if p[\"context\"] == context), None)\n",
    "\n",
    "    if existing_paragraph:\n",
    "        existing_paragraph[\"qas\"].append(qas_entry)\n",
    "    else:\n",
    "        # Create a new paragraph entry\n",
    "        article_dict[title][\"paragraphs\"].append({\n",
    "            \"context\": context,\n",
    "            \"qas\": [qas_entry]\n",
    "        })\n",
    "\n",
    "# Append to squad format\n",
    "squad_format[\"data\"] = list(article_dict.values())\n",
    "\n",
    "# Save converted dataset\n",
    "with open(\"tydiqa_telugu_squad_format.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(squad_format, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"✅ TyDi QA Telugu converted to SQuAD format and saved as 'tydiqa_telugu_squad_format.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using tokenizer: xlm-roberta-large\n",
      "[INFO] Filtering answerable QAs in tydiqa_telugu_squad_format.json...\n",
      "[INFO] Building tokenized examples...\n",
      "[INFO] Total answerable examples: 669\n",
      "[INFO] Saving to processed_tydiqa_telugu/tydiqa_telugu.pt\n",
      "[DONE] Preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# preprocess_tydiqa_telugu.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "# ------------------\n",
    "# Adjust paths here\n",
    "# ------------------\n",
    "input_json  = \"tydiqa_telugu_squad_format.json\" \n",
    "out_dir     = \"processed_tydiqa_telugu\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "output_pt   = os.path.join(out_dir, \"tydiqa_telugu.pt\")\n",
    "\n",
    "# ------------------\n",
    "# Hyperparameters\n",
    "# ------------------\n",
    "max_length = 512\n",
    "model_tokenizer_name = \"xlm-roberta-large\"  # or \"xlm-roberta-base\" if GPU memory is limited\n",
    "\n",
    "def filter_answerable_squad(input_path):\n",
    "    \"\"\"\n",
    "    Returns a new SQuAD JSON dict containing only QAs where is_impossible=False\n",
    "    and at least one answer exists.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    new_data = {\n",
    "        \"version\": data.get(\"version\", \"filtered_tydiqa_telugu\"),\n",
    "        \"data\": []\n",
    "    }\n",
    "    for article in data[\"data\"]:\n",
    "        new_paragraphs = []\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            new_qas = []\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                # Keep only answerable (i.e., is_impossible=False) with at least one answer\n",
    "                if not qa.get(\"is_impossible\", False) and qa.get(\"answers\"):\n",
    "                    new_qas.append(qa)\n",
    "            if new_qas:\n",
    "                new_paragraphs.append({\n",
    "                    \"context\": context,\n",
    "                    \"qas\": new_qas\n",
    "                })\n",
    "        if new_paragraphs:\n",
    "            new_data[\"data\"].append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"paragraphs\": new_paragraphs\n",
    "            })\n",
    "    return new_data\n",
    "\n",
    "def build_answerable_examples(squad_data, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    For each answerable QA:\n",
    "     - tokenize (question + context)\n",
    "     - find start/end token indices based on char offsets\n",
    "     - store offset_mapping, context, gold_text, etc.\n",
    "    \"\"\"\n",
    "    examples_out = []\n",
    "    for article in squad_data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                # Take the first answer (or you could loop over all)\n",
    "                ans = qa[\"answers\"][0]\n",
    "                ans_start = ans[\"answer_start\"]\n",
    "                ans_text  = ans[\"text\"]\n",
    "                ans_end   = ans_start + len(ans_text)\n",
    "\n",
    "                enc = tokenizer(\n",
    "                    qa[\"question\"],\n",
    "                    context,\n",
    "                    max_length=max_length,\n",
    "                    truncation=\"only_second\",  # question=first, context=second\n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\"\n",
    "                )\n",
    "\n",
    "                input_ids      = enc[\"input_ids\"][0]\n",
    "                attention_mask = enc[\"attention_mask\"][0]\n",
    "                offset_mapping = enc[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "                # find start/end token indices in tokenized context\n",
    "                start_token, end_token = None, None\n",
    "                for i, (start_char, end_char) in enumerate(offset_mapping):\n",
    "                    if start_char <= ans_start < end_char:\n",
    "                        start_token = i\n",
    "                    if start_char < ans_end <= end_char:\n",
    "                        end_token = i\n",
    "\n",
    "                # fallback if mismatch\n",
    "                if start_token is None or end_token is None or end_token < start_token:\n",
    "                    start_token = 0\n",
    "                    end_token   = 0\n",
    "\n",
    "                ex_item = {\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"start_positions\": torch.tensor(start_token, dtype=torch.long),\n",
    "                    \"end_positions\":   torch.tensor(end_token,   dtype=torch.long),\n",
    "                    \"offset_mapping\":  offset_mapping,\n",
    "                    \"context\":         context,\n",
    "                    \"gold_text\":       ans_text\n",
    "                }\n",
    "                examples_out.append(ex_item)\n",
    "    return examples_out\n",
    "\n",
    "def main():\n",
    "    print(f\"[INFO] Using tokenizer: {model_tokenizer_name}\")\n",
    "    tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_tokenizer_name)\n",
    "\n",
    "    print(f\"[INFO] Filtering answerable QAs in {input_json}...\")\n",
    "    filtered_data = filter_answerable_squad(input_json)\n",
    "\n",
    "    print(\"[INFO] Building tokenized examples...\")\n",
    "    examples = build_answerable_examples(filtered_data, tokenizer, max_length)\n",
    "    print(f\"[INFO] Total answerable examples: {len(examples)}\")\n",
    "\n",
    "    print(f\"[INFO] Saving to {output_pt}\")\n",
    "    torch.save(examples, output_pt)\n",
    "\n",
    "    print(\"[DONE] Preprocessing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Loading processed dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2419/3862691885.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_list = torch.load(DATA_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from ./final_xlmr_tel_answerable_3_v2...\n",
      "\n",
      "[INFO] Running inference on all examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2419/3862691885.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids      = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
      "/tmp/ipykernel_2419/3862691885.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Computing metrics...\n",
      "\n",
      "===== Final Evaluation Metrics =====\n",
      "exact_match: 55.46\n",
      "f1: 71.94\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluate_tydiqa_telugu.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    XLMRobertaForQuestionAnswering,\n",
    "    XLMRobertaTokenizerFast\n",
    ")\n",
    "\n",
    "# ------------------\n",
    "# Adjust paths here\n",
    "# ------------------\n",
    "DATA_DIR = \"processed_tydiqa_telugu\"\n",
    "DATA_PATH = os.path.join(DATA_DIR, \"tydiqa_telugu.pt\")\n",
    "\n",
    "MODEL_PATH = \"./final_xlmr_tel_answerable_3_v2\"  # Path to your fine-tuned QA model\n",
    "\n",
    "print(\"\\n[INFO] Loading processed dataset...\")\n",
    "data_list = torch.load(DATA_PATH)\n",
    "dataset = Dataset.from_list(data_list)\n",
    "\n",
    "print(f\"[INFO] Loading model from {MODEL_PATH}...\")\n",
    "model = XLMRobertaForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def postprocess_qa_predictions(examples, start_logits, end_logits):\n",
    "    \"\"\"\n",
    "    Convert model logits into text predictions by:\n",
    "      - Finding best start/end\n",
    "      - Using offset_mapping to slice the original context\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    num_examples = len(examples)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        ex = examples[i]\n",
    "        offsets = ex[\"offset_mapping\"]\n",
    "        context = ex[\"context\"]\n",
    "\n",
    "        # If mismatch in array sizes, skip\n",
    "        if i >= len(start_logits) or i >= len(end_logits):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_idx = int(np.argmax(start_logits[i]))\n",
    "        end_idx   = int(np.argmax(end_logits[i]))\n",
    "\n",
    "        # Check valid indices\n",
    "        if (\n",
    "            start_idx >= len(offsets) or\n",
    "            end_idx   >= len(offsets) or\n",
    "            start_idx > end_idx\n",
    "        ):\n",
    "            preds[ex[\"id\"]] = \"\"\n",
    "            continue\n",
    "\n",
    "        start_char = offsets[start_idx][0]\n",
    "        end_char   = offsets[end_idx][1]\n",
    "        pred_text  = context[start_char:end_char]\n",
    "\n",
    "        preds[ex[\"id\"]] = pred_text\n",
    "\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(eval_preds, examples):\n",
    "    \"\"\"\n",
    "    Compute EM and F1 on the predictions vs. gold_text.\n",
    "    \"\"\"\n",
    "    start_logits, end_logits = eval_preds\n",
    "\n",
    "    # Convert any torch.Tensors to numpy\n",
    "    if isinstance(start_logits, torch.Tensor):\n",
    "        start_logits = start_logits.cpu().numpy()\n",
    "    if isinstance(end_logits, torch.Tensor):\n",
    "        end_logits = end_logits.cpu().numpy()\n",
    "\n",
    "    predictions = postprocess_qa_predictions(examples, start_logits, end_logits)\n",
    "\n",
    "    total_em, total_f1 = 0.0, 0.0\n",
    "    for ex_idx, ex in enumerate(examples):\n",
    "        ex_id = ex[\"id\"]\n",
    "        pred  = predictions.get(ex_id, \"\")\n",
    "        gold  = ex[\"gold_text\"]\n",
    "\n",
    "        total_em += exact_match(pred, gold)\n",
    "        total_f1 += f1_score(pred, gold)\n",
    "\n",
    "    count = len(examples)\n",
    "    return {\n",
    "        \"exact_match\": 100.0 * total_em / count,\n",
    "        \"f1\":          100.0 * total_f1 / count\n",
    "    }\n",
    "\n",
    "def exact_match(pred, gold):\n",
    "    return 1.0 if normalize_text(pred) == normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred, gold):\n",
    "    pred_tokens = normalize_text(pred).split()\n",
    "    gold_tokens = normalize_text(gold).split()\n",
    "\n",
    "    common   = set(pred_tokens) & set(gold_tokens)\n",
    "    num_same = len(common)\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 1.0 if pred_tokens == gold_tokens else 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall    = num_same / len(gold_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"\n",
    "    Lower text and remove punctuation, articles, extra whitespace.\n",
    "    \"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "    def remove_punc(text):\n",
    "        return re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    s = s.lower()\n",
    "    s = remove_articles(s)\n",
    "    s = remove_punc(s)\n",
    "    s = white_space_fix(s)\n",
    "    return s\n",
    "\n",
    "print(\"\\n[INFO] Running inference on all examples...\")\n",
    "start_logits_list = []\n",
    "end_logits_list   = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in data_list:\n",
    "        input_ids      = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        start_logits_list.append(outputs.start_logits.cpu().numpy())\n",
    "        end_logits_list.append(outputs.end_logits.cpu().numpy())\n",
    "\n",
    "# Concatenate to get final arrays\n",
    "start_logits = np.concatenate(start_logits_list, axis=0)\n",
    "end_logits   = np.concatenate(end_logits_list,   axis=0)\n",
    "\n",
    "print(\"[INFO] Computing metrics...\")\n",
    "metrics = compute_metrics((start_logits, end_logits), data_list)\n",
    "\n",
    "print(\"\\n===== Final Evaluation Metrics =====\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
